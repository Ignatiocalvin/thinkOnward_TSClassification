{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize first Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Import the data sets\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "base_path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\"\n",
    "data_path = r\"\\data\\building-instinct-starter-notebook\\Starter notebook\"\n",
    "preprocessing_path = r\"\\kai\\preprocessing\"\n",
    "sys.path.append(base_path+data_path)\n",
    "sys.path.append(base_path+\"\\kai\")\n",
    "sys.path.append(base_path+preprocessing_path)\n",
    "from preprocessing.preprocessing import Preprocessor\n",
    "\n",
    "df_features_dict = {}\n",
    "\n",
    "for s in [\"monthly\", \"weekly\", \"daily\", \"with_regional/monthly\", \"with_regional/weekly\", \"with_regional/daily\"]:\n",
    "    df_features = pd.read_parquet(base_path + f'/preprocessed_data/{s}_data.parquet', engine='pyarrow')\n",
    "    df_features.sort_index(inplace=True)\n",
    "    df_features_dict[s] = df_features\n",
    "\n",
    "# Full Dataset\n",
    "df_features_full = pd.read_parquet(base_path + '/preprocessed_data/standard_data.parquet', engine='pyarrow')\n",
    "df_features_full.sort_index(inplace=True)\n",
    "df_features_dict['full'] = df_features_full\n",
    "\n",
    "# Full with regional Dataset\n",
    "df_features_full = pd.read_parquet(base_path + '/preprocessed_data/with_regional/standard_data.parquet', engine='pyarrow')\n",
    "df_features_full.sort_index(inplace=True)\n",
    "df_features_dict['with_regional/full'] = df_features_full\n",
    "\n",
    "# Labels\n",
    "load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def sample_hyperparameters(param_grid):\n",
    "    \"\"\"\n",
    "    Sample hyperparameters from the given grid using scipy.stats.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    param_grid : dict\n",
    "        A dictionary where keys are hyperparameter names and values are lists of options or ranges.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with sampled hyperparameters.\n",
    "    \"\"\"\n",
    "    sampled_params = {}\n",
    "    \n",
    "    for param, values in param_grid.items():\n",
    "        if isinstance(values, list):\n",
    "            # Randomly choose from list of options\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        elif isinstance(values, tuple) and len(values) == 2:\n",
    "            min_val, max_val = values\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                # Sample integer values using scipy.stats.randint\n",
    "                sampled_params[param] = stats.randint.rvs(min_val, max_val + 1)\n",
    "            elif isinstance(min_val, float) and isinstance(max_val, float):\n",
    "                # Sample float values using scipy.stats.uniform\n",
    "                sampled_params[param] = stats.uniform.rvs(min_val, max_val - min_val)\n",
    "        elif isinstance(values, str) and values == 'choice':\n",
    "            # Sample from a list of options if 'choice' is specified\n",
    "            sampled_params[param] = np.random.choice(param_grid[param])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported parameter type for {param}: {values}\")\n",
    "    \n",
    "    # Adjust parameters for RandomForestClassifier\n",
    "    if not sampled_params.get('bootstrap', True):\n",
    "        sampled_params['max_samples'] = None  # Reset max_samples if bootstrap is False\n",
    "        # sampled_params['oob_score'] = False\n",
    "    # Ensure max_features is correctly set\n",
    "    max_features = sampled_params.get('max_features')\n",
    "    if isinstance(max_features, str) and max_features.startswith('0'):\n",
    "        sampled_params['max_features'] = float(max_features)\n",
    "        \n",
    "    \n",
    "    return sampled_params\n",
    "\n",
    "\n",
    "def random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\"):\n",
    "    if target == \"building_stock_type\":\n",
    "        results_file = \"results.csv\"\n",
    "        y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    elif target == \"com\":\n",
    "        results_file = \"com_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    elif target == \"res\":\n",
    "        results_file = \"res_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    \n",
    "    # Check if results file exists\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        # Create an empty DataFrame if the results file doesn't exist\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'train_set', 'f1_train', 'f1_val', 'n_estimators', 'criterion', 'max_depth', \n",
    "            'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap'\n",
    "        ])\n",
    "\n",
    "    for _ in tqdm(range(n_runs), file=sys.stdout, desc=\"Running Random Search\"):\n",
    "        # print(f\"Run {_ + 1}/{n_runs}\")\n",
    "        # sample parameters\n",
    "        params = sample_hyperparameters(param_grid)\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - Params: {params}\")\n",
    "        train_set = params['train_set']\n",
    "        df_features = df_features_dict[train_set].copy()\n",
    "        if target == \"building_stock_type\":\n",
    "            X = df_features\n",
    "        elif target == \"com\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'commercial']\n",
    "        elif target == \"res\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'residential']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        rf_params = {key: value for key, value in params.items() if key != 'train_set'}\n",
    "        clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                ('scaler', StandardScaler(), df_features.columns),\n",
    "                ('encoder', OneHotEncoder(), [])\n",
    "            ])),\n",
    "            ('classifier', RandomForestClassifier(**rf_params))\n",
    "        ])\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_val_pred = clf.predict(X_val)\n",
    "\n",
    "        if target == \"building_stock_type\":\n",
    "            # Evaluate on the training set\n",
    "            f1_train = f1_score(y_train, y_train, average='macro')\n",
    "            train_conf_matrix = confusion_matrix(y_train, y_train)\n",
    "            train_TN, train_FP, train_FN, train_TP = train_conf_matrix.ravel()\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
    "            val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the validation set confusion matrix\n",
    "            val_TN, val_FP, val_FN, val_TP = val_conf_matrix.ravel()\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                'train_TP': train_TP,\n",
    "                'train_TN': train_TN,\n",
    "                'train_FP': train_FP,\n",
    "                'train_FN': train_FN,\n",
    "                'val_TP': val_TP,\n",
    "                'val_TN': val_TN,\n",
    "                'val_FP': val_FP,\n",
    "                'val_FN': val_FN,\n",
    "                **rf_params\n",
    "                }\n",
    "        else:\n",
    "            y_train_pred = pd.DataFrame(y_train_pred, columns=y.columns)\n",
    "            y_val_pred = pd.DataFrame(y_val_pred, columns=y.columns)\n",
    "\n",
    "            # Evaluate on the training set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_train[col], y_train_pred[col], average=\"macro\")\n",
    "            f1_train = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_val[col], y_val_pred[col], average=\"macro\")\n",
    "            f1_val = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "        \n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                **rf_params\n",
    "            }\n",
    "        # Update tqdm description with the current F1 score\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - F1 Score (Val): {f1_val:.4f}\")\n",
    "        # tqdm.set_description(f\"Run {_ + 1}/{n_runs} - F1 Val: {f1_val:.4f}\")\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'n_estimators': (10, 1000),  # Integer range\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # List of options\n",
    "    'max_depth': (1, 100),  # Integer range\n",
    "    'min_samples_split': (2, 10),  # Integer range\n",
    "    'min_samples_leaf': (1, 10),  # Integer range\n",
    "    'max_features': ['sqrt', 'log2'],  # List of options\n",
    "    'bootstrap': [True, False],  # List of options\n",
    "    'train_set': ['full', 'daily', 'weekly', 'monthly', 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/full']  # List of options\n",
    "}\n",
    "\n",
    "# random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commercial and residential columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/30 - Params: {'n_estimators': 56, 'criterion': 'log_loss', 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'with_regional/full', 'n_jobs': -1, 'random_state': 224}\n",
      "Run 1/30 - F1 Score (Val): 0.2743                            \n",
      "Run 2/30 - Params: {'n_estimators': 198, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'with_regional/monthly', 'n_jobs': -1, 'random_state': 342}\n",
      "Run 2/30 - F1 Score (Val): 0.2360                                    \n",
      "Run 3/30 - Params: {'n_estimators': 111, 'criterion': 'gini', 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'train_set': 'with_regional/full', 'n_jobs': -1, 'random_state': 11, 'max_samples': None}\n",
      "Run 3/30 - F1 Score (Val): 0.2830                                    \n",
      "Run 4/30 - Params: {'n_estimators': 155, 'criterion': 'entropy', 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'full', 'n_jobs': -1, 'random_state': 1}\n",
      "Run 4/30 - F1 Score (Val): 0.2046                                    \n",
      "Run 5/30 - Params: {'n_estimators': 66, 'criterion': 'log_loss', 'max_depth': 28, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'with_regional/daily', 'n_jobs': -1, 'random_state': 138, 'max_samples': None}\n",
      "Run 5/30 - F1 Score (Val): 0.2793                                    \n",
      "Run 6/30 - Params: {'n_estimators': 152, 'criterion': 'log_loss', 'max_depth': 20, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'with_regional/monthly', 'n_jobs': -1, 'random_state': 155}\n",
      "Run 6/30 - F1 Score (Val): 0.2839                                    \n",
      "Run 7/30 - Params: {'n_estimators': 111, 'criterion': 'log_loss', 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'full', 'n_jobs': -1, 'random_state': 30, 'max_samples': None}\n",
      "Run 7/30 - F1 Score (Val): 0.2768                                    \n",
      "Run 8/30 - Params: {'n_estimators': 137, 'criterion': 'gini', 'max_depth': 28, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'full', 'n_jobs': -1, 'random_state': 379}\n",
      "Run 8/30 - F1 Score (Val): 0.2743                                    \n",
      "Run 9/30 - Params: {'n_estimators': 141, 'criterion': 'log_loss', 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'with_regional/daily', 'n_jobs': -1, 'random_state': 43, 'max_samples': None}\n",
      "Run 9/30 - F1 Score (Val): 0.2795                                    \n",
      "Run 10/30 - Params: {'n_estimators': 123, 'criterion': 'log_loss', 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'with_regional/monthly', 'n_jobs': -1, 'random_state': 234}\n",
      "Run 10/30 - F1 Score (Val): 0.2869                                   \n",
      "Run 11/30 - Params: {'n_estimators': 55, 'criterion': 'entropy', 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'with_regional/full', 'n_jobs': -1, 'random_state': 298}\n",
      "Run 11/30 - F1 Score (Val): 0.2592                                    \n",
      "Run 12/30 - Params: {'n_estimators': 27, 'criterion': 'entropy', 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'daily', 'n_jobs': -1, 'random_state': 233, 'max_samples': None}\n",
      "Run 12/30 - F1 Score (Val): 0.2784                                    \n",
      "Run 13/30 - Params: {'n_estimators': 75, 'criterion': 'gini', 'max_depth': 27, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'train_set': 'with_regional/daily', 'n_jobs': -1, 'random_state': 173, 'max_samples': None}\n",
      "Run 13/30 - F1 Score (Val): 0.2831                                    \n",
      "Run 14/30 - Params: {'n_estimators': 184, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'daily', 'n_jobs': -1, 'random_state': 217}\n",
      "Run 14/30 - F1 Score (Val): 0.2742                                    \n",
      "Run 15/30 - Params: {'n_estimators': 150, 'criterion': 'log_loss', 'max_depth': 24, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'weekly', 'n_jobs': -1, 'random_state': 99}\n",
      "Run 15/30 - F1 Score (Val): 0.2829                                    \n",
      "Run 16/30 - Params: {'n_estimators': 24, 'criterion': 'gini', 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'full', 'n_jobs': -1, 'random_state': 32}\n",
      "Run 16/30 - F1 Score (Val): 0.2727                                    \n",
      "Run 17/30 - Params: {'n_estimators': 199, 'criterion': 'log_loss', 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'with_regional/daily', 'n_jobs': -1, 'random_state': 368, 'max_samples': None}\n",
      "Run 17/30 - F1 Score (Val): 0.2757                                    \n",
      "Run 18/30 - Params: {'n_estimators': 144, 'criterion': 'entropy', 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'with_regional/monthly', 'n_jobs': -1, 'random_state': 244, 'max_samples': None}\n",
      "Run 18/30 - F1 Score (Val): 0.2874                                    \n",
      "Run 19/30 - Params: {'n_estimators': 106, 'criterion': 'log_loss', 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'weekly', 'n_jobs': -1, 'random_state': 299}\n",
      "Run 19/30 - F1 Score (Val): 0.2840                                    \n",
      "Run 20/30 - Params: {'n_estimators': 55, 'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'daily', 'n_jobs': -1, 'random_state': 79, 'max_samples': None}\n",
      "Run 20/30 - F1 Score (Val): 0.2369                                    \n",
      "Run 21/30 - Params: {'n_estimators': 39, 'criterion': 'entropy', 'max_depth': 18, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'with_regional/weekly', 'n_jobs': -1, 'random_state': 109, 'max_samples': None}\n",
      "Run 21/30 - F1 Score (Val): 0.2863                                    \n",
      "Run 22/30 - Params: {'n_estimators': 112, 'criterion': 'entropy', 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'with_regional/monthly', 'n_jobs': -1, 'random_state': 112}\n",
      "Run 22/30 - F1 Score (Val): 0.2828                                    \n",
      "Run 23/30 - Params: {'n_estimators': 46, 'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'monthly', 'n_jobs': -1, 'random_state': 159, 'max_samples': None}\n",
      "Run 23/30 - F1 Score (Val): 0.2588                                    \n",
      "Run 24/30 - Params: {'n_estimators': 139, 'criterion': 'log_loss', 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'with_regional/weekly', 'n_jobs': -1, 'random_state': 290, 'max_samples': None}\n",
      "Run 24/30 - F1 Score (Val): 0.2768                                    \n",
      "Run 25/30 - Params: {'n_estimators': 11, 'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'train_set': 'monthly', 'n_jobs': -1, 'random_state': 124, 'max_samples': None}\n",
      "Run 25/30 - F1 Score (Val): 0.2881                                    \n",
      "Run 26/30 - Params: {'n_estimators': 157, 'criterion': 'gini', 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'train_set': 'monthly', 'n_jobs': -1, 'random_state': 305, 'max_samples': None}\n",
      "Run 26/30 - F1 Score (Val): 0.2928                                    \n",
      "Run 27/30 - Params: {'n_estimators': 111, 'criterion': 'log_loss', 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'train_set': 'with_regional/full', 'n_jobs': -1, 'random_state': 302, 'max_samples': None}\n",
      "Run 27/30 - F1 Score (Val): 0.2804                                    \n",
      "Run 28/30 - Params: {'n_estimators': 150, 'criterion': 'entropy', 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'monthly', 'n_jobs': -1, 'random_state': 135}\n",
      "Run 28/30 - F1 Score (Val): 0.2683                                     \n",
      "Run 29/30 - Params: {'n_estimators': 144, 'criterion': 'gini', 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'monthly', 'n_jobs': -1, 'random_state': 20}\n",
      "Run 29/30 - F1 Score (Val): 0.2789                                    \n",
      "Run 30/30 - Params: {'n_estimators': 10, 'criterion': 'entropy', 'max_depth': 27, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'with_regional/monthly', 'n_jobs': -1, 'random_state': 196}\n",
      "Run 30/30 - F1 Score (Val): 0.2801                                    \n",
      "Running Random Search: 100%|██████████| 30/30 [20:00<00:00, 40.01s/it]\n"
     ]
    }
   ],
   "source": [
    "def calculate_hierarchical_f1_score(df_targets, df_pred, alpha=0.4, average='macro', F1_list=False):\n",
    "    \"\"\"\n",
    "    Calculate the hierarchical F1-score for a multi-level classification problem.\n",
    "\n",
    "    This function computes the F1-score at two hierarchical levels:\n",
    "    1. The 'building_stock_type' level, which is the first level of hierarchy.\n",
    "    2. The second level, which is conditional on the 'building_stock_type' being either 'commercial' or 'residential'.\n",
    "\n",
    "    The final F1-score is a weighted average of the first level and second level F1-scores.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df_targets : pd.DataFrame\n",
    "        The dataframe containing the true target values. It must include a column 'building_stock_type' and other\n",
    "        columns ending with '_com' or '_res' representing the second level of classification.\n",
    "\n",
    "    df_pred : pd.DataFrame\n",
    "        The dataframe containing the predicted values. It must be structured similarly to `df_targets`.\n",
    "\n",
    "    alpha : float, optional, default=0.3\n",
    "        The weight given to the first level F1-score in the final score calculation. The weight for the second level\n",
    "        F1-score will be (1 - alpha).\n",
    "\n",
    "    average : str, optional, default='macro'\n",
    "        The averaging method for calculating the F1-score. It is passed directly to the `f1_score` function from sklearn.\n",
    "\n",
    "    F1_list : bool, optional, default=False\n",
    "        If True, the function returns a dictionary of F1-scores for all individual columns along with the overall F1-score.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float or tuple\n",
    "        If `F1_list` is False, returns a single float representing the overall hierarchical F1-score.\n",
    "        If `F1_list` is True, returns a tuple where the first element is the overall hierarchical F1-score and the second\n",
    "        element is a dictionary containing the F1-scores for all individual columns.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_f1_l2(df_targets, df_pred, average):\n",
    "        \"\"\"\n",
    "        Calculate the F1-score for the second level of hierarchy.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_targets : pd.DataFrame\n",
    "            The dataframe containing the true target values for the second level of hierarchy.\n",
    "        df_pred : pd.DataFrame\n",
    "            The dataframe containing the predicted values for the second level of hierarchy.\n",
    "        average : str\n",
    "            The averaging method for calculating the F1-score.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary where keys are column names and values are the corresponding F1-scores.\n",
    "        \"\"\"\n",
    "        F1_l2_dict = {column: 0 for column in df_targets.columns}\n",
    "\n",
    "        # Find the intersection of indices\n",
    "        common_indices = df_targets.index.intersection(df_pred.index)\n",
    "\n",
    "        # Check if the intersection is empty\n",
    "        if common_indices.empty:\n",
    "            return F1_l2_dict\n",
    "        else:\n",
    "            # Select only the rows with common indices\n",
    "            df_targets_common = df_targets.loc[common_indices]\n",
    "            df_pred_common = df_pred.loc[common_indices]\n",
    "\n",
    "            # Calculate the F1-score for each column based on the common rows\n",
    "            for column in df_targets.columns:\n",
    "                F1_l2_dict[column] = f1_score(df_targets_common[column], df_pred_common[column], average=average)\n",
    "\n",
    "        return F1_l2_dict\n",
    "\n",
    "    # Sort both dataframes based on index\n",
    "    df_targets = df_targets.sort_index()\n",
    "    df_pred = df_pred.sort_index()\n",
    "\n",
    "    # Calculate F1 score for the first level of hierarchy\n",
    "    F1_l1 = f1_score(df_targets['building_stock_type'], df_pred['building_stock_type'], average=average)\n",
    "    F1_dict = {'building_stock_type': F1_l1}\n",
    "\n",
    "    # Calculate F1 score for the second level of hierarchy (commercial buildings)\n",
    "    df_com_targets = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    df_com_pred = df_pred[df_pred['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    F1_l2_dict_com = calculate_f1_l2(df_com_targets, df_com_pred, average)\n",
    "    F1_l2_com = sum(F1_l2_dict_com.values()) / len(F1_l2_dict_com.values())\n",
    "\n",
    "    F1_l2_dict = {}\n",
    "    F1_l2_dict.update(F1_l2_dict_com)\n",
    "\n",
    "    # Calculate F1 score for the second level of hierarchy (residential buildings)\n",
    "    df_res_targets = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    df_res_pred = df_pred[df_pred['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    F1_l2_dict_res = calculate_f1_l2(df_res_targets, df_res_pred, average)\n",
    "    F1_l2_res = sum(F1_l2_dict_res.values()) / len(F1_l2_dict_res.values())\n",
    "\n",
    "    F1_l2_dict.update(F1_l2_dict_res)\n",
    "    F1_l2_dict_sorted = sorted(F1_l2_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    F1_dict.update(F1_l2_dict_sorted)\n",
    "\n",
    "    # Calculate F1 score for the second level of hierarchy\n",
    "    F1_l2 = (F1_l2_com + F1_l2_res) / 2\n",
    "\n",
    "    # Calculate overall F1 score\n",
    "    F1 = alpha * F1_l1 + (1 - alpha) * F1_l2\n",
    "\n",
    "    if F1_list:\n",
    "        return F1, F1_dict\n",
    "\n",
    "    return F1\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'n_estimators': (10, 200),  # Integer range\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # List of options\n",
    "    'max_depth': (2, 30),  # Integer range\n",
    "    'min_samples_split': (2, 10),  # Integer range\n",
    "    'min_samples_leaf': (1, 5),  # Integer range\n",
    "    'max_features': ['sqrt', 'log2'],#, 0.5, 0.8],  # List of options None,\n",
    "    'bootstrap': [True, False],  # List of options\n",
    "    'train_set': ['full', 'daily', 'weekly', 'monthly', 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/full'],  # List of options\n",
    "    'n_jobs': [-1],\n",
    "    'random_state': (1, 420),\n",
    "    # 'min_weight_fraction_leaf': (0.0, 0.5),  # Float range\n",
    "    # 'max_leaf_nodes': (10, 1000),  # Integer range\n",
    "    # 'min_impurity_decrease': (0.0, 0.1),  # Float range\n",
    "    # # 'oob_score': [True, False],  # List of options\n",
    "    # 'class_weight': [None, 'balanced', 'balanced_subsample'],  # Class weights\n",
    "    # 'ccp_alpha': (0.0, 0.1),  # Complexity parameter for pruning\n",
    "    # 'max_samples': [None, 0.5, 0.8, 1.0],  # Fraction or integer number of samples\n",
    "}\n",
    "\n",
    "# Call the function\n",
    "random_search(df_features_dict, df_targets, param_grid, n_runs=30, target=\"res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- xgboost\n",
    "- MulticolumnClassifier\n",
    "- check submission quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_features_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 129\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m    109\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m200\u001b[39m),  \u001b[38;5;66;03m# Integer range\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_loss\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# List of options\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# 'max_samples': [None, 0.5, 0.8, 1.0],  # Fraction or integer number of samples\u001b[39;00m\n\u001b[0;32m    127\u001b[0m }\n\u001b[1;32m--> 129\u001b[0m multi_random_search(\u001b[43mdf_features_dict\u001b[49m, df_targets, param_grid, n_runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mres\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_features_dict' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "def multi_random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\"):\n",
    "    if target == \"building_stock_type\":\n",
    "        results_file = \"HP_results/multiRF/multi_results.csv\"\n",
    "        y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    elif target == \"com\":\n",
    "        results_file = \"HP_results/multiRF/multi_com_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    elif target == \"res\":\n",
    "        results_file = \"HP_results/multiRF/multi_res_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    \n",
    "    # Check if results file exists\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        # Create an empty DataFrame if the results file doesn't exist\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'train_set', 'f1_train', 'f1_val', 'n_estimators', 'criterion', 'max_depth', \n",
    "            'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap'\n",
    "        ])\n",
    "\n",
    "    for _ in tqdm(range(n_runs), file=sys.stdout, desc=\"Running Random Search\"):\n",
    "        # print(f\"Run {_ + 1}/{n_runs}\")\n",
    "        # sample parameters\n",
    "        params = sample_hyperparameters(param_grid)\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - Params: {params}\")\n",
    "        train_set = params['train_set']\n",
    "        df_features = df_features_dict[train_set].copy()\n",
    "        if target == \"building_stock_type\":\n",
    "            X = df_features\n",
    "        elif target == \"com\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'commercial']\n",
    "        elif target == \"res\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'residential']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        rf_params = {key: value for key, value in params.items() if key != 'train_set'}\n",
    "        clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                ('scaler', StandardScaler(), df_features.columns),\n",
    "                ('encoder', OneHotEncoder(), [])\n",
    "            ])),\n",
    "            ('classifier', MultiOutputClassifier(RandomForestClassifier(**rf_params), n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_val_pred = clf.predict(X_val)\n",
    "\n",
    "        if target == \"building_stock_type\":\n",
    "            # Evaluate on the training set\n",
    "            f1_train = f1_score(y_train, y_train, average='macro')\n",
    "            train_conf_matrix = confusion_matrix(y_train, y_train)\n",
    "            train_TN, train_FP, train_FN, train_TP = train_conf_matrix.ravel()\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
    "            val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the validation set confusion matrix\n",
    "            val_TN, val_FP, val_FN, val_TP = val_conf_matrix.ravel()\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                'train_TP': train_TP,\n",
    "                'train_TN': train_TN,\n",
    "                'train_FP': train_FP,\n",
    "                'train_FN': train_FN,\n",
    "                'val_TP': val_TP,\n",
    "                'val_TN': val_TN,\n",
    "                'val_FP': val_FP,\n",
    "                'val_FN': val_FN,\n",
    "                **rf_params\n",
    "                }\n",
    "        else:\n",
    "            y_train_pred = pd.DataFrame(y_train_pred, columns=y.columns)\n",
    "            y_val_pred = pd.DataFrame(y_val_pred, columns=y.columns)\n",
    "\n",
    "            # Evaluate on the training set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_train[col], y_train_pred[col], average=\"macro\")\n",
    "            f1_train = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_val[col], y_val_pred[col], average=\"macro\")\n",
    "            f1_val = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "        \n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                **rf_params\n",
    "            }\n",
    "        # Update tqdm description with the current F1 score\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - F1 Score (Val): {f1_val:.4f}\")\n",
    "        # tqdm.set_description(f\"Run {_ + 1}/{n_runs} - F1 Val: {f1_val:.4f}\")\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'n_estimators': (50, 200),  # Integer range\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # List of options\n",
    "    'max_depth': (25, 50),  # Integer range\n",
    "    'min_samples_split': (3, 10),  # Integer range\n",
    "    'min_samples_leaf': (1, 5),  # Integer range\n",
    "    'max_features': ['sqrt' ],#None, 0.5, 0.8],  # List of options\n",
    "    'bootstrap': [True, False],  # List of options\n",
    "    'train_set': [ 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/full'],  # List of options 'full', 'daily', 'weekly', 'monthly',\n",
    "    'n_jobs': [-1],\n",
    "    'random_state': (1, 420),\n",
    "    # 'min_weight_fraction_leaf': (0.0, 0.5),  # Float range\n",
    "    # 'max_leaf_nodes': (10, 1000),  # Integer range\n",
    "    # 'min_impurity_decrease': (0.0, 0.1),  # Float range\n",
    "    # 'oob_score': [True, False],  # List of options\n",
    "    # 'class_weight': [None, 'balanced', 'balanced_subsample'],  # Class weights\n",
    "    # 'ccp_alpha': (0.0, 0.1),  # Complexity parameter for pruning\n",
    "    # 'max_samples': [None, 0.5, 0.8, 1.0],  # Fraction or integer number of samples\n",
    "}\n",
    "\n",
    "multi_random_search(df_features_dict, df_targets, param_grid, n_runs=15, target=\"res\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
