{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize first Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Import the data sets\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "base_path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\"\n",
    "data_path = r\"\\data\\building-instinct-starter-notebook\\Starter notebook\"\n",
    "preprocessing_path = r\"\\kai\\preprocessing\"\n",
    "sys.path.append(base_path+data_path)\n",
    "sys.path.append(base_path+\"\\kai\")\n",
    "sys.path.append(base_path+preprocessing_path)\n",
    "from preprocessing.preprocessing import Preprocessor\n",
    "\n",
    "pre_load = False\n",
    "if pre_load:\n",
    "    df_features_dict = {}\n",
    "\n",
    "    for s in [\"monthly\", \"weekly\", \"daily\", \"with_regional/monthly\", \"with_regional/weekly\", \"with_regional/daily\"]:\n",
    "        df_features = pd.read_parquet(base_path + f'/preprocessed_data/{s}_data.parquet', engine='pyarrow')\n",
    "        df_features.sort_index(inplace=True)\n",
    "        df_features_dict[s] = df_features\n",
    "\n",
    "    # Full Dataset\n",
    "    df_features_full = pd.read_parquet(base_path + '/preprocessed_data/standard_data.parquet', engine='pyarrow')\n",
    "    df_features_full.sort_index(inplace=True)\n",
    "    df_features_dict['full'] = df_features_full\n",
    "\n",
    "    # Full with regional Dataset\n",
    "    df_features_full = pd.read_parquet(base_path + '/preprocessed_data/with_regional/standard_data.parquet', engine='pyarrow')\n",
    "    df_features_full.sort_index(inplace=True)\n",
    "    df_features_dict['with_regional/full'] = df_features_full\n",
    "\n",
    "# Labels\n",
    "load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5 - Params: {'n_estimators': 13, 'max_depth': 81, 'eta': 0.2853758974319447, 'random_state': 42, 'device': 'cuda', 'train_set': 'with_regional/weekly'}\n",
      "Run 1/5 - Finished column: in.bedrooms_res (1/13) with F1 Score (Val): 0.2662\n",
      "Run 1/5 - Finished column: in.cooling_setpoint_res (2/13) with F1 Score (Val): 0.0976\n",
      "Run 1/5 - Finished column: in.heating_setpoint_res (3/13) with F1 Score (Val): 0.1785\n",
      "Run 1/5 - Finished column: in.geometry_building_type_recs_res (4/13) with F1 Score (Val): 0.3703\n",
      "Run 1/5 - Finished column: in.geometry_floor_area_res (5/13) with F1 Score (Val): 0.2588\n",
      "Run 1/5 - Finished column: in.geometry_foundation_type_res (6/13) with F1 Score (Val): 0.3031\n",
      "Run 1/5 - Finished column: in.geometry_wall_type_res (7/13) with F1 Score (Val): 0.3904\n",
      "Run 1/5 - Finished column: in.heating_fuel_res (8/13) with F1 Score (Val): 0.2619\n",
      "Run 1/5 - Finished column: in.income_res (9/13) with F1 Score (Val): 0.0550\n",
      "Run 1/5 - Finished column: in.roof_material_res (10/13) with F1 Score (Val): 0.1927\n",
      "Run 1/5 - Finished column: in.tenure_res (11/13) with F1 Score (Val): 0.6678\n",
      "Run 1/5 - Finished column: in.vacancy_status_res (12/13) with F1 Score (Val): 0.9852\n",
      "Run 1/5 - Finished column: in.vintage_res (13/13) with F1 Score (Val): 0.1154\n",
      "Run 1/5 - F1 Score (Val): 0.3187                            \n",
      "Run 2/5 - Params: {'n_estimators': 16, 'max_depth': 91, 'eta': 0.2547682459927767, 'random_state': 42, 'device': 'cuda', 'train_set': 'with_regional/daily'}\n",
      "Run 2/5 - Finished column: in.bedrooms_res (1/13) with F1 Score (Val): 0.2878\n",
      "Run 2/5 - Finished column: in.cooling_setpoint_res (2/13) with F1 Score (Val): 0.1117\n",
      "Run 2/5 - Finished column: in.heating_setpoint_res (3/13) with F1 Score (Val): 0.1510\n",
      "Run 2/5 - Finished column: in.geometry_building_type_recs_res (4/13) with F1 Score (Val): 0.3391\n",
      "Run 2/5 - Finished column: in.geometry_floor_area_res (5/13) with F1 Score (Val): 0.2013\n",
      "Run 2/5 - Finished column: in.geometry_foundation_type_res (6/13) with F1 Score (Val): 0.2923\n",
      "Run 2/5 - Finished column: in.geometry_wall_type_res (7/13) with F1 Score (Val): 0.3789\n",
      "Run 2/5 - Finished column: in.heating_fuel_res (8/13) with F1 Score (Val): 0.2808\n",
      "Run 2/5 - Finished column: in.income_res (9/13) with F1 Score (Val): 0.0481\n",
      "Run 2/5 - Finished column: in.roof_material_res (10/13) with F1 Score (Val): 0.1930\n",
      "Run 2/5 - Finished column: in.tenure_res (11/13) with F1 Score (Val): 0.6679\n",
      "Run 2/5 - Finished column: in.vacancy_status_res (12/13) with F1 Score (Val): 0.9762\n",
      "Run 2/5 - Finished column: in.vintage_res (13/13) with F1 Score (Val): 0.1232\n",
      "Run 2/5 - F1 Score (Val): 0.3116                                     \n",
      "Run 3/5 - Params: {'n_estimators': 21, 'max_depth': 47, 'eta': 0.3852013200610305, 'random_state': 42, 'device': 'cuda', 'train_set': 'monthly'}\n",
      "Run 3/5 - Finished column: in.bedrooms_res (1/13) with F1 Score (Val): 0.2832\n",
      "Run 3/5 - Finished column: in.cooling_setpoint_res (2/13) with F1 Score (Val): 0.1112\n",
      "Run 3/5 - Finished column: in.heating_setpoint_res (3/13) with F1 Score (Val): 0.1745\n",
      "Run 3/5 - Finished column: in.geometry_building_type_recs_res (4/13) with F1 Score (Val): 0.3054\n",
      "Run 3/5 - Finished column: in.geometry_floor_area_res (5/13) with F1 Score (Val): 0.2454\n",
      "Run 3/5 - Finished column: in.geometry_foundation_type_res (6/13) with F1 Score (Val): 0.2866\n",
      "Run 3/5 - Finished column: in.geometry_wall_type_res (7/13) with F1 Score (Val): 0.2838\n",
      "Run 3/5 - Finished column: in.heating_fuel_res (8/13) with F1 Score (Val): 0.2193\n",
      "Run 3/5 - Finished column: in.income_res (9/13) with F1 Score (Val): 0.0557\n",
      "Run 3/5 - Finished column: in.roof_material_res (10/13) with F1 Score (Val): 0.2066\n",
      "Run 3/5 - Finished column: in.tenure_res (11/13) with F1 Score (Val): 0.6608\n",
      "Run 3/5 - Finished column: in.vacancy_status_res (12/13) with F1 Score (Val): 0.9881\n",
      "Run 3/5 - Finished column: in.vintage_res (13/13) with F1 Score (Val): 0.1324\n",
      "Run 3/5 - F1 Score (Val): 0.3041                                     \n",
      "Run 4/5 - Params: {'n_estimators': 11, 'max_depth': 9, 'eta': 0.2914179689896034, 'random_state': 42, 'device': 'cuda', 'train_set': 'with_regional/daily'}\n",
      "Run 4/5 - Finished column: in.bedrooms_res (1/13) with F1 Score (Val): 0.2719\n",
      "Run 4/5 - Finished column: in.cooling_setpoint_res (2/13) with F1 Score (Val): 0.1089\n",
      "Run 4/5 - Finished column: in.heating_setpoint_res (3/13) with F1 Score (Val): 0.1441\n",
      "Run 4/5 - Finished column: in.geometry_building_type_recs_res (4/13) with F1 Score (Val): 0.3155\n",
      "Run 4/5 - Finished column: in.geometry_floor_area_res (5/13) with F1 Score (Val): 0.2012\n",
      "Run 4/5 - Finished column: in.geometry_foundation_type_res (6/13) with F1 Score (Val): 0.2928\n",
      "Run 4/5 - Finished column: in.geometry_wall_type_res (7/13) with F1 Score (Val): 0.3699\n",
      "Run 4/5 - Finished column: in.heating_fuel_res (8/13) with F1 Score (Val): 0.2844\n",
      "Run 4/5 - Finished column: in.income_res (9/13) with F1 Score (Val): 0.0607\n",
      "Run 4/5 - Finished column: in.roof_material_res (10/13) with F1 Score (Val): 0.2013\n",
      "Run 4/5 - Finished column: in.tenure_res (11/13) with F1 Score (Val): 0.6725\n",
      "Run 4/5 - Finished column: in.vacancy_status_res (12/13) with F1 Score (Val): 0.9762\n",
      "Run 4/5 - Finished column: in.vintage_res (13/13) with F1 Score (Val): 0.1281\n",
      "Run 4/5 - F1 Score (Val): 0.3098                                     \n",
      "Run 5/5 - Params: {'n_estimators': 49, 'max_depth': 100, 'eta': 0.37783656080700795, 'random_state': 42, 'device': 'cuda', 'train_set': 'standard'}\n",
      "Running Random Search:  80%|████████  | 4/5 [24:52<06:13, 373.01s/it]\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[21:07:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:861: Exception in gpu_hist: [21:07:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\device_helpers.cuh:400: Memory allocation error on worker 0: bad allocation: cudaErrorMemoryAllocation: out of memory\n- Free memory: 0\n- Requested memory: 13169602560\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 251\u001b[0m\n\u001b[0;32m    248\u001b[0m cp\u001b[38;5;241m.\u001b[39mget_default_memory_pool()\u001b[38;5;241m.\u001b[39mfree_all_blocks()\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# print_memory_usage()\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# print_gpu_memory()\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid_xgb_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 135\u001b[0m, in \u001b[0;36mrandom_search\u001b[1;34m(df_features_dict, df_targets, param_grid, n_runs, target)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m xgb_params:\n\u001b[0;32m    134\u001b[0m     bst \u001b[38;5;241m=\u001b[39m XGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxgb_params)\n\u001b[1;32m--> 135\u001b[0m     \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     y_train_pred \u001b[38;5;241m=\u001b[39m bst\u001b[38;5;241m.\u001b[39mpredict(X_train_gpu)\n\u001b[0;32m    137\u001b[0m     y_val_pred \u001b[38;5;241m=\u001b[39m bst\u001b[38;5;241m.\u001b[39mpredict(X_val_gpu)\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:2100\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2100\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [21:07:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:861: Exception in gpu_hist: [21:07:24] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\device_helpers.cuh:400: Memory allocation error on worker 0: bad allocation: cudaErrorMemoryAllocation: out of memory\n- Free memory: 0\n- Requested memory: 13169602560\n\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "import multiprocessing\n",
    "import gc\n",
    "import psutil\n",
    "# import torch\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "# from preprocessing.utils import print_memory_usage, print_gpu_memory, free_gpu_memory\n",
    "from xgboost import XGBClassifier\n",
    "from memory_profiler import profile\n",
    "\n",
    "\n",
    "def sample_hyperparameters(param_grid):\n",
    "    \"\"\"\n",
    "    Sample hyperparameters from the given grid using scipy.stats.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    param_grid : dict\n",
    "        A dictionary where keys are hyperparameter names and values are lists of options or ranges.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with sampled hyperparameters.\n",
    "    \"\"\"\n",
    "    sampled_params = {}\n",
    "    \n",
    "    for param, values in param_grid.items():\n",
    "        if isinstance(values, list):\n",
    "            # Randomly choose from list of options\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        elif isinstance(values, tuple) and len(values) == 2:\n",
    "            min_val, max_val = values\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                # Sample integer values using scipy.stats.randint\n",
    "                sampled_params[param] = stats.randint.rvs(min_val, max_val + 1)\n",
    "            elif isinstance(min_val, float) and isinstance(max_val, float):\n",
    "                # Sample float values using scipy.stats.uniform\n",
    "                sampled_params[param] = stats.uniform.rvs(min_val, max_val - min_val)\n",
    "        elif isinstance(values, str) and values == 'choice':\n",
    "            # Sample from a list of options if 'choice' is specified\n",
    "            sampled_params[param] = np.random.choice(param_grid[param])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported parameter type for {param}: {values}\")\n",
    "    \n",
    "    # Adjust parameters for RandomForestClassifier\n",
    "    if not sampled_params.get('bootstrap', True):\n",
    "        sampled_params['max_samples'] = None  # Reset max_samples if bootstrap is False\n",
    "        # sampled_params['oob_score'] = False\n",
    "    # Ensure max_features is correctly set\n",
    "    max_features = sampled_params.get('max_features')\n",
    "    if isinstance(max_features, str) and max_features.startswith('0'):\n",
    "        sampled_params['max_features'] = float(max_features)\n",
    "        \n",
    "    \n",
    "    return sampled_params\n",
    "\n",
    "# @profile(precision=4)\n",
    "def random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\"):\n",
    "    if target == \"building_stock_type\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"HP_results/multixgboost/multi_results.csv\"\n",
    "        y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    elif target == \"com\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"\\HP_results/multixgboost/multi_com_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "        label_encoders = {}\n",
    "        for col in y.columns:\n",
    "            le = LabelEncoder()\n",
    "            y[col] = le.fit_transform(y[col])\n",
    "            label_encoders[col] = le\n",
    "    elif target == \"res\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"\\HP_results/multixgboost/multi_res_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "        label_encoders = {}\n",
    "        for col in y.columns:\n",
    "            le = LabelEncoder()\n",
    "            y[col] = le.fit_transform(y[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Check if results file exists\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        # Create an empty DataFrame if the results file doesn't exist\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'train_set', 'f1_train', 'f1_val', 'n_estimators', 'criterion', 'max_depth', \n",
    "            'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap'\n",
    "        ])\n",
    "\n",
    "    for _ in tqdm(range(n_runs), file=sys.stdout, desc=\"Running Random Search\"):\n",
    "        # print(f\"Run {_ + 1}/{n_runs}\")\n",
    "        # sample parameters\n",
    "        # print_memory_usage()\n",
    "        # print_gpu_memory()\n",
    "        params = sample_hyperparameters(param_grid)\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - Params: {params}\")\n",
    "        train_set = params['train_set']\n",
    "        if df_features_dict is None:\n",
    "            df_features = pd.read_parquet(base_path + f'/preprocessed_data/{train_set}_data.parquet', engine='pyarrow')\n",
    "            df_features.sort_index(inplace=True)\n",
    "        else:\n",
    "            df_features = df_features_dict[train_set].copy()\n",
    "        \n",
    "        if target == \"building_stock_type\":\n",
    "            X = df_features\n",
    "        elif target == \"com\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'commercial']\n",
    "        elif target == \"res\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'residential']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        xgb_params = {key: value for key, value in params.items() if key not in ['train_set', \"num_boost_round\"]}\n",
    "\n",
    "        F1_l2_dict_train = {}\n",
    "        F1_l2_dict_val = {}\n",
    "        for i, col in enumerate(y.columns):\n",
    "            y_train_col = y_train.iloc[:, i]\n",
    "            y_val_col = y_val.iloc[:, i]\n",
    "            \n",
    "            if params[\"device\"] == \"cuda\":\n",
    "                X_train_gpu = cp.array(X_train)\n",
    "                X_val_gpu = cp.array(X_val)\n",
    "                y_train_gpu = cp.array(y_train_col)\n",
    "                y_val_gpu = cp.array(y_val_col)\n",
    "            else:\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train_col)\n",
    "                dval = xgb.DMatrix(X_val, label=y_val_col)\n",
    "            \n",
    "            # Train the model\n",
    "            if \"n_estimators\" in xgb_params:\n",
    "                bst = XGBClassifier(**xgb_params)\n",
    "                bst.fit(X_train_gpu, y_train_gpu)\n",
    "                y_train_pred = bst.predict(X_train_gpu)\n",
    "                y_val_pred = bst.predict(X_val_gpu)\n",
    "            else:# xgb.train can work with DMatrix objects\n",
    "                dtrain = xgb.DMatrix(X_train_gpu, label=y_train_gpu)\n",
    "                dval = xgb.DMatrix(X_val_gpu, label=y_val_gpu)\n",
    "                bst = xgb.train(\n",
    "                    xgb_params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=params[\"num_boost_round\"],)\n",
    "                y_train_pred = bst.predict(dtrain)\n",
    "                y_val_pred = bst.predict(dval)\n",
    "            \n",
    "            if params[\"device\"] == \"cuda\":# retransform to cpu for F1 score computation\n",
    "                y_train_pred = cp.asnumpy(y_train_pred)\n",
    "                y_val_pred = cp.asnumpy(y_val_pred)\n",
    "\n",
    "            # compute F1\n",
    "            F1_l2_dict_train[col] = f1_score(y_train_col, y_train_pred.round(), average='macro')\n",
    "            F1_l2_dict_val[col] = f1_score(y_val_col, y_val_pred.round(), average='macro')\n",
    "\n",
    "            # Clean up GPU memory after each model\n",
    "            if params[\"device\"] == \"cuda\":\n",
    "                del X_train_gpu, X_val_gpu, y_train_gpu, y_val_gpu\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                gc.collect()\n",
    "            tqdm.write(f\"Run {_ + 1}/{n_runs} - Finished column: {col} ({i+1}/{len(y.columns)}) with F1 Score (Val): {F1_l2_dict_val[col]:.4f}\")\n",
    "\n",
    "        if target == \"building_stock_type\":\n",
    "            # Evaluate on the training set\n",
    "            f1_train = f1_score(y_train, y_train, average='macro')\n",
    "            train_conf_matrix = confusion_matrix(y_train, y_train)\n",
    "            train_TN, train_FP, train_FN, train_TP = train_conf_matrix.ravel()\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
    "            val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the validation set confusion matrix\n",
    "            val_TN, val_FP, val_FN, val_TP = val_conf_matrix.ravel()\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                'train_TP': train_TP,\n",
    "                'train_TN': train_TN,\n",
    "                'train_FP': train_FP,\n",
    "                'train_FN': train_FN,\n",
    "                'val_TP': val_TP,\n",
    "                'val_TN': val_TN,\n",
    "                'val_FP': val_FP,\n",
    "                'val_FN': val_FN,\n",
    "                **xgb_params\n",
    "                }\n",
    "        else:\n",
    "            f1_train = sum(F1_l2_dict_train.values()) / len(F1_l2_dict_train.values())\n",
    "            f1_val = sum(F1_l2_dict_val.values()) / len(F1_l2_dict_val.values())\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                **xgb_params\n",
    "            }\n",
    "\n",
    "        # Update tqdm description with the current F1 score\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - F1 Score (Val): {f1_val:.4f}\")\n",
    "        # tqdm.set_description(f\"Run {_ + 1}/{n_runs} - F1 Val: {f1_val:.4f}\")\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'num_boost_round': (1, 4),  # Number of boosting rounds\n",
    "    'max_depth': (3, 20),  # Maximum tree depth for base learners\n",
    "    'eta': (0.05, 0.4),  # Boosting learning rate (xgb's \"eta\")\n",
    "    'min_child_weight': (1, 20),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'subsample': [0.5, 0.75, 1.0],  # Subsample ratio of the training instance\n",
    "    'n_rounds': (2, 8),\n",
    "    # 'max_leaves': (1, 100),  # Maximum number of leaves; 0 indicates no limit\n",
    "    # 'max_bin': (256, 512),  # Maximum number of bins per feature for histogram-based algorithm\n",
    "    # 'grow_policy': ['depthwise', 'lossguide'],  # Tree growing policy\n",
    "    # 'learning_rate': (0.01, 0.3),  # Boosting learning rate (eta)\n",
    "    # 'verbosity': [0, 1, 2, 3],  # Degree of verbosity (0: silent, 1: warning, 2: info, 3: debug)\n",
    "    # 'objective': ['binary:logistic', 'multi:softprob', 'reg:squarederror'],  # Learning objective\n",
    "    'booster': ['gbtree',],# 'gblinear', 'dart'],  # Booster to use\n",
    "    'tree_method': ['hist'],  # Tree method\n",
    "    # 'gamma': (0, 5),  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "    # 'min_child_weight': (0, 10),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    # 'subsample': (0.5, 1.0),  # Subsample ratio of the training instance\n",
    "    # 'sampling_method': ['uniform', 'gradient_based'],  # Sampling method (only for GPU version of hist tree method)\n",
    "    # 'colsample_bytree': (0.5, 1.0),  # Subsample ratio of columns when constructing each tree\n",
    "    # 'colsample_bylevel': (0.5, 1.0),  # Subsample ratio of columns for each level\n",
    "    # 'colsample_bynode': (0.5, 1.0),  # Subsample ratio of columns for each split\n",
    "    # 'reg_alpha': (0, 1),  # L1 regularization term on weights\n",
    "    # 'reg_lambda': (1, 10),  # L2 regularization term on weights\n",
    "    # 'scale_pos_weight': (0.1, 10),  # Balancing of positive and negative weights\n",
    "    # 'base_score': (0.5, 0.5),  # The initial prediction score of all instances, global bias\n",
    "    # 'multi_strategy': ['one_output_per_tree', 'multi_output_tree'],\n",
    "    'random_state': [42],  # Random number seed for reproducibility\n",
    "    # 'early_stopping_rounds': (10, 100),  # Number of rounds for early stopping\n",
    "    'device': ['cuda'],#, 'cuda'],  # Device to use\n",
    "    'train_set': ['standard', 'daily', 'weekly', 'monthly', 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/standard']  # List of options\n",
    "}\n",
    "\n",
    "param_grid_xgb_1 = {\n",
    "    'n_estimators': (4, 50),  # Number of boosting rounds\n",
    "    'max_depth': (3, 100),  # Maximum tree depth for base learners\n",
    "    'eta': (0.05, 0.4),  # Boosting learning rate (xgb's \"eta\")\n",
    "    'random_state': [42],  # Random number seed for reproducibility\n",
    "    'device': ['cuda'],#, 'cuda'],  # Device to use\n",
    "    'train_set': ['standard', 'daily', 'weekly', 'monthly', 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/standard']  # List of options\n",
    "    }\n",
    "\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "# print_memory_usage()\n",
    "# print_gpu_memory()\n",
    "random_search(None, df_targets, param_grid_xgb_1, n_runs=5, target=\"res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "import multiprocessing\n",
    "import gc\n",
    "import psutil\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "def random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\"):\n",
    "    if target == \"building_stock_type\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"HP_results/multixgboost/multi_results.csv\"\n",
    "        y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    elif target == \"com\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"\\HP_results/multixgboost/multi_com_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "        label_encoders = {}\n",
    "        for col in y.columns:\n",
    "            le = LabelEncoder()\n",
    "            y[col] = le.fit_transform(y[col])\n",
    "            label_encoders[col] = le\n",
    "    elif target == \"res\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"\\HP_results/multixgboost/multi_res_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "        label_encoders = {}\n",
    "        for col in y.columns:\n",
    "            le = LabelEncoder()\n",
    "            y[col] = le.fit_transform(y[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Check if results file exists\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        # Create an empty DataFrame if the results file doesn't exist\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'train_set', 'f1_train', 'f1_val', 'n_estimators', 'criterion', 'max_depth', \n",
    "            'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap'\n",
    "        ])\n",
    "\n",
    "    for _ in tqdm(range(n_runs), file=sys.stdout, desc=\"Running Random Search\"):\n",
    "        # print(f\"Run {_ + 1}/{n_runs}\")\n",
    "        # sample parameters\n",
    "        params = sample_hyperparameters(param_grid)\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - Params: {params}\")\n",
    "        train_set = params['train_set']\n",
    "        if df_features_dict is None:\n",
    "            df_features = pd.read_parquet(base_path + f'/preprocessed_data/{train_set}_data.parquet', engine='pyarrow')\n",
    "            df_features.sort_index(inplace=True)\n",
    "        else:\n",
    "            df_features = df_features_dict[train_set].copy()\n",
    "        \n",
    "        if target == \"building_stock_type\":\n",
    "            X = df_features\n",
    "        elif target == \"com\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'commercial']\n",
    "        elif target == \"res\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'residential']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        xgb_params = {key: value for key, value in params.items() if key != 'train_set'}\n",
    "        if params[\"device\"] == \"cuda\":\n",
    "            X_train = cp.array(X_train)\n",
    "            X_val = cp.array(X_val)\n",
    "            y_train = cp.array(y_train)\n",
    "            y_val = cp.array(y_val)\n",
    "            \n",
    "            for key, value in xgb_params.items():\n",
    "                if isinstance(value, np.int32) or isinstance(value, np.int64):\n",
    "                    xgb_params[key] = int(value)\n",
    "                elif isinstance(value, np.float32) or isinstance(value, np.float64):\n",
    "                    xgb_params[key] = float(value)\n",
    "                elif isinstance(value, np.str_):\n",
    "                    xgb_params[key] = str(value)\n",
    "        clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                            ('scaler', StandardScaler(), slice(0, X_train.shape[1])),\n",
    "                            ('encoder', OneHotEncoder(), [])  # No need to change the encoder for GPU\n",
    "                        ])),\n",
    "                        ('classifier', MultiOutputClassifier(\n",
    "                            xgb.XGBClassifier(**xgb_params),))\n",
    "                    ])\n",
    "\n",
    "        if params[\"device\"] == \"cuda\":\n",
    "            clf.fit(cp.asnumpy(X_train), cp.asnumpy(y_train))  # XGBoost expects NumPy input\n",
    "            print(\"Fitted\")\n",
    "            # d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "            # d_val = xgb.DMatrix(X_val, label=y_val)\n",
    "            y_train_pred = clf.predict(cp.asnumpy(X_train))\n",
    "            y_val_pred = clf.predict(cp.asnumpy(X_val))\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_train_pred = clf.predict(X_train)\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "\n",
    "        if target == \"building_stock_type\":\n",
    "            # Evaluate on the training set\n",
    "            f1_train = f1_score(y_train, y_train, average='macro')\n",
    "            train_conf_matrix = confusion_matrix(y_train, y_train)\n",
    "            train_TN, train_FP, train_FN, train_TP = train_conf_matrix.ravel()\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
    "            val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the validation set confusion matrix\n",
    "            val_TN, val_FP, val_FN, val_TP = val_conf_matrix.ravel()\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                'train_TP': train_TP,\n",
    "                'train_TN': train_TN,\n",
    "                'train_FP': train_FP,\n",
    "                'train_FN': train_FN,\n",
    "                'val_TP': val_TP,\n",
    "                'val_TN': val_TN,\n",
    "                'val_FP': val_FP,\n",
    "                'val_FN': val_FN,\n",
    "                **xgb_params\n",
    "                }\n",
    "        else:\n",
    "            y_train_pred = pd.DataFrame(y_train_pred, columns=[f'class_{i}' for i in range(y_train.shape[1])])\n",
    "            y_val_pred = pd.DataFrame(y_val_pred, columns=[f'class_{i}' for i in range(y_val.shape[1])])\n",
    "\n",
    "            if params[\"device\"] == \"cuda\":\n",
    "                F1_l2_dict_train = {column: f1_score(cp.asnumpy(y_train[:, i]), y_train_pred.iloc[:, i], average='macro') for i, column in enumerate(y_train_pred.columns)}\n",
    "                F1_l2_dict_val = {column: f1_score(cp.asnumpy(y_val[:, i]), y_val_pred.iloc[:, i], average='macro') for i, column in enumerate(y_val_pred.columns)}\n",
    "            else:\n",
    "                F1_l2_dict_train = {column: f1_score(y_train.iloc[:, i], y_train_pred.iloc[:, i], average='macro') for i, column in enumerate(y_train_pred.columns)}\n",
    "                F1_l2_dict_val = {column: f1_score(y_val.iloc[:, i], y_val_pred.iloc[:, i], average='macro') for i, column in enumerate(y_val_pred.columns)}\n",
    "            f1_train = sum(F1_l2_dict_train.values()) / len(F1_l2_dict_train.values())\n",
    "            f1_val = sum(F1_l2_dict_val.values()) / len(F1_l2_dict_val.values())\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                **xgb_params\n",
    "            }\n",
    "            if params[\"device\"] == \"cuda\":\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "            del clf\n",
    "            del X_train, X_val, y_train, y_val\n",
    "            del df_features, X, y_train_pred, y_val_pred, F1_l2_dict_train, F1_l2_dict_val\n",
    "            gc.collect()\n",
    "            print_memory_usage()\n",
    "        # Update tqdm description with the current F1 score\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - F1 Score (Val): {f1_val:.4f}\")\n",
    "        # tqdm.set_description(f\"Run {_ + 1}/{n_runs} - F1 Val: {f1_val:.4f}\")\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': (4, 30),  # Number of boosting rounds\n",
    "    'max_depth': (3, 20),  # Maximum tree depth for base learners\n",
    "    # 'max_leaves': (1, 100),  # Maximum number of leaves; 0 indicates no limit\n",
    "    # 'max_bin': (256, 512),  # Maximum number of bins per feature for histogram-based algorithm\n",
    "    # 'grow_policy': ['depthwise', 'lossguide'],  # Tree growing policy\n",
    "    # 'learning_rate': (0.01, 0.3),  # Boosting learning rate (eta)\n",
    "    # 'verbosity': [0, 1, 2, 3],  # Degree of verbosity (0: silent, 1: warning, 2: info, 3: debug)\n",
    "    # 'objective': ['binary:logistic', 'multi:softprob', 'reg:squarederror'],  # Learning objective\n",
    "    # 'booster': ['gbtree', 'gblinear', 'dart'],  # Booster to use\n",
    "    'tree_method': ['hist'],  # Tree method\n",
    "    # 'gamma': (0, 5),  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "    # 'min_child_weight': (0, 10),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    # 'subsample': (0.5, 1.0),  # Subsample ratio of the training instance\n",
    "    # 'sampling_method': ['uniform', 'gradient_based'],  # Sampling method (only for GPU version of hist tree method)\n",
    "    # 'colsample_bytree': (0.5, 1.0),  # Subsample ratio of columns when constructing each tree\n",
    "    # 'colsample_bylevel': (0.5, 1.0),  # Subsample ratio of columns for each level\n",
    "    # 'colsample_bynode': (0.5, 1.0),  # Subsample ratio of columns for each split\n",
    "    # 'reg_alpha': (0, 1),  # L1 regularization term on weights\n",
    "    # 'reg_lambda': (1, 10),  # L2 regularization term on weights\n",
    "    # 'scale_pos_weight': (0.1, 10),  # Balancing of positive and negative weights\n",
    "    # 'base_score': (0.5, 0.5),  # The initial prediction score of all instances, global bias\n",
    "    'random_state': [42],  # Random number seed for reproducibility\n",
    "    # 'early_stopping_rounds': (10, 100),  # Number of rounds for early stopping\n",
    "    'device': ['cuda'],#, 'cuda'],  # Device to use\n",
    "    'train_set': ['standard', 'daily', 'weekly', 'monthly', 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/standard']  # List of options\n",
    "}\n",
    "\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "print_memory_usage()\n",
    "print_gpu_memory()\n",
    "random_search(None, df_targets, param_grid_xgb, n_runs=10, target=\"res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:02:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:02:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:03:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:03:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:03:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:03:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:04:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:04:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:05:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:05:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:05:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:05:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:06:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:06:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:06:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:06:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:07:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:07:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:08:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:08:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:10:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:10:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:12:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:12:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:12:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:12:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:12:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:12:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:12:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1: 0.9976\n",
      "Validation F1: 0.2391\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "df_features_full = df_features_dict['with_regional/daily']\n",
    "s=\"com\"\n",
    "if s==\"res\":\n",
    "    X = df_features_full[df_targets['building_stock_type'] == 'residential']\n",
    "    y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "if s==\"com\":\n",
    "    X = df_features_full[df_targets['building_stock_type'] == 'commercial']\n",
    "    y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "\n",
    "label_encoders = {}\n",
    "for col in y.columns:\n",
    "    le = LabelEncoder()\n",
    "    y[col] = le.fit_transform(y[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "#         ('scaler', StandardScaler(), X_train.columns),\n",
    "#         ('encoder', OneHotEncoder(), [])\n",
    "#     ])),\n",
    "#     ('classifier', MultiOutputClassifier(xgb.XGBClassifier(n_estimators=50, device=\"cuda\", tree_method=\"hist\",# verbose=\"1\",\n",
    "#                                                            max_depth=12,  # Reduce tree depth<\n",
    "#                                                            )))\n",
    "#                                                            ])\n",
    "\n",
    "# Convert training and validation sets to CuPy arrays for GPU\n",
    "X_train = cp.array(X_train)\n",
    "X_val = cp.array(X_val)\n",
    "y_train = cp.array(y_train)\n",
    "y_val = cp.array(y_val)\n",
    "\n",
    "# Set up pipeline\n",
    "clf = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('scaler', StandardScaler(), slice(0, X_train.shape[1])),\n",
    "        ('encoder', OneHotEncoder(), [])  # No need to change the encoder for GPU\n",
    "    ])),\n",
    "    ('classifier', MultiOutputClassifier(\n",
    "        xgb.XGBClassifier(n_estimators=20,\n",
    "                          tree_method='hist',\n",
    "                          max_depth=8,\n",
    "                          verbosity=1,\n",
    "                          random_state=42),))\n",
    "])\n",
    "# Train and evaluate the model\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# y_train_pred = clf.predict(X_train)\n",
    "# y_val_pred = clf.predict(X_val)\n",
    "\n",
    "clf.fit(cp.asnumpy(X_train), cp.asnumpy(y_train))  # XGBoost expects NumPy input\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = clf.predict(cp.asnumpy(X_train))\n",
    "y_val_pred = clf.predict(cp.asnumpy(X_val))\n",
    "\n",
    "# y_train_pred = pd.DataFrame(y_train_pred, columns=y.columns)\n",
    "# y_val_pred = pd.DataFrame(y_val_pred, columns=y.columns)\n",
    "\n",
    "# # Evaluate on the training set\n",
    "# F1_l2_dict = {column: 0 for column in y.columns}\n",
    "# for col in y.columns:\n",
    "#     F1_l2_dict[col] = f1_score(y_train[col], y_train_pred[col], average=\"macro\")\n",
    "# f1_train = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "\n",
    "# # Evaluate on the validation set\n",
    "# F1_l2_dict = {column: 0 for column in y.columns}\n",
    "# for col in y.columns:\n",
    "#     F1_l2_dict[col] = f1_score(y_val[col], y_val_pred[col], average=\"macro\")\n",
    "# f1_val = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "# print(f\"Training F1: {f1_train:.4f}\")\n",
    "# print(f\"Validation F1: {f1_val:.4f}\")\n",
    "\n",
    "# Convert predictions back to DataFrame for evaluation\n",
    "y_train_pred = pd.DataFrame(y_train_pred, columns=[f'class_{i}' for i in range(y_train.shape[1])])\n",
    "y_val_pred = pd.DataFrame(y_val_pred, columns=[f'class_{i}' for i in range(y_val.shape[1])])\n",
    "\n",
    "# Evaluate on the training set\n",
    "F1_l2_dict_train = {column: f1_score(cp.asnumpy(y_train[:, i]), y_train_pred.iloc[:, i], average='macro') for i, column in enumerate(y_train_pred.columns)}\n",
    "f1_train = sum(F1_l2_dict_train.values()) / len(F1_l2_dict_train.values())\n",
    "\n",
    "# Evaluate on the validation set\n",
    "F1_l2_dict_val = {column: f1_score(cp.asnumpy(y_val[:, i]), y_val_pred.iloc[:, i], average='macro') for i, column in enumerate(y_val_pred.columns)}\n",
    "f1_val = sum(F1_l2_dict_val.values()) / len(F1_l2_dict_val.values())\n",
    "\n",
    "print(f\"Training F1: {f1_train:.4f}\")\n",
    "print(f\"Validation F1: {f1_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def sample_hyperparameters(param_grid):\n",
    "    \"\"\"\n",
    "    Sample hyperparameters from the given grid using scipy.stats.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    param_grid : dict\n",
    "        A dictionary where keys are hyperparameter names and values are lists of options or ranges.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with sampled hyperparameters.\n",
    "    \"\"\"\n",
    "    sampled_params = {}\n",
    "    \n",
    "    for param, values in param_grid.items():\n",
    "        if isinstance(values, list):\n",
    "            # Randomly choose from list of options\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        elif isinstance(values, tuple) and len(values) == 2:\n",
    "            min_val, max_val = values\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                # Sample integer values using scipy.stats.randint\n",
    "                sampled_params[param] = stats.randint.rvs(min_val, max_val + 1)\n",
    "            elif isinstance(min_val, float) and isinstance(max_val, float):\n",
    "                # Sample float values using scipy.stats.uniform\n",
    "                sampled_params[param] = stats.uniform.rvs(min_val, max_val - min_val)\n",
    "        elif isinstance(values, str) and values == 'choice':\n",
    "            # Sample from a list of options if 'choice' is specified\n",
    "            sampled_params[param] = np.random.choice(param_grid[param])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported parameter type for {param}: {values}\")\n",
    "    \n",
    "    # Adjust parameters for RandomForestClassifier\n",
    "    if not sampled_params.get('bootstrap', True):\n",
    "        sampled_params['max_samples'] = None  # Reset max_samples if bootstrap is False\n",
    "        # sampled_params['oob_score'] = False\n",
    "    # Ensure max_features is correctly set\n",
    "    max_features = sampled_params.get('max_features')\n",
    "    if isinstance(max_features, str) and max_features.startswith('0'):\n",
    "        sampled_params['max_features'] = float(max_features)\n",
    "        \n",
    "    \n",
    "    return sampled_params\n",
    "\n",
    "\n",
    "def random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\"):\n",
    "    if target == \"building_stock_type\":\n",
    "        results_file = \"results.csv\"\n",
    "        y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    elif target == \"com\":\n",
    "        results_file = \"com_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    elif target == \"res\":\n",
    "        results_file = \"res_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    \n",
    "    # Check if results file exists\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        # Create an empty DataFrame if the results file doesn't exist\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'train_set', 'f1_train', 'f1_val', 'n_estimators', 'criterion', 'max_depth', \n",
    "            'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap'\n",
    "        ])\n",
    "\n",
    "    for _ in tqdm(range(n_runs), file=sys.stdout, desc=\"Running Random Search\"):\n",
    "        # print(f\"Run {_ + 1}/{n_runs}\")\n",
    "        # sample parameters\n",
    "        params = sample_hyperparameters(param_grid)\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - Params: {params}\")\n",
    "        train_set = params['train_set']\n",
    "        df_features = df_features_dict[train_set].copy()\n",
    "        if target == \"building_stock_type\":\n",
    "            X = df_features\n",
    "        elif target == \"com\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'commercial']\n",
    "        elif target == \"res\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'residential']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        rf_params = {key: value for key, value in params.items() if key != 'train_set'}\n",
    "        clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                ('scaler', StandardScaler(), df_features.columns),\n",
    "                ('encoder', OneHotEncoder(), [])\n",
    "            ])),\n",
    "            ('classifier', xgb.XGBClassifier(n_jobs=multiprocessing.cpu_count() // 2, tree_method=\"hist\"))])\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_val_pred = clf.predict(X_val)\n",
    "\n",
    "        if target == \"building_stock_type\":\n",
    "            # Evaluate on the training set\n",
    "            f1_train = f1_score(y_train, y_train, average='macro')\n",
    "            train_conf_matrix = confusion_matrix(y_train, y_train)\n",
    "            train_TN, train_FP, train_FN, train_TP = train_conf_matrix.ravel()\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
    "            val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the validation set confusion matrix\n",
    "            val_TN, val_FP, val_FN, val_TP = val_conf_matrix.ravel()\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                'train_TP': train_TP,\n",
    "                'train_TN': train_TN,\n",
    "                'train_FP': train_FP,\n",
    "                'train_FN': train_FN,\n",
    "                'val_TP': val_TP,\n",
    "                'val_TN': val_TN,\n",
    "                'val_FP': val_FP,\n",
    "                'val_FN': val_FN,\n",
    "                **rf_params\n",
    "                }\n",
    "        else:\n",
    "            y_train_pred = pd.DataFrame(y_train_pred, columns=y.columns)\n",
    "            y_val_pred = pd.DataFrame(y_val_pred, columns=y.columns)\n",
    "\n",
    "            # Evaluate on the training set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_train[col], y_train_pred[col], average=\"macro\")\n",
    "            f1_train = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_val[col], y_val_pred[col], average=\"macro\")\n",
    "            f1_val = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "        \n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                **rf_params\n",
    "            }\n",
    "        # Update tqdm description with the current F1 score\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - F1 Score (Val): {f1_val:.4f}\")\n",
    "        # tqdm.set_description(f\"Run {_ + 1}/{n_runs} - F1 Val: {f1_val:.4f}\")\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'n_estimators': (10, 1000),  # Integer range\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # List of options\n",
    "    'max_depth': (1, 100),  # Integer range\n",
    "    'min_samples_split': (2, 10),  # Integer range\n",
    "    'min_samples_leaf': (1, 10),  # Integer range\n",
    "    'max_features': ['sqrt', 'log2'],  # List of options\n",
    "    'bootstrap': [True, False],  # List of options\n",
    "    'train_set': ['full', 'daily', 'weekly', 'monthly', 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/full']  # List of options\n",
    "}\n",
    "\n",
    "# random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "import lightgbm as lgb\n",
    "import cupy as cp\n",
    "X = cp.array(X)\n",
    "y = cp.array(y)\n",
    "y -= y.min()\n",
    "num_round = 3000\n",
    "\n",
    "# Leave most parameters as default\n",
    "clf = xgb.XGBClassifier(device=\"cuda\", n_estimators=num_round)\n",
    "# Train model\n",
    "start = time.time()\n",
    "clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "gpu_res = clf.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Import the data sets\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "base_path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\"\n",
    "data_path = r\"\\data\\building-instinct-starter-notebook\\Starter notebook\"\n",
    "preprocessing_path = r\"\\kai\\preprocessing\"\n",
    "sys.path.append(base_path+data_path)\n",
    "sys.path.append(base_path+\"\\kai\")\n",
    "sys.path.append(base_path+preprocessing_path)\n",
    "from preprocessing.preprocessing import Preprocessor\n",
    "\n",
    "df_features_dict = {}\n",
    "\n",
    "for s in [\"monthly\", \"weekly\", \"daily\"]:\n",
    "    df_features = pd.read_parquet(base_path + f'/preprocessed_data/{s}_data.parquet', engine='pyarrow')\n",
    "    df_features.sort_index(inplace=True)\n",
    "    df_features_dict[s] = df_features\n",
    "\n",
    "# Full Dataset\n",
    "df_features_full = pd.read_parquet(base_path + '/preprocessed_data/standard_data.parquet', engine='pyarrow')\n",
    "df_features_full.sort_index(inplace=True)\n",
    "df_features_dict['full'] = df_features_full\n",
    "\n",
    "# Labels\n",
    "load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def sample_hyperparameters(param_grid):\n",
    "    \"\"\"\n",
    "    Sample hyperparameters from the given grid using scipy.stats.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    param_grid : dict\n",
    "        A dictionary where keys are hyperparameter names and values are lists of options or ranges.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with sampled hyperparameters.\n",
    "    \"\"\"\n",
    "    sampled_params = {}\n",
    "    \n",
    "    for param, values in param_grid.items():\n",
    "        if isinstance(values, list):\n",
    "            # Randomly choose from list of options\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        elif isinstance(values, tuple) and len(values) == 2:\n",
    "            min_val, max_val = values\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                # Sample integer values using scipy.stats.randint\n",
    "                sampled_params[param] = stats.randint.rvs(min_val, max_val + 1)\n",
    "            elif isinstance(min_val, float) and isinstance(max_val, float):\n",
    "                # Sample float values using scipy.stats.uniform\n",
    "                sampled_params[param] = stats.uniform.rvs(min_val, max_val - min_val)\n",
    "        elif isinstance(values, str) and values == 'choice':\n",
    "            # Sample from a list of options if 'choice' is specified\n",
    "            sampled_params[param] = np.random.choice(param_grid[param])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported parameter type for {param}: {values}\")\n",
    "    \n",
    "    # Adjust parameters for RandomForestClassifier\n",
    "    if not sampled_params.get('bootstrap', True):\n",
    "        sampled_params['max_samples'] = None  # Reset max_samples if bootstrap is False\n",
    "        # sampled_params['oob_score'] = False\n",
    "    # Ensure max_features is correctly set\n",
    "    max_features = sampled_params.get('max_features')\n",
    "    if isinstance(max_features, str) and max_features.startswith('0'):\n",
    "        sampled_params['max_features'] = float(max_features)\n",
    "        \n",
    "    \n",
    "    return sampled_params\n",
    "\n",
    "\n",
    "def random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\"):\n",
    "    if target == \"building_stock_type\":\n",
    "        results_file = \"results.csv\"\n",
    "        y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    elif target == \"com\":\n",
    "        results_file = \"com_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    elif target == \"res\":\n",
    "        results_file = \"res_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    \n",
    "    # Check if results file exists\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        # Create an empty DataFrame if the results file doesn't exist\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'train_set', 'f1_train', 'f1_val', 'n_estimators', 'criterion', 'max_depth', \n",
    "            'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap'\n",
    "        ])\n",
    "\n",
    "    for _ in tqdm(range(n_runs), file=sys.stdout, desc=\"Running Random Search\"):\n",
    "        # print(f\"Run {_ + 1}/{n_runs}\")\n",
    "        # sample parameters\n",
    "        params = sample_hyperparameters(param_grid)\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - Params: {params}\")\n",
    "        train_set = params['train_set']\n",
    "        df_features = df_features_dict[train_set].copy()\n",
    "        if target == \"building_stock_type\":\n",
    "            X = df_features\n",
    "        elif target == \"com\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'commercial']\n",
    "        elif target == \"res\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'residential']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        rf_params = {key: value for key, value in params.items() if key != 'train_set'}\n",
    "        clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                ('scaler', StandardScaler(), df_features.columns),\n",
    "                ('encoder', OneHotEncoder(), [])\n",
    "            ])),\n",
    "            ('classifier', RandomForestClassifier(**rf_params, random_state=42))\n",
    "        ])\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_val_pred = clf.predict(X_val)\n",
    "\n",
    "        if target == \"building_stock_type\":\n",
    "            # Evaluate on the training set\n",
    "            f1_train = f1_score(y_train, y_train, average='macro')\n",
    "            train_conf_matrix = confusion_matrix(y_train, y_train)\n",
    "            train_TN, train_FP, train_FN, train_TP = train_conf_matrix.ravel()\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
    "            val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the validation set confusion matrix\n",
    "            val_TN, val_FP, val_FN, val_TP = val_conf_matrix.ravel()\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                'train_TP': train_TP,\n",
    "                'train_TN': train_TN,\n",
    "                'train_FP': train_FP,\n",
    "                'train_FN': train_FN,\n",
    "                'val_TP': val_TP,\n",
    "                'val_TN': val_TN,\n",
    "                'val_FP': val_FP,\n",
    "                'val_FN': val_FN,\n",
    "                **rf_params\n",
    "                }\n",
    "        else:\n",
    "            y_train_pred = pd.DataFrame(y_train_pred, columns=y.columns)\n",
    "            y_val_pred = pd.DataFrame(y_val_pred, columns=y.columns)\n",
    "\n",
    "            # Evaluate on the training set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_train[col], y_train_pred[col], average=\"macro\")\n",
    "            f1_train = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_val[col], y_val_pred[col], average=\"macro\")\n",
    "            f1_val = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "        \n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                **rf_params\n",
    "            }\n",
    "        # Update tqdm description with the current F1 score\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - F1 Score (Val): {f1_val:.4f}\")\n",
    "        # tqdm.set_description(f\"Run {_ + 1}/{n_runs} - F1 Val: {f1_val:.4f}\")\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'n_estimators': (10, 1000),  # Integer range\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # List of options\n",
    "    'max_depth': (1, 100),  # Integer range\n",
    "    'min_samples_split': (2, 10),  # Integer range\n",
    "    'min_samples_leaf': (1, 10),  # Integer range\n",
    "    'max_features': ['sqrt', 'log2'],  # List of options\n",
    "    'bootstrap': [True, False],  # List of options\n",
    "    'train_set': ['full', 'daily', 'weekly', 'monthly']  # List of options\n",
    "}\n",
    "\n",
    "# random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commercial and residential columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/30 - Params: {'n_estimators': 358, 'criterion': 'log_loss', 'max_depth': 35, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'weekly', 'n_jobs': -1}\n",
      "Run 1/30 - F1 Score (Val): 0.2786                            \n",
      "Run 2/30 - Params: {'n_estimators': 443, 'criterion': 'log_loss', 'max_depth': 32, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'weekly', 'n_jobs': -1}\n",
      "Run 2/30 - F1 Score (Val): 0.2821                                    \n",
      "Run 3/30 - Params: {'n_estimators': 428, 'criterion': 'log_loss', 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'train_set': 'monthly', 'n_jobs': -1, 'max_samples': None}\n",
      "Run 3/30 - F1 Score (Val): 0.2595                                       \n",
      "Run 4/30 - Params: {'n_estimators': 448, 'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'weekly', 'n_jobs': -1, 'max_samples': None}\n",
      "Run 4/30 - F1 Score (Val): 0.2776                                       \n",
      "Run 5/30 - Params: {'n_estimators': 375, 'criterion': 'gini', 'max_depth': 31, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': False, 'train_set': 'weekly', 'n_jobs': -1, 'max_samples': None}\n",
      "Run 5/30 - F1 Score (Val): 0.2883                                     \n",
      "Run 6/30 - Params: {'n_estimators': 234, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': False, 'train_set': 'monthly', 'n_jobs': -1, 'max_samples': None}\n",
      "Run 6/30 - F1 Score (Val): 0.2900                                    \n",
      "Run 7/30 - Params: {'n_estimators': 160, 'criterion': 'entropy', 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'weekly', 'n_jobs': -1}\n",
      "Run 7/30 - F1 Score (Val): 0.2812                                    \n",
      "Run 8/30 - Params: {'n_estimators': 365, 'criterion': 'entropy', 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True, 'train_set': 'weekly', 'n_jobs': -1}\n",
      "Run 8/30 - F1 Score (Val): 0.2810                                    \n",
      "Run 9/30 - Params: {'n_estimators': 480, 'criterion': 'entropy', 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'monthly', 'n_jobs': -1}\n",
      "Run 9/30 - F1 Score (Val): 0.2840                                    \n",
      "Run 10/30 - Params: {'n_estimators': 302, 'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': True, 'train_set': 'full', 'n_jobs': -1}\n",
      "Running Random Search:  30%|███       | 9/30 [14:27<32:10, 91.92s/it]"
     ]
    }
   ],
   "source": [
    "def calculate_hierarchical_f1_score(df_targets, df_pred, alpha=0.4, average='macro', F1_list=False):\n",
    "    \"\"\"\n",
    "    Calculate the hierarchical F1-score for a multi-level classification problem.\n",
    "\n",
    "    This function computes the F1-score at two hierarchical levels:\n",
    "    1. The 'building_stock_type' level, which is the first level of hierarchy.\n",
    "    2. The second level, which is conditional on the 'building_stock_type' being either 'commercial' or 'residential'.\n",
    "\n",
    "    The final F1-score is a weighted average of the first level and second level F1-scores.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df_targets : pd.DataFrame\n",
    "        The dataframe containing the true target values. It must include a column 'building_stock_type' and other\n",
    "        columns ending with '_com' or '_res' representing the second level of classification.\n",
    "\n",
    "    df_pred : pd.DataFrame\n",
    "        The dataframe containing the predicted values. It must be structured similarly to `df_targets`.\n",
    "\n",
    "    alpha : float, optional, default=0.3\n",
    "        The weight given to the first level F1-score in the final score calculation. The weight for the second level\n",
    "        F1-score will be (1 - alpha).\n",
    "\n",
    "    average : str, optional, default='macro'\n",
    "        The averaging method for calculating the F1-score. It is passed directly to the `f1_score` function from sklearn.\n",
    "\n",
    "    F1_list : bool, optional, default=False\n",
    "        If True, the function returns a dictionary of F1-scores for all individual columns along with the overall F1-score.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float or tuple\n",
    "        If `F1_list` is False, returns a single float representing the overall hierarchical F1-score.\n",
    "        If `F1_list` is True, returns a tuple where the first element is the overall hierarchical F1-score and the second\n",
    "        element is a dictionary containing the F1-scores for all individual columns.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_f1_l2(df_targets, df_pred, average):\n",
    "        \"\"\"\n",
    "        Calculate the F1-score for the second level of hierarchy.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_targets : pd.DataFrame\n",
    "            The dataframe containing the true target values for the second level of hierarchy.\n",
    "        df_pred : pd.DataFrame\n",
    "            The dataframe containing the predicted values for the second level of hierarchy.\n",
    "        average : str\n",
    "            The averaging method for calculating the F1-score.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary where keys are column names and values are the corresponding F1-scores.\n",
    "        \"\"\"\n",
    "        F1_l2_dict = {column: 0 for column in df_targets.columns}\n",
    "\n",
    "        # Find the intersection of indices\n",
    "        common_indices = df_targets.index.intersection(df_pred.index)\n",
    "\n",
    "        # Check if the intersection is empty\n",
    "        if common_indices.empty:\n",
    "            return F1_l2_dict\n",
    "        else:\n",
    "            # Select only the rows with common indices\n",
    "            df_targets_common = df_targets.loc[common_indices]\n",
    "            df_pred_common = df_pred.loc[common_indices]\n",
    "\n",
    "            # Calculate the F1-score for each column based on the common rows\n",
    "            for column in df_targets.columns:\n",
    "                F1_l2_dict[column] = f1_score(df_targets_common[column], df_pred_common[column], average=average)\n",
    "\n",
    "        return F1_l2_dict\n",
    "\n",
    "    # Sort both dataframes based on index\n",
    "    df_targets = df_targets.sort_index()\n",
    "    df_pred = df_pred.sort_index()\n",
    "\n",
    "    # Calculate F1 score for the first level of hierarchy\n",
    "    F1_l1 = f1_score(df_targets['building_stock_type'], df_pred['building_stock_type'], average=average)\n",
    "    F1_dict = {'building_stock_type': F1_l1}\n",
    "\n",
    "    # Calculate F1 score for the second level of hierarchy (commercial buildings)\n",
    "    df_com_targets = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    df_com_pred = df_pred[df_pred['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    F1_l2_dict_com = calculate_f1_l2(df_com_targets, df_com_pred, average)\n",
    "    F1_l2_com = sum(F1_l2_dict_com.values()) / len(F1_l2_dict_com.values())\n",
    "\n",
    "    F1_l2_dict = {}\n",
    "    F1_l2_dict.update(F1_l2_dict_com)\n",
    "\n",
    "    # Calculate F1 score for the second level of hierarchy (residential buildings)\n",
    "    df_res_targets = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    df_res_pred = df_pred[df_pred['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    F1_l2_dict_res = calculate_f1_l2(df_res_targets, df_res_pred, average)\n",
    "    F1_l2_res = sum(F1_l2_dict_res.values()) / len(F1_l2_dict_res.values())\n",
    "\n",
    "    F1_l2_dict.update(F1_l2_dict_res)\n",
    "    F1_l2_dict_sorted = sorted(F1_l2_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    F1_dict.update(F1_l2_dict_sorted)\n",
    "\n",
    "    # Calculate F1 score for the second level of hierarchy\n",
    "    F1_l2 = (F1_l2_com + F1_l2_res) / 2\n",
    "\n",
    "    # Calculate overall F1 score\n",
    "    F1 = alpha * F1_l1 + (1 - alpha) * F1_l2\n",
    "\n",
    "    if F1_list:\n",
    "        return F1, F1_dict\n",
    "\n",
    "    return F1\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'n_estimators': (100, 500),  # Integer range\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # List of options\n",
    "    'max_depth': (5, 40),  # Integer range\n",
    "    'min_samples_split': (2, 10),  # Integer range\n",
    "    'min_samples_leaf': (1, 5),  # Integer range\n",
    "    'max_features': ['sqrt', 'log2'],#, 0.5, 0.8],  # List of options None,\n",
    "    'bootstrap': [True, False],  # List of options\n",
    "    'train_set': ['full', 'daily', 'weekly', 'monthly'],  # List of options\n",
    "    'n_jobs': [-1],\n",
    "    'random_state': [1, 420],\n",
    "    # 'min_weight_fraction_leaf': (0.0, 0.5),  # Float range\n",
    "    # 'max_leaf_nodes': (10, 1000),  # Integer range\n",
    "    # 'min_impurity_decrease': (0.0, 0.1),  # Float range\n",
    "    # # 'oob_score': [True, False],  # List of options\n",
    "    # 'class_weight': [None, 'balanced', 'balanced_subsample'],  # Class weights\n",
    "    # 'ccp_alpha': (0.0, 0.1),  # Complexity parameter for pruning\n",
    "    # 'max_samples': [None, 0.5, 0.8, 1.0],  # Fraction or integer number of samples\n",
    "}\n",
    "\n",
    "# Call the function\n",
    "random_search(df_features_dict, df_targets, param_grid, n_runs=30, target=\"res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- xgboost\n",
    "- MulticolumnClassifier\n",
    "- check submission quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "def multi_random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"building_stock_type\"):\n",
    "    if target == \"building_stock_type\":\n",
    "        results_file = \"multi_results.csv\"\n",
    "        y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    elif target == \"com\":\n",
    "        results_file = \"multi_com_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "    elif target == \"res\":\n",
    "        results_file = \"multi_res_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "    \n",
    "    # Check if results file exists\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        # Create an empty DataFrame if the results file doesn't exist\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'train_set', 'f1_train', 'f1_val', 'n_estimators', 'criterion', 'max_depth', \n",
    "            'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap'\n",
    "        ])\n",
    "\n",
    "    for _ in tqdm(range(n_runs), file=sys.stdout, desc=\"Running Random Search\"):\n",
    "        # print(f\"Run {_ + 1}/{n_runs}\")\n",
    "        # sample parameters\n",
    "        params = sample_hyperparameters(param_grid)\n",
    "        train_set = params['train_set']\n",
    "        df_features = df_features_dict[train_set].copy()\n",
    "        if target == \"building_stock_type\":\n",
    "            X = df_features\n",
    "        elif target == \"com\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'commercial']\n",
    "        elif target == \"res\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'residential']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        rf_params = {key: value for key, value in params.items() if key != 'train_set'}\n",
    "        clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                ('scaler', StandardScaler(), df_features.columns),\n",
    "                ('encoder', OneHotEncoder(), [])\n",
    "            ])),\n",
    "            ('classifier', MultiOutputClassifier(RandomForestClassifier(**rf_params, random_state=42), n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_val_pred = clf.predict(X_val)\n",
    "\n",
    "        if target == \"building_stock_type\":\n",
    "            # Evaluate on the training set\n",
    "            f1_train = f1_score(y_train, y_train, average='macro')\n",
    "            train_conf_matrix = confusion_matrix(y_train, y_train)\n",
    "            train_TN, train_FP, train_FN, train_TP = train_conf_matrix.ravel()\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
    "            val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the validation set confusion matrix\n",
    "            val_TN, val_FP, val_FN, val_TP = val_conf_matrix.ravel()\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                'train_TP': train_TP,\n",
    "                'train_TN': train_TN,\n",
    "                'train_FP': train_FP,\n",
    "                'train_FN': train_FN,\n",
    "                'val_TP': val_TP,\n",
    "                'val_TN': val_TN,\n",
    "                'val_FP': val_FP,\n",
    "                'val_FN': val_FN,\n",
    "                **rf_params\n",
    "                }\n",
    "        else:\n",
    "            y_train_pred = pd.DataFrame(y_train_pred, columns=y.columns)\n",
    "            y_val_pred = pd.DataFrame(y_val_pred, columns=y.columns)\n",
    "\n",
    "            # Evaluate on the training set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_train[col], y_train_pred[col], average=\"macro\")\n",
    "            f1_train = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "\n",
    "            # Evaluate on the validation set\n",
    "            F1_l2_dict = {column: 0 for column in y.columns}\n",
    "            for col in y.columns:\n",
    "                F1_l2_dict[col] = f1_score(y_val[col], y_val_pred[col], average=\"macro\")\n",
    "            f1_val = sum(F1_l2_dict.values()) / len(F1_l2_dict.values())\n",
    "        \n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                **rf_params\n",
    "            }\n",
    "        # Update tqdm description with the current F1 score\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - F1 Score (Val): {f1_val:.4f}\")\n",
    "        # tqdm.set_description(f\"Run {_ + 1}/{n_runs} - F1 Val: {f1_val:.4f}\")\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'n_estimators': (100, 500),  # Integer range\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # List of options\n",
    "    'max_depth': (5, 40),  # Integer range\n",
    "    'min_samples_split': (2, 10),  # Integer range\n",
    "    'min_samples_leaf': (1, 10),  # Integer range\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5, 0.8],  # List of options\n",
    "    'bootstrap': [True, False],  # List of options\n",
    "    'train_set': ['full', 'daily', 'weekly', 'monthly'],  # List of options\n",
    "    'n_jobs': [-1],\n",
    "    'min_weight_fraction_leaf': (0.0, 0.5),  # Float range\n",
    "    'max_leaf_nodes': (10, 1000),  # Integer range\n",
    "    'min_impurity_decrease': (0.0, 0.1),  # Float range\n",
    "    # 'oob_score': [True, False],  # List of options\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample'],  # Class weights\n",
    "    'ccp_alpha': (0.0, 0.1),  # Complexity parameter for pruning\n",
    "    'max_samples': [None, 0.5, 0.8, 1.0],  # Fraction or integer number of samples\n",
    "}\n",
    "\n",
    "multi_random_search(df_features_dict, df_targets, param_grid, n_runs=5, target=\"com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"results.csv\")\n",
    "df2 = pd.read_csv(\"com_results.csv\")\n",
    "df3 = pd.read_csv(\"res_results.csv\")\n",
    "display(df1)\n",
    "display(df2)\n",
    "display(df3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
