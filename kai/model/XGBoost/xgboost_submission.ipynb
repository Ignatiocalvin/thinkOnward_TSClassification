{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Import the data sets\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "base_path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\"\n",
    "data_path = r\"\\data\\building-instinct-starter-notebook\\Starter notebook\"\n",
    "preprocessing_path = r\"\\kai\\preprocessing\"\n",
    "sys.path.append(base_path+data_path)\n",
    "sys.path.append(base_path+\"\\kai\")\n",
    "sys.path.append(base_path+preprocessing_path)\n",
    "from preprocessing.preprocessing import Preprocessor\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "import gc\n",
    "from preprocessing.utils import print_memory_usage, print_gpu_memory, free_gpu_memory\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pre_load = False\n",
    "if pre_load:\n",
    "    df_features_dict = {}\n",
    "\n",
    "    for s in [\"monthly\", \"weekly\", \"daily\", \"with_regional/monthly\", \"with_regional/weekly\", \"with_regional/daily\"]:\n",
    "        df_features = pd.read_parquet(base_path + f'/preprocessed_data/{s}_data.parquet', engine='pyarrow')\n",
    "        df_features.sort_index(inplace=True)\n",
    "        df_features_dict[s] = df_features\n",
    "\n",
    "    # Full Dataset\n",
    "    df_features_full = pd.read_parquet(base_path + '/preprocessed_data/standard_data.parquet', engine='pyarrow')\n",
    "    df_features_full.sort_index(inplace=True)\n",
    "    df_features_dict['full'] = df_features_full\n",
    "\n",
    "    # Full with regional Dataset\n",
    "    df_features_full = pd.read_parquet(base_path + '/preprocessed_data/with_regional/standard_data.parquet', engine='pyarrow')\n",
    "    df_features_full.sort_index(inplace=True)\n",
    "    df_features_dict['with_regional/full'] = df_features_full\n",
    "\n",
    "# Labels\n",
    "load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for in.comstock_building_type_group_com\n",
      "Training model for in.heating_fuel_com\n",
      "Training model for in.hvac_category_com\n",
      "Training model for in.number_of_stories_com\n",
      "Training model for in.ownership_type_com\n",
      "Training model for in.vintage_com\n",
      "Training model for in.wall_construction_type_com\n",
      "Training model for in.tstat_clg_sp_f..f_com\n",
      "Training model for in.tstat_htg_sp_f..f_com\n",
      "Training model for in.weekday_opening_time..hr_com\n",
      "Training model for in.weekday_operating_hours..hr_com\n",
      "Training model for in.bedrooms_res\n",
      "Training model for in.cooling_setpoint_res\n",
      "Training model for in.heating_setpoint_res\n",
      "Training model for in.geometry_building_type_recs_res\n",
      "Training model for in.geometry_floor_area_res\n",
      "Training model for in.geometry_foundation_type_res\n",
      "Training model for in.geometry_wall_type_res\n",
      "Training model for in.heating_fuel_res\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[13:44:06] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\io.h:320: bad_malloc: Failed to allocate 4245555648 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 125\u001b[0m\n\u001b[0;32m    120\u001b[0m param_grid_com \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m7\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,}\n\u001b[0;32m    122\u001b[0m param_grid_res \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m7\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.3273831428143833\u001b[39m}\n\u001b[1;32m--> 125\u001b[0m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid_com\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmission_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/submissions/xgboost_submission_31_08.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_from_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 107\u001b[0m, in \u001b[0;36mrun_model\u001b[1;34m(param_grid, param_grid_com, param_grid_res, submission_path, read_from_file)\u001b[0m\n\u001b[0;32m    100\u001b[0m bst \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, ColumnTransformer([\n\u001b[0;32m    101\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler(), \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])),\n\u001b[0;32m    102\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(), [])  \u001b[38;5;66;03m# No need to change the encoder for GPU\u001b[39;00m\n\u001b[0;32m    103\u001b[0m                 ])),\n\u001b[0;32m    104\u001b[0m                 (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m,xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam_grid_res),)\n\u001b[0;32m    105\u001b[0m             ])\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# bst = XGBClassifier(**res_params)\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_res_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m y_res_pred_col \u001b[38;5;241m=\u001b[39m bst\u001b[38;5;241m.\u001b[39mpredict(X_res_test)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# y_res_pred_col = cp.asnumpy(y_res_pred_col)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\sklearn\\pipeline.py:475\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    474\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 475\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:2100\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2100\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\xgboost\\core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [13:44:06] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\io.h:320: bad_malloc: Failed to allocate 4245555648 bytes."
     ]
    }
   ],
   "source": [
    "from preprocessing.run import create_submission\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "def run_model(param_grid, param_grid_com, param_grid_res, submission_path=\"submission.parquet\", read_from_file = [False, False, False]):\n",
    "    '''\n",
    "    Function to run the model and create the submission file.\n",
    "    '''\n",
    "    \n",
    "    # 0. Load the data\n",
    "    X = pd.read_parquet(base_path + f'/preprocessed_data/with_regional/standard_data.parquet', engine='pyarrow')\n",
    "    X.sort_index(inplace=True)\n",
    "    load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "    df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "    y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    X_test = pd.read_parquet(base_path + '/preprocessed_data/test/with_regional_data_test.parquet', engine='pyarrow')\n",
    "    X_test.sort_index(inplace=True)\n",
    "    # 1. prediction of building_type\n",
    "    if read_from_file[0]:\n",
    "        y_pred = pd.read_parquet(base_path + '/kai/model/XGBoost/y_pred.parquet', engine='pyarrow')\n",
    "    else:\n",
    "\n",
    "        clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                        ('scaler', StandardScaler(), X.columns),\n",
    "                        ('encoder', OneHotEncoder(), [])\n",
    "                    ])),\n",
    "                    ('classifier', RandomForestClassifier(**param_grid, random_state=42))\n",
    "                ])\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred = pd.DataFrame(y_pred, index=X_test.index, columns=[\"building_stock_type\"])\n",
    "        y_pred.to_parquet(base_path + '/kai/model/XGBoost/y_pred.parquet', engine='pyarrow')\n",
    "\n",
    "    # 2.making the com prediction\n",
    "    if read_from_file[1]:\n",
    "        y_com_pred = pd.read_parquet(base_path + '/kai/model/XGBoost/y_com_pred.parquet', engine='pyarrow')\n",
    "    else:\n",
    "        X_com = X[df_targets['building_stock_type'] == 'commercial']\n",
    "        y_com = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "        label_encoders = {}\n",
    "        for col in y_com.columns:\n",
    "            le = LabelEncoder()\n",
    "            # le = OneHotEncoder()\n",
    "            y_com[col] = le.fit_transform(y_com[col])\n",
    "            label_encoders[col] = le\n",
    "        X_com_test = X_test[y_pred == 1]\n",
    "\n",
    "        y_com_pred = pd.DataFrame(columns=y_com.columns)\n",
    "        for i, col in enumerate(y_com.columns):\n",
    "            print(f\"Training model for {col}\")\n",
    "            y_com_col = y_com.iloc[:, i]\n",
    "            # bst = Pipeline([('preprocessor', ColumnTransformer([\n",
    "            #                     ('scaler', StandardScaler(), slice(0, X.shape[1])),\n",
    "            #                     ('encoder', OneHotEncoder(), [])  # No need to change the encoder for GPU\n",
    "            #                 ])),\n",
    "            #                 ('classifier', MultiOutputClassifier(\n",
    "            #                     xgb.XGBClassifier(**param_grid_com),))\n",
    "                        # ])\n",
    "            bst = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                                ('scaler', StandardScaler(), slice(0, X.shape[1])),\n",
    "                                ('encoder', OneHotEncoder(), [])  # No need to change the encoder for GPU\n",
    "                            ])),\n",
    "                            ('classifier',xgb.XGBClassifier(**param_grid_com),)\n",
    "                        ])\n",
    "            bst.fit(X_com, y_com_col)\n",
    "            y_com_pred_col = bst.predict(X_com_test)\n",
    "\n",
    "            # y_com_pred_col = cp.asnumpy(y_com_pred_col)\n",
    "            y_com_pred_col = label_encoders[col].inverse_transform(y_com_pred_col)\n",
    "            y_com_pred[col] = y_com_pred_col\n",
    "        y_com_pred.index = X_com_test.index\n",
    "        pd.DataFrame(y_com_pred).to_parquet(base_path + '/kai/model/XGBoost/y_com_pred.parquet', engine='pyarrow')\n",
    "\n",
    "    # 3. making the res prediction\n",
    "    if read_from_file[2]:\n",
    "        y_res_pred = pd.read_parquet(base_path + '/kai/model/XGBoost/y_res_pred.parquet', engine='pyarrow')\n",
    "    else: \n",
    "        X_res = X[df_targets['building_stock_type'] == 'residential']\n",
    "        y_res = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "        label_encoders = {}\n",
    "        for col in y_res.columns:\n",
    "            le = LabelEncoder()\n",
    "            y_res[col] = le.fit_transform(y_res[col])\n",
    "            label_encoders[col] = le\n",
    "        X_res_test = X_test[y_pred == 0]\n",
    "\n",
    "        # X_train_gpu = cp.array(X_res)\n",
    "        # X_test_gpu = cp.array(X_res_test)\n",
    "        y_res_pred = pd.DataFrame(columns=y_res.columns)\n",
    "        for i, col in enumerate(y_res.columns):\n",
    "            print(f\"Training model for {col}\")\n",
    "            y_res_col = y_res.iloc[:, i]\n",
    "            # y_train_gpu = cp.array(y_res_col)\n",
    "            # bst = Pipeline([('preprocessor', ColumnTransformer([\n",
    "            #                     ('scaler', StandardScaler(), slice(0, X.shape[1])),\n",
    "            #                     ('encoder', OneHotEncoder(), [])  # No need to change the encoder for GPU\n",
    "            #                 ])),\n",
    "            #                 ('classifier', MultiOutputClassifier(\n",
    "            #                     xgb.XGBClassifier(**param_grid_res),))\n",
    "            #             ])\n",
    "            bst = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                                ('scaler', StandardScaler(), slice(0, X.shape[1])),\n",
    "                                ('encoder', OneHotEncoder(), [])  # No need to change the encoder for GPU\n",
    "                            ])),\n",
    "                            ('classifier',xgb.XGBClassifier(**param_grid_res),)\n",
    "                        ])\n",
    "            # bst = XGBClassifier(**res_params)\n",
    "            bst.fit(X_res, y_res_col)\n",
    "            y_res_pred_col = bst.predict(X_res_test)\n",
    "\n",
    "            # y_res_pred_col = cp.asnumpy(y_res_pred_col)\n",
    "            y_res_pred_col = label_encoders[col].inverse_transform(y_res_pred_col)\n",
    "            y_res_pred[col] = y_res_pred_col\n",
    "        y_res_pred.index = X_res_test.index\n",
    "        y_res_pred.to_parquet(base_path + '/kai/model/XGBoost/y_res_pred.parquet', engine='pyarrow')\n",
    "    submission_df = create_submission(y_com_pred, y_res_pred, y_pred, save_filepath=base_path + submission_path)\n",
    "    return submission_df\n",
    "\n",
    "param_grid = {'n_estimators': 136, 'criterion': 'gini', 'max_depth': 61,   'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False,}\n",
    "\n",
    "param_grid_com = {'n_estimators': 7, 'max_depth': 5, 'random_state': 42, 'device': 'cpu',}\n",
    "\n",
    "param_grid_res = {'n_estimators': 7, 'max_depth': 5, 'random_state': 42, 'device': 'cpu','eta':0.3273831428143833}\n",
    "\n",
    "\n",
    "run_model(param_grid, param_grid_com, param_grid_res, submission_path='/submissions/xgboost_submission_31_08.parquet', read_from_file=[True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of building_type\n",
    "param_grid = {'n_estimators': 136,  # Integer range\n",
    "    'criterion': 'gini',  # List of options\n",
    "    'max_depth': 61,  # Integer range\n",
    "    'min_samples_split': 7,  # Integer range\n",
    "    'min_samples_leaf': 3,  # Integer range\n",
    "    'max_features': 'sqrt',  # List of options\n",
    "    'bootstrap': False,  # List of options\n",
    "}\n",
    "X = pd.read_parquet(base_path + f'/preprocessed_data/with_regional/standard_data.parquet', engine='pyarrow')\n",
    "X.sort_index(inplace=True)\n",
    "clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "                ('scaler', StandardScaler(), X.columns),\n",
    "                ('encoder', OneHotEncoder(), [])\n",
    "            ])),\n",
    "            ('classifier', RandomForestClassifier(**param_grid, random_state=42))\n",
    "        ])\n",
    "clf.fit(X, y)\n",
    "X_test = pd.read_parquet(base_path + '/preprocessed_data/test/with_regional_data_test.parquet', engine='pyarrow')\n",
    "X_test.sort_index(inplace=True)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for in.comstock_building_type_group_com\n",
      "Training model for in.heating_fuel_com\n",
      "Training model for in.hvac_category_com\n",
      "Training model for in.number_of_stories_com\n",
      "Training model for in.ownership_type_com\n",
      "Training model for in.vintage_com\n",
      "Training model for in.wall_construction_type_com\n",
      "Training model for in.tstat_clg_sp_f..f_com\n",
      "Training model for in.tstat_htg_sp_f..f_com\n",
      "Training model for in.weekday_opening_time..hr_com\n",
      "Training model for in.weekday_operating_hours..hr_com\n"
     ]
    }
   ],
   "source": [
    "# making the com prediction\n",
    "com_params = {'n_estimators': 3, 'max_depth': 7, 'random_state': 42, 'device': 'cpu',}\n",
    "X_com = X[df_targets['building_stock_type'] == 'commercial']\n",
    "y_com = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "label_encoders = {}\n",
    "for col in y_com.columns:\n",
    "    le = LabelEncoder()\n",
    "    # le = OneHotEncoder()\n",
    "    y_com[col] = le.fit_transform(y_com[col])\n",
    "    label_encoders[col] = le\n",
    "X_com_test = X_test[y_pred == 1]\n",
    "\n",
    "y_com_pred = pd.DataFrame(columns=y_com.columns)\n",
    "for i, col in enumerate(y_com.columns):\n",
    "    print(f\"Training model for {col}\")\n",
    "    y_com_col = y_com.iloc[:, i]\n",
    "\n",
    "    bst = XGBClassifier(**com_params)\n",
    "    bst.fit(X_com, y_com_col)\n",
    "    y_com_pred_col = bst.predict(X_com_test)\n",
    "\n",
    "    # y_com_pred_col = cp.asnumpy(y_com_pred_col)\n",
    "    y_com_pred_col = label_encoders[col].inverse_transform(y_com_pred_col)\n",
    "    y_com_pred[col] = y_com_pred_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_com_pred).to_parquet(base_path + '/kai/model/XGBoost/y_com_pred.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the res prediction \n",
    "res_params = {'n_estimators': 4, 'max_depth': 3, 'random_state': 42, 'device': 'cpu','eta':0.3273831428143833}\n",
    "X_res = X[df_targets['building_stock_type'] == 'residential']\n",
    "y_res = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "label_encoders = {}\n",
    "for col in y_res.columns:\n",
    "    le = LabelEncoder()\n",
    "    y_res[col] = le.fit_transform(y_res[col])\n",
    "    label_encoders[col] = le\n",
    "X_res_test = X_test[y_pred == 0]\n",
    "\n",
    "# X_train_gpu = cp.array(X_res)\n",
    "# X_test_gpu = cp.array(X_res_test)\n",
    "y_res_pred = pd.DataFrame(columns=y_res.columns)\n",
    "for i, col in enumerate(y_res.columns):\n",
    "    y_res_col = y_res.iloc[:, i]\n",
    "    # y_train_gpu = cp.array(y_res_col)\n",
    "\n",
    "    bst = XGBClassifier(**res_params)\n",
    "    bst.fit(X_res, y_res_col)\n",
    "    y_res_pred_col = bst.predict(X_res_test)\n",
    "\n",
    "    # y_res_pred_col = cp.asnumpy(y_res_pred_col)\n",
    "    y_res_pred_col = label_encoders[col].inverse_transform(y_res_pred_col)\n",
    "    y_res_pred[col] = y_res_pred_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_pred.to_parquet(base_path + '/kai/model/XGBoost/y_res_pred.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(df_com, df_res, df_test, save_filepath=None):\n",
    "    \"\"\"\n",
    "    Given a df_test dataframe that already contains the predictions for the building_stock_type column,\n",
    "    and two dataframes df_com and df_res that contain the predictions for the residential and commercial\n",
    "    columns respectively, this function will create a submission dataframe that is compatible with the submission format.\n",
    "    \"\"\"\n",
    "    # First load the training labels again to get the correct column order\n",
    "    load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')\n",
    "    df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "    df_targets.sort_index(inplace=True)\n",
    "\n",
    "    # Create a new dataframe with the same index as df_targets\n",
    "    bldg_id_list = [i for i in range(1,1441)]\n",
    "    df = pd.DataFrame(index=bldg_id_list, columns=df_targets.columns)\n",
    "    df.index.name = df_targets.index.name\n",
    "\n",
    "    # Populate the first column 'building_stock_type'\n",
    "    df['building_stock_type'] = df_test[\"building_stock_type\"].map({0: 'residential', 1: 'commercial'})\n",
    "\n",
    "    res_columns = [col for col in df_targets.columns if col.endswith('_res')]\n",
    "    com_columns = [col for col in df_targets.columns if col.endswith('_com')]\n",
    "    for bldg_id in df.index:\n",
    "        if df.at[bldg_id, 'building_stock_type'] == 'residential':\n",
    "            df.loc[bldg_id, com_columns] = np.nan\n",
    "            for col in res_columns:\n",
    "                df.at[bldg_id, col] = df_res.at[bldg_id, col]\n",
    "        else:\n",
    "            df.loc[bldg_id, res_columns] = np.nan\n",
    "            for col in com_columns:\n",
    "                df.at[bldg_id, col] = df_com.at[bldg_id, col]\n",
    "    df = df.astype(str)\n",
    "    if save_filepath:\n",
    "        df.to_parquet(save_filepath)\n",
    "    return df\n",
    "\n",
    "y_res_pred.index = X_res_test.index\n",
    "y_com_pred.index = X_com_test.index\n",
    "y_pred = pd.DataFrame(y_pred, index=X_test.index, columns=[\"building_stock_type\"])\n",
    "submission_df = create_submission(y_com_pred, y_res_pred, y_pred, save_filepath=base_path + '/submissions/xgboost_submission_30_08.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_submission\n\u001b[1;32m----> 2\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_com_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_res_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuilding_stock_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/submissions/xgboost_submission_30_08.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\kai\\preprocessing\\run.py:196\u001b[0m, in \u001b[0;36mcreate_submission\u001b[1;34m(df_com, df_res, df_test, save_filepath)\u001b[0m\n\u001b[0;32m    194\u001b[0m         df\u001b[38;5;241m.\u001b[39mloc[bldg_id, res_columns] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m com_columns:\n\u001b[1;32m--> 196\u001b[0m             df\u001b[38;5;241m.\u001b[39mat[bldg_id, col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_com\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbldg_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    197\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_filepath:\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:2419\u001b[0m, in \u001b[0;36m_AtIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key]\n\u001b[1;32m-> 2419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:2371\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2368\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2370\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_key(key)\n\u001b[1;32m-> 2371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3877\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3871\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   3873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   3874\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   3875\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   3876\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[1;32m-> 3877\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[0;32m   3880\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[0;32m   3881\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "from preprocessing.run import create_submission\n",
    "submission_df = create_submission(y_com_pred, y_res_pred, pd.DataFrame(y_pred, columns=[\"building_stock_type\"]), save_filepath=base_path + '/submissions/xgboost_submission_30_08.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hyperparameters(param_grid):\n",
    "    \"\"\"\n",
    "    Sample hyperparameters from the given grid using scipy.stats.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    param_grid : dict\n",
    "        A dictionary where keys are hyperparameter names and values are lists of options or ranges.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with sampled hyperparameters.\n",
    "    \"\"\"\n",
    "    sampled_params = {}\n",
    "    \n",
    "    for param, values in param_grid.items():\n",
    "        if isinstance(values, list):\n",
    "            # Randomly choose from list of options\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        elif isinstance(values, tuple) and len(values) == 2:\n",
    "            min_val, max_val = values\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                # Sample integer values using scipy.stats.randint\n",
    "                sampled_params[param] = stats.randint.rvs(min_val, max_val + 1)\n",
    "            elif isinstance(min_val, float) and isinstance(max_val, float):\n",
    "                # Sample float values using scipy.stats.uniform\n",
    "                sampled_params[param] = stats.uniform.rvs(min_val, max_val - min_val)\n",
    "        elif isinstance(values, str) and values == 'choice':\n",
    "            # Sample from a list of options if 'choice' is specified\n",
    "            sampled_params[param] = np.random.choice(param_grid[param])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported parameter type for {param}: {values}\")\n",
    "    \n",
    "    # Adjust parameters for RandomForestClassifier\n",
    "    if not sampled_params.get('bootstrap', True):\n",
    "        sampled_params['max_samples'] = None  # Reset max_samples if bootstrap is False\n",
    "        # sampled_params['oob_score'] = False\n",
    "    # Ensure max_features is correctly set\n",
    "    max_features = sampled_params.get('max_features')\n",
    "    if isinstance(max_features, str) and max_features.startswith('0'):\n",
    "        sampled_params['max_features'] = float(max_features)\n",
    "        \n",
    "    \n",
    "    return sampled_params\n",
    "\n",
    "# @profile(precision=4)\n",
    "def produce_submission(df_features_dict, df_targets, param_grid_list, n_runs=5, target=\"building_stock_type\"):\n",
    "    \"\"\"\n",
    "    1. fit simple classifier for building_stock_type\n",
    "    2. fit multioutput classifier for com\n",
    "    3. fit multioutput classifier for res\n",
    "    \"\"\"\n",
    "    if target == \"building_stock_type\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"HP_results/multixgboost/multi_results.csv\"\n",
    "        y = df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "    elif target == \"com\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"\\HP_results/multixgboost/multi_com_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'commercial'].filter(like='_com')\n",
    "        label_encoders = {}\n",
    "        for col in y.columns:\n",
    "            le = LabelEncoder()\n",
    "            y[col] = le.fit_transform(y[col])\n",
    "            label_encoders[col] = le\n",
    "    elif target == \"res\":\n",
    "        results_file = base_path + r\"\\kai\\model\"+ \"\\HP_results/multixgboost/multi_res_results.csv\"\n",
    "        y = df_targets[df_targets['building_stock_type'] == 'residential'].filter(like='_res')\n",
    "        label_encoders = {}\n",
    "        for col in y.columns:\n",
    "            le = LabelEncoder()\n",
    "            y[col] = le.fit_transform(y[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Check if results file exists\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        # Create an empty DataFrame if the results file doesn't exist\n",
    "        results_df = pd.DataFrame(columns=[\n",
    "            'train_set', 'f1_train', 'f1_val', 'n_estimators', 'criterion', 'max_depth', \n",
    "            'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap'\n",
    "        ])\n",
    "\n",
    "    for _ in tqdm(range(n_runs), file=sys.stdout, desc=\"Running Random Search\"):\n",
    "        # print(f\"Run {_ + 1}/{n_runs}\")\n",
    "        # sample parameters\n",
    "        # print_memory_usage()\n",
    "        # print_gpu_memory()\n",
    "        params = sample_hyperparameters(param_grid)\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - Params: {params}\")\n",
    "        train_set = params['train_set']\n",
    "        if df_features_dict is None:\n",
    "            df_features = pd.read_parquet(base_path + f'/preprocessed_data/{train_set}_data.parquet', engine='pyarrow')\n",
    "            df_features.sort_index(inplace=True)\n",
    "        else:\n",
    "            df_features = df_features_dict[train_set].copy()\n",
    "        \n",
    "        if target == \"building_stock_type\":\n",
    "            X = df_features\n",
    "        elif target == \"com\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'commercial']\n",
    "        elif target == \"res\":\n",
    "            X = df_features[df_targets['building_stock_type'] == 'residential']\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        xgb_params = {key: value for key, value in params.items() if key not in ['train_set', \"num_boost_round\"]}\n",
    "\n",
    "        F1_l2_dict_train = {}\n",
    "        F1_l2_dict_val = {}\n",
    "        for i, col in enumerate(y.columns):\n",
    "            y_train_col = y_train.iloc[:, i]\n",
    "            y_val_col = y_val.iloc[:, i]\n",
    "            \n",
    "            if params[\"device\"] == \"cuda\":\n",
    "                X_train_gpu = cp.array(X_train)\n",
    "                X_val_gpu = cp.array(X_val)\n",
    "                y_train_gpu = cp.array(y_train_col)\n",
    "                y_val_gpu = cp.array(y_val_col)\n",
    "            else:\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train_col)\n",
    "                dval = xgb.DMatrix(X_val, label=y_val_col)\n",
    "            \n",
    "            # Train the model\n",
    "            if \"n_estimators\" in xgb_params:\n",
    "                bst = XGBClassifier(**xgb_params)\n",
    "                bst.fit(X_train_gpu, y_train_gpu)\n",
    "                y_train_pred = bst.predict(X_train_gpu)\n",
    "                y_val_pred = bst.predict(X_val_gpu)\n",
    "            else:# xgb.train can work with DMatrix objects\n",
    "                dtrain = xgb.DMatrix(X_train_gpu, label=y_train_gpu)\n",
    "                dval = xgb.DMatrix(X_val_gpu, label=y_val_gpu)\n",
    "                bst = xgb.train(\n",
    "                    xgb_params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=params[\"num_boost_round\"],)\n",
    "                y_train_pred = bst.predict(dtrain)\n",
    "                y_val_pred = bst.predict(dval)\n",
    "            \n",
    "            if params[\"device\"] == \"cuda\":# retransform to cpu for F1 score computation\n",
    "                y_train_pred = cp.asnumpy(y_train_pred)\n",
    "                y_val_pred = cp.asnumpy(y_val_pred)\n",
    "\n",
    "            # compute F1\n",
    "            F1_l2_dict_train[col] = f1_score(y_train_col, y_train_pred.round(), average='macro')\n",
    "            F1_l2_dict_val[col] = f1_score(y_val_col, y_val_pred.round(), average='macro')\n",
    "\n",
    "            # Clean up GPU memory after each model\n",
    "            if params[\"device\"] == \"cuda\":\n",
    "                del X_train_gpu, X_val_gpu, y_train_gpu, y_val_gpu\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                gc.collect()\n",
    "            tqdm.write(f\"Run {_ + 1}/{n_runs} - Finished column: {col} ({i+1}/{len(y.columns)}) with F1 Score (Val): {F1_l2_dict_val[col]:.4f}\")\n",
    "\n",
    "        if target == \"building_stock_type\":\n",
    "            # Evaluate on the training set\n",
    "            f1_train = f1_score(y_train, y_train, average='macro')\n",
    "            train_conf_matrix = confusion_matrix(y_train, y_train)\n",
    "            train_TN, train_FP, train_FN, train_TP = train_conf_matrix.ravel()\n",
    "            f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
    "            val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the validation set confusion matrix\n",
    "            val_TN, val_FP, val_FN, val_TP = val_conf_matrix.ravel()\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                'train_TP': train_TP,\n",
    "                'train_TN': train_TN,\n",
    "                'train_FP': train_FP,\n",
    "                'train_FN': train_FN,\n",
    "                'val_TP': val_TP,\n",
    "                'val_TN': val_TN,\n",
    "                'val_FP': val_FP,\n",
    "                'val_FN': val_FN,\n",
    "                **xgb_params\n",
    "                }\n",
    "        else:\n",
    "            f1_train = sum(F1_l2_dict_train.values()) / len(F1_l2_dict_train.values())\n",
    "            f1_val = sum(F1_l2_dict_val.values()) / len(F1_l2_dict_val.values())\n",
    "            # Append the results to the DataFrame\n",
    "            new_row = {\n",
    "                'train_set': train_set,\n",
    "                'f1_train': f1_train,\n",
    "                'f1_val': f1_val,\n",
    "                **xgb_params\n",
    "            }\n",
    "\n",
    "        # Update tqdm description with the current F1 score\n",
    "        tqdm.write(f\"Run {_ + 1}/{n_runs} - F1 Score (Val): {f1_val:.4f}\")\n",
    "        # tqdm.set_description(f\"Run {_ + 1}/{n_runs} - F1 Val: {f1_val:.4f}\")\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'num_boost_round': (1, 4),  # Number of boosting rounds\n",
    "    'max_depth': (3, 20),  # Maximum tree depth for base learners\n",
    "    'eta': (0.05, 0.4),  # Boosting learning rate (xgb's \"eta\")\n",
    "    'min_child_weight': (1, 20),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'subsample': [0.5, 0.75, 1.0],  # Subsample ratio of the training instance\n",
    "    'n_rounds': (2, 8),\n",
    "    # 'max_leaves': (1, 100),  # Maximum number of leaves; 0 indicates no limit\n",
    "    # 'max_bin': (256, 512),  # Maximum number of bins per feature for histogram-based algorithm\n",
    "    # 'grow_policy': ['depthwise', 'lossguide'],  # Tree growing policy\n",
    "    # 'learning_rate': (0.01, 0.3),  # Boosting learning rate (eta)\n",
    "    # 'verbosity': [0, 1, 2, 3],  # Degree of verbosity (0: silent, 1: warning, 2: info, 3: debug)\n",
    "    # 'objective': ['binary:logistic', 'multi:softprob', 'reg:squarederror'],  # Learning objective\n",
    "    'booster': ['gbtree',],# 'gblinear', 'dart'],  # Booster to use\n",
    "    'tree_method': ['hist'],  # Tree method\n",
    "    # 'gamma': (0, 5),  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "    # 'min_child_weight': (0, 10),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    # 'subsample': (0.5, 1.0),  # Subsample ratio of the training instance\n",
    "    # 'sampling_method': ['uniform', 'gradient_based'],  # Sampling method (only for GPU version of hist tree method)\n",
    "    # 'colsample_bytree': (0.5, 1.0),  # Subsample ratio of columns when constructing each tree\n",
    "    # 'colsample_bylevel': (0.5, 1.0),  # Subsample ratio of columns for each level\n",
    "    # 'colsample_bynode': (0.5, 1.0),  # Subsample ratio of columns for each split\n",
    "    # 'reg_alpha': (0, 1),  # L1 regularization term on weights\n",
    "    # 'reg_lambda': (1, 10),  # L2 regularization term on weights\n",
    "    # 'scale_pos_weight': (0.1, 10),  # Balancing of positive and negative weights\n",
    "    # 'base_score': (0.5, 0.5),  # The initial prediction score of all instances, global bias\n",
    "    # 'multi_strategy': ['one_output_per_tree', 'multi_output_tree'],\n",
    "    'random_state': [42],  # Random number seed for reproducibility\n",
    "    # 'early_stopping_rounds': (10, 100),  # Number of rounds for early stopping\n",
    "    'device': ['cuda'],#, 'cuda'],  # Device to use\n",
    "    'train_set': ['standard', 'daily', 'weekly', 'monthly', 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/standard']  # List of options\n",
    "}\n",
    "\n",
    "param_grid_xgb_1 = {\n",
    "    'n_estimators': (4, 50),  # Number of boosting rounds\n",
    "    'max_depth': (3, 100),  # Maximum tree depth for base learners\n",
    "    'eta': (0.05, 0.4),  # Boosting learning rate (xgb's \"eta\")\n",
    "    'random_state': [42],  # Random number seed for reproducibility\n",
    "    'device': ['cuda'],#, 'cuda'],  # Device to use\n",
    "    'train_set': ['standard', 'daily', 'weekly', 'monthly', 'with_regional/monthly', 'with_regional/weekly', 'with_regional/daily', 'with_regional/standard']  # List of options\n",
    "    }\n",
    "\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "# print_memory_usage()\n",
    "# print_gpu_memory()\n",
    "# random_search(None, df_targets, param_grid_xgb_1, n_runs=5, target=\"res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1450/1450 [01:07<00:00, 21.40it/s]\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\pandas\\io\\parquet.py:159: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_energy_consumption(folder_path, season_months_dict=None, type='daily', with_regional=False):\n",
    "    \"\"\"\n",
    "    Process multiple parquet files in a folder, calculate average energy consumption,\n",
    "    and return a pandas DataFrame with each row corresponding to one file in the folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing parquet files.\n",
    "    - season_months_dict (dict): A dictionary where keys are season names (strings) and values are lists\n",
    "    of corresponding month numbers. For example, {'cold': [1, 2, 12], 'hot': [6, 7, 8], 'mild': [3, 4, 5, 9, 10, 11]}.\n",
    "\n",
    "    Returns:\n",
    "    - df_ave (pd.DataFrame): A pandas DataFrame with each row corresponding to one file in the folder (i.e. one building).\n",
    "    The columns are multi-layer with the first layer being the day/week/month/season and the second layer the hour of the day \n",
    "    Index ('bldg_id') contains building IDs. Column values are average hourly electricity energy consumption\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store individual DataFrames for each file\n",
    "    result_dfs = []\n",
    "    if with_regional:\n",
    "        locations = {\n",
    "            \"WI\": {\"latitude\": 44.500000, \"longitude\": -89.500000},  # Wisconsin\n",
    "            \"WV\": {\"latitude\": 39.000000, \"longitude\": -80.500000},  # West Virginia\n",
    "            \"VT\": {\"latitude\": 44.000000, \"longitude\": -72.699997},  # Vermont\n",
    "            \"TX\": {\"latitude\": 31.000000, \"longitude\": -100.000000}, # Texas\n",
    "            \"SD\": {\"latitude\": 44.500000, \"longitude\": -100.000000}, # South Dakota\n",
    "            \"RI\": {\"latitude\": 41.742325, \"longitude\": -71.742332},  # Rhode Island\n",
    "            \"OR\": {\"latitude\": 44.000000, \"longitude\": -120.500000}, # Oregon\n",
    "            \"NY\": {\"latitude\": 43.000000, \"longitude\": -75.000000},  # New York\n",
    "            \"NH\": {\"latitude\": 44.000000, \"longitude\": -71.500000},  # New Hampshire\n",
    "            \"NE\": {\"latitude\": 41.500000, \"longitude\": -100.000000}, # Nebraska\n",
    "            \"KS\": {\"latitude\": 38.500000, \"longitude\": -98.000000},  # Kansas\n",
    "            \"MS\": {\"latitude\": 33.000000, \"longitude\": -90.000000},  # Mississippi\n",
    "            \"IL\": {\"latitude\": 40.000000, \"longitude\": -89.000000},  # Illinois\n",
    "            \"DE\": {\"latitude\": 39.000000, \"longitude\": -75.500000},  # Delaware\n",
    "            \"CT\": {\"latitude\": 41.599998, \"longitude\": -72.699997},  # Connecticut\n",
    "            \"AR\": {\"latitude\": 34.799999, \"longitude\": -92.199997},  # Arkansas\n",
    "            \"IN\": {\"latitude\": 40.273502, \"longitude\": -86.126976},  # Indiana\n",
    "            \"MO\": {\"latitude\": 38.573936, \"longitude\": -92.603760},  # Missouri\n",
    "            \"FL\": {\"latitude\": 27.994402, \"longitude\": -81.760254},  # Florida\n",
    "            \"NV\": {\"latitude\": 39.876019, \"longitude\": -117.224121}, # Nevada\n",
    "            \"ME\": {\"latitude\": 45.367584, \"longitude\": -68.972168},  # Maine\n",
    "            \"MI\": {\"latitude\": 44.182205, \"longitude\": -84.506836},  # Michigan\n",
    "            \"GA\": {\"latitude\": 33.247875, \"longitude\": -83.441162},  # Georgia\n",
    "            \"HI\": {\"latitude\": 19.741755, \"longitude\": -155.844437}, # Hawaii\n",
    "            \"AK\": {\"latitude\": 66.160507, \"longitude\": -153.369141}, # Alaska\n",
    "            \"TN\": {\"latitude\": 35.860119, \"longitude\": -86.660156},  # Tennessee\n",
    "            \"VA\": {\"latitude\": 37.926868, \"longitude\": -78.024902},  # Virginia\n",
    "            \"NJ\": {\"latitude\": 39.833851, \"longitude\": -74.871826},  # New Jersey\n",
    "            \"KY\": {\"latitude\": 37.839333, \"longitude\": -84.270020},  # Kentucky\n",
    "            \"ND\": {\"latitude\": 47.650589, \"longitude\": -100.437012}, # North Dakota\n",
    "            \"MN\": {\"latitude\": 46.392410, \"longitude\": -94.636230},  # Minnesota\n",
    "            \"OK\": {\"latitude\": 36.084621, \"longitude\": -96.921387},  # Oklahoma\n",
    "            \"MT\": {\"latitude\": 46.965260, \"longitude\": -109.533691}, # Montana\n",
    "            \"WA\": {\"latitude\": 47.751076, \"longitude\": -120.740135}, # Washington\n",
    "            \"UT\": {\"latitude\": 39.419220, \"longitude\": -111.950684}, # Utah\n",
    "            \"CO\": {\"latitude\": 39.113014, \"longitude\": -105.358887}, # Colorado\n",
    "            \"OH\": {\"latitude\": 40.367474, \"longitude\": -82.996216},  # Ohio\n",
    "            \"AL\": {\"latitude\": 32.318230, \"longitude\": -86.902298},  # Alabama\n",
    "            \"IA\": {\"latitude\": 42.032974, \"longitude\": -93.581543},  # Iowa\n",
    "            \"NM\": {\"latitude\": 34.307144, \"longitude\": -106.018066}, # New Mexico\n",
    "            \"SC\": {\"latitude\": 33.836082, \"longitude\": -81.163727},  # South Carolina\n",
    "            \"PA\": {\"latitude\": 41.203323, \"longitude\": -77.194527},  # Pennsylvania\n",
    "            \"AZ\": {\"latitude\": 34.048927, \"longitude\": -111.093735}, # Arizona\n",
    "            \"MD\": {\"latitude\": 39.045753, \"longitude\": -76.641273},  # Maryland\n",
    "            \"MA\": {\"latitude\": 42.407211, \"longitude\": -71.382439},  # Massachusetts\n",
    "            \"CA\": {\"latitude\": 36.778259, \"longitude\": -119.417931}, # California\n",
    "            \"ID\": {\"latitude\": 44.068203, \"longitude\": -114.742043}, # Idaho\n",
    "            \"WY\": {\"latitude\": 43.075970, \"longitude\": -107.290283}, # Wyoming\n",
    "            \"NC\": {\"latitude\": 35.782169, \"longitude\": -80.793457},  # North Carolina\n",
    "            \"LA\": {\"latitude\": 30.391830, \"longitude\": -92.329102},  # Louisiana\n",
    "            \"DC\": {\"latitude\": 38.907200, \"longitude\": -77.036900},  # Washington, D.C.\n",
    "        }\n",
    "\n",
    "    # Iterate through all files in the folder_path\n",
    "    for file_name in tqdm(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            # Extract the bldg_id from the file name\n",
    "            bldg_id = int(file_name.split('.')[0])\n",
    "\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Read the original parquet file\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            # Convert 'timestamp' column to datetime\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df['hour'] = df['timestamp'].dt.hour\n",
    "            if with_regional:\n",
    "                state = df[\"in.state\"].unique()[0]\n",
    "                latitude = locations[state][\"latitude\"]\n",
    "                longitude = locations[state][\"longitude\"]\n",
    "            if type == 'daily':# -> goes from Input: 365 * 24 * 4 = 35,040 columns to 365 * 24 = 8,760 values per building\n",
    "                df['day_of_year'] = df['timestamp'].dt.day_of_year\n",
    "                df['hourly_energy_consumption'] = df.groupby(['day_of_year', 'hour'])['out.electricity.total.energy_consumption'].transform('mean')\n",
    "                result_df = df.pivot_table(values='hourly_energy_consumption', index='bldg_id', columns=['day_of_year', 'hour'])\n",
    "            \n",
    "            elif type == 'weekly':# -> goes from Input: 365 * 24 * 4 = 35,040 columns to 52 * 24 = 1,248 values per building\n",
    "                df['week'] = df['timestamp'].dt.isocalendar().week\n",
    "                df['weekly_energy_consumption'] = df.groupby(['week', 'hour'])['out.electricity.total.energy_consumption'].transform('mean')\n",
    "                result_df = df.pivot_table(values='weekly_energy_consumption', index='bldg_id', columns=['week', 'hour'])\n",
    "\n",
    "            elif type == 'monthly':# -> goes from Input: 365 * 24 * 4 = 35,040 columns to 12 * 24 = 288 values per building\n",
    "                df['month'] = df['timestamp'].dt.month\n",
    "                df['monthly_energy_consumption'] = df.groupby(['month', 'hour'])['out.electricity.total.energy_consumption'].transform('mean')\n",
    "                result_df = df.pivot_table(values='monthly_energy_consumption', index='bldg_id', columns=['month', 'hour'])\n",
    "\n",
    "            elif type == 'seasonal': # originally provided prerpocessing method -> goes from Input: 365 * 24 * 4 = 35,040 columns to 365 * (12/s)  = ... values per building\n",
    "                df['month'] = df['timestamp'].dt.month\n",
    "                # Create a mapping from month to the corresponding season\n",
    "                month_to_season = {month: season for season, months_list in season_months_dict.items() for month in months_list}\n",
    "\n",
    "                # Assign a season to each row based on the month\n",
    "                df['season'] = df['month'].map(month_to_season)\n",
    "\n",
    "                # Calculate hourly average energy consumption for each row\n",
    "                df['hourly_avg_energy_consumption'] = 4 * df.groupby(['season', 'hour'])['out.electricity.total.energy_consumption'].transform('mean')\n",
    "\n",
    "                # Pivot the dataframe to create the desired output format\n",
    "                result_df = df.pivot_table(values='hourly_avg_energy_consumption', index='bldg_id', columns=['season', 'hour'])\n",
    "\n",
    "                # Reset the column names\n",
    "                result_df.columns = pd.MultiIndex.from_tuples([(season, hour+1) for season, months_list in season_months_dict.items() for hour in range(24)])\n",
    "            else:\n",
    "                raise ValueError('Invalid type. Please select from hourly, weekly, or monthly.')\n",
    "\n",
    "            # Add 'bldg_id' index with values corresponding to the names of the parquet files\n",
    "            result_df['bldg_id'] = bldg_id\n",
    "            if with_regional:\n",
    "                result_df[\"latitude\"] = latitude\n",
    "                result_df[\"longitude\"] = longitude\n",
    "            result_df.set_index('bldg_id', inplace=True)\n",
    "\n",
    "            # Append the result_df to the list\n",
    "            result_dfs.append(result_df)\n",
    "\n",
    "    # Concatenate all individual DataFrames into a single DataFrame\n",
    "    df_ave = pd.concat(result_dfs, ignore_index=False)\n",
    "\n",
    "    return df_ave\n",
    "\n",
    "\n",
    "for s in [\"monthly\"]:\n",
    "    # save_path = os.path.join(base_path + f'/preprocessed_data/with_regional/{s}_data.parquet')\n",
    "    # df_features = pd.read_parquet(base_path + f'/preprocessed_data/{s}_data.parquet', engine='pyarrow')\n",
    "    folder_path = os.path.join(base_path + data_path,'building-instinct-test-data')\n",
    "    df_features = calculate_average_energy_consumption(folder_path, type=s, with_regional=True)\n",
    "    df_features.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "df_features_full = pd.read_parquet(base_path + '/preprocessed_data/test/data_test.parquet', engine='pyarrow')\n",
    "df_features_full.sort_index(inplace=True)\n",
    "df_features_full[\"latitude\"] = df_features[\"latitude\"]\n",
    "df_features_full[\"longitude\"] = df_features[\"longitude\"]\n",
    "save_path = os.path.join(base_path + f'/preprocessed_data/with_regional_data_test.parquet')\n",
    "df_features_full.to_parquet(save_path, engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
