{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n",
      "Cuda device count:  1\n",
      "Cuda current device:  0\n",
      "Cuda device name:  NVIDIA GeForce RTX 3060\n",
      "Cuda device capability:  (8, 6)\n",
      "Cuda device memory:  12884246528\n",
      "Cuda device memory:  11.9993896484375 GB\n",
      "Cuda device memory:  12287.375 MB\n",
      "Cuda device memory:  12582272.0 KB\n",
      "Cuda version:  11.8\n",
      "Cuda version:  ['11', '8']\n",
      "Cuda version:  11\n",
      "Cuda version:  8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "print(\"Cuda device count: \", torch.cuda.device_count())\n",
    "print(\"Cuda current device: \", torch.cuda.current_device())\n",
    "print(\"Cuda device name: \", torch.cuda.get_device_name(0))\n",
    "print(\"Cuda device capability: \", torch.cuda.get_device_capability(0))\n",
    "print(\"Cuda device memory: \", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"Cuda device memory: \", torch.cuda.get_device_properties(0).total_memory/1024**3, \"GB\")\n",
    "print(\"Cuda device memory: \", torch.cuda.get_device_properties(0).total_memory/1024**2, \"MB\")\n",
    "print(\"Cuda device memory: \", torch.cuda.get_device_properties(0).total_memory/1024, \"KB\")\n",
    "# version\n",
    "print(\"Cuda version: \", torch.version.cuda)\n",
    "print(\"Cuda version: \", torch.version.cuda.split(\".\"))\n",
    "print(\"Cuda version: \", torch.version.cuda.split(\".\")[0])\n",
    "print(\"Cuda version: \", torch.version.cuda.split(\".\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importaing the required libraries\n",
    "import os\n",
    "import sys\n",
    "base_path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\"\n",
    "data_path = r\"\\data\\building-instinct-starter-notebook\\Starter notebook\"\n",
    "sys.path.append(base_path+data_path)\n",
    "path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\kai\"\n",
    "sys.path.append(base_path+path)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import (calculate_average_hourly_energy_consumption, train_model, get_pred, calculate_hierarchical_f1_score,\n",
    "sample_submission_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standard_df(folder_path):\n",
    "    \"\"\"\n",
    "    Process multiple parquet files in a folder and return a pandas DataFrame with each row corresponding to one file in the folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing parquet files.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): A pandas DataFrame with each row corresponding to one file in the folder (i.e. one building).\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store individual DataFrames for each file\n",
    "    result_dfs = []\n",
    "\n",
    "    # Iterate through all files in the folder_path\n",
    "    for file_name in tqdm(os.listdir(folder_path)[:1000]):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            # Extract the bldg_id from the file name\n",
    "            bldg_id = int(file_name.split('.')[0])\n",
    "\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Read the original parquet file\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            # Convert 'timestamp' column to datetime\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            result_df = df.pivot_table(values='out.electricity.total.energy_consumption', index='bldg_id', columns=['timestamp'])\n",
    "\n",
    "            # Add 'bldg_id' index with values corresponding to the names of the parquet files\n",
    "            result_df['bldg_id'] = bldg_id\n",
    "            result_df.set_index('bldg_id', inplace=True)\n",
    "\n",
    "            # Append the result_df to the list\n",
    "            result_dfs.append(result_df)\n",
    "\n",
    "    # Concatenate all individual DataFrames into a single DataFrame\n",
    "    output_df = pd.concat(result_dfs, ignore_index=False)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:01<00:00,  8.25it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = base_path + data_path + r\"\\building-instinct-train-data\"\n",
    "df_features = load_standard_df(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp  2018-01-01 00:15:00  2018-01-01 00:30:00  2018-01-01 00:45:00  \\\n",
      "bldg_id                                                                    \n",
      "1                     2.288000             2.190000             2.101000   \n",
      "10                    0.188000             0.181000             0.184000   \n",
      "11                    0.336000             0.336000             0.216000   \n",
      "12                   15.851305            15.081237            14.091116   \n",
      "13                    1.871387             1.883041             1.893783   \n",
      "\n",
      "timestamp  2018-01-01 01:00:00  2018-01-01 01:15:00  2018-01-01 01:30:00  \\\n",
      "bldg_id                                                                    \n",
      "1                     2.016000             2.027000             2.050000   \n",
      "10                    0.303000             0.311000             0.331000   \n",
      "11                    0.216000             0.212000             0.212000   \n",
      "12                   13.586164            12.635340            12.166746   \n",
      "13                    1.902753             1.919542             1.950845   \n",
      "\n",
      "timestamp  2018-01-01 01:45:00  2018-01-01 02:00:00  2018-01-01 02:15:00  \\\n",
      "bldg_id                                                                    \n",
      "1                     2.074000             2.097000             2.129000   \n",
      "10                    0.352000             0.375000             0.376000   \n",
      "11                    0.212000             0.212000             0.202000   \n",
      "12                   12.019187            11.951870            11.298906   \n",
      "13                    2.086655             2.109721             2.103540   \n",
      "\n",
      "timestamp  2018-01-01 02:30:00  ...  2018-12-31 21:45:00  2018-12-31 22:00:00  \\\n",
      "bldg_id                         ...                                             \n",
      "1                     2.162000  ...             0.366000             0.369000   \n",
      "10                    0.383000  ...             0.478000             0.444000   \n",
      "11                    0.202000  ...             0.374000             0.374000   \n",
      "12                   11.231951  ...            20.844561            20.810894   \n",
      "13                    1.983639  ...             1.934569             1.850057   \n",
      "\n",
      "timestamp  2018-12-31 22:15:00  2018-12-31 22:30:00  2018-12-31 22:45:00  \\\n",
      "bldg_id                                                                    \n",
      "1                     0.164000             0.168000             0.170000   \n",
      "10                    0.322000             0.395000             0.437000   \n",
      "11                    0.336000             0.336000             0.336000   \n",
      "12                   20.833709            20.857239            20.454540   \n",
      "13                    1.843447             1.840333             1.834103   \n",
      "\n",
      "timestamp  2018-12-31 23:00:00  2018-12-31 23:15:00  2018-12-31 23:30:00  \\\n",
      "bldg_id                                                                    \n",
      "1                     0.170000             0.178000             0.204000   \n",
      "10                    0.424000             0.265000             0.213000   \n",
      "11                    0.336000             0.316000             0.316000   \n",
      "12                   20.266105            19.828590            19.604916   \n",
      "13                    1.830988             1.827243             1.829479   \n",
      "\n",
      "timestamp  2018-12-31 23:45:00  2019-01-01 00:00:00  \n",
      "bldg_id                                              \n",
      "1                     0.340000             0.375000  \n",
      "10                    0.208000             0.204000  \n",
      "11                    0.316000             0.389000  \n",
      "12                   18.114525            17.365646  \n",
      "13                    1.838243             1.858520  \n",
      "\n",
      "[5 rows x 35040 columns]\n"
     ]
    }
   ],
   "source": [
    "df_features.sort_index(inplace=True)\n",
    "print(df_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2880,  2.1900,  2.1010,  ...,  0.2040,  0.3400,  0.3750],\n",
      "        [ 0.1880,  0.1810,  0.1840,  ...,  0.2130,  0.2080,  0.2040],\n",
      "        [ 0.3360,  0.3360,  0.2160,  ...,  0.3160,  0.3160,  0.3890],\n",
      "        ...,\n",
      "        [56.1725, 58.3277, 59.9776,  ..., 51.5117, 53.8454, 53.7773],\n",
      "        [ 2.9919,  2.8974,  2.8502,  ...,  3.1664,  3.1336,  3.0391],\n",
      "        [ 0.5830,  0.5340,  0.4900,  ...,  0.1070,  0.2210,  0.2570]])\n",
      "35040\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(df_features.values, dtype=torch.float32)\n",
    "print(data)\n",
    "print(data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_com(df_com):\n",
    "    if \"residential\" in df_com.building_stock_type:\n",
    "        print(\"Residential still present\")\n",
    "\n",
    "    com_exception_cols = [\"in.number_of_stories_com\", \"in.vintage_com\", \"in.tstat_clg_sp_f..f_com\", \"in.tstat_htg_sp_f..f_com\", \n",
    "                        \"in.weekday_opening_time..hr_com\", \"in.weekday_operating_hours..hr_com\"]\n",
    "\n",
    "    # columns that are simply transformed to numeric\n",
    "    for col in com_exception_cols:\n",
    "        if col == \"in.vintage_com\":\n",
    "            period_dict = {'Before 1946': 1940,  # Using an estimated middle year\n",
    "                            '1946 to 1959': 1952,\n",
    "                            '1960 to 1969': 1965,\n",
    "                            '1970 to 1979': 1975,\n",
    "                            '1980 to 1989': 1985,\n",
    "                            '1990 to 1999': 1995,\n",
    "                            '2000 to 2012': 2006,\n",
    "                            '2013 to 2018': 2016\n",
    "                            }\n",
    "            df_com['in.vintage_com'] = df_com['in.vintage_com'].map(period_dict)\n",
    "        else:\n",
    "            df_com[col] = pd.to_numeric(df_com[col])\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    categorical_encoded = one_hot_encoder.fit_transform(df_com[df_com.columns.difference(com_exception_cols)])\n",
    "    categorical_encoded = pd.DataFrame(categorical_encoded)\n",
    "    categorical_encoded.index = df_com.index\n",
    "    df_com = pd.concat([df_com[com_exception_cols], pd.DataFrame(categorical_encoded)], axis=1)\n",
    "    return df_com.fillna(0)\n",
    "\n",
    "def convert_income_range_to_mean(income):\n",
    "    if '+' in income:\n",
    "        return float(income[:-1])  # Take the numeric part of '200000+'\n",
    "    if '<' in income:\n",
    "        return float(income[1:])  # Take the numeric part of '<10000'\n",
    "    \n",
    "    lower, upper = map(int, income.split('-'))\n",
    "    return int((lower + upper) / 2)\n",
    "\n",
    "def convert_geometry_to_mean(area):\n",
    "    if '+' in area:\n",
    "        return float(area[:-1])  # Take the numeric part of '4000+'\n",
    "    lower, upper = map(int, area.split('-'))\n",
    "    return int((lower + upper) / 2)\n",
    "\n",
    "def preprocess_res(df_res):\n",
    "    if \"commercial\" in df_res.building_stock_type:\n",
    "        print(\"Commercial still present\")\n",
    "\n",
    "    res_exception_cols = [\"in.bedrooms_res\", \"in.cooling_setpoint_res\", \"in.heating_setpoint_res\", \"in.geometry_floor_area_res\", \n",
    "                        \"in.income_res\", \"in.vintage_res\"]\n",
    "    \n",
    "    df_res[\"in.bedrooms_res\"] = pd.to_numeric(df_res['in.bedrooms_res'])\n",
    "    \n",
    "    # handling the temperature columns f.e. going from 78F to 78\n",
    "    df_res['in.cooling_setpoint_res'] = df_res['in.cooling_setpoint_res'].apply(lambda temp: float(temp[:-1]))\n",
    "    df_res['in.heating_setpoint_res'] = df_res['in.heating_setpoint_res'].apply(lambda temp: float(temp[:-1]))\n",
    "\n",
    "    df_res['in.geometry_floor_area_res'] = df_res['in.geometry_floor_area_res'].apply(convert_geometry_to_mean)\n",
    "    df_res['in.income_res'] = df_res['in.income_res'].apply(convert_income_range_to_mean)\n",
    "\n",
    "    vintage_mapping = {\n",
    "        '<1940': 1940,\n",
    "        '1940s': 1945,\n",
    "        '1950s': 1955,\n",
    "        '1960s': 1965,\n",
    "        '1970s': 1975,\n",
    "        '1980s': 1985,\n",
    "        '1990s': 1995,\n",
    "        '2000s': 2005,\n",
    "        '2010s': 2015\n",
    "    }\n",
    "\n",
    "    df_res['in.vintage_res'] = df_res['in.vintage_res'].map(vintage_mapping)\n",
    "    # One-hot encode categorical features\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    categorical_encoded = one_hot_encoder.fit_transform(df_res[df_res.columns.difference(res_exception_cols)])\n",
    "    categorical_encoded = pd.DataFrame(categorical_encoded)\n",
    "    categorical_encoded.index = df_res.index\n",
    "    df_res = pd.concat([df_res[res_exception_cols], pd.DataFrame(categorical_encoded)], axis=1)\n",
    "    return df_res.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "df_targets = df_targets[:1000]\n",
    "\n",
    "df_targets_res = df_targets[df_targets.building_stock_type == \"residential\"].copy()\n",
    "df_targets_com = df_targets[df_targets.building_stock_type == \"commercial\"].copy()\n",
    "df_targets_res = preprocess_res(df_targets_res)\n",
    "df_targets_com = preprocess_com(df_targets_com)\n",
    "\n",
    "# print(df_targets.columns)\n",
    "# for col in df_targets.columns:\n",
    "#     print(f\"{col} - {df_targets[col].unique()}\")\n",
    "# print(df_targets.nunique())\n",
    "# Fill NaNs with a placeholder\n",
    "# df_categorical = df_targets.fillna('Unknown')\n",
    "# print(df_targets.isna().sum())\n",
    "\n",
    "# print(categorical_encoded.shape)\n",
    "# combined_data = np.hstack([time_series_data, categorical_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         in.number_of_stories_com  in.vintage_com  in.tstat_clg_sp_f..f_com  \\\n",
      "bldg_id                                                                       \n",
      "3                               2            1995                       999   \n",
      "5                               1            2006                       999   \n",
      "6                               1            1965                        72   \n",
      "9                               1            1985                       999   \n",
      "12                              1            1940                        72   \n",
      "...                           ...             ...                       ...   \n",
      "992                             1            1995                        73   \n",
      "993                             1            1995                        74   \n",
      "994                             2            1952                        71   \n",
      "995                             1            2006                        76   \n",
      "996                             1            1975                        72   \n",
      "\n",
      "         in.tstat_htg_sp_f..f_com  in.weekday_opening_time..hr_com  \\\n",
      "bldg_id                                                              \n",
      "3                             999                             8.25   \n",
      "5                             999                             7.00   \n",
      "6                              67                             9.00   \n",
      "9                             999                             7.00   \n",
      "12                             67                             6.75   \n",
      "...                           ...                              ...   \n",
      "992                            68                             5.00   \n",
      "993                            69                            10.25   \n",
      "994                            68                             6.75   \n",
      "995                            69                             7.50   \n",
      "996                            68                             8.50   \n",
      "\n",
      "         in.weekday_operating_hours..hr_com    0    1    2    3  ...   29  \\\n",
      "bldg_id                                                          ...        \n",
      "3                                      9.25  1.0  1.0  0.0  0.0  ...  1.0   \n",
      "5                                      8.00  1.0  1.0  0.0  0.0  ...  1.0   \n",
      "6                                     11.75  1.0  1.0  0.0  0.0  ...  0.0   \n",
      "9                                      8.50  1.0  1.0  0.0  0.0  ...  0.0   \n",
      "12                                     7.00  1.0  1.0  0.0  0.0  ...  0.0   \n",
      "...                                     ...  ...  ...  ...  ...  ...  ...   \n",
      "992                                   10.75  1.0  1.0  0.0  0.0  ...  0.0   \n",
      "993                                   11.25  1.0  1.0  0.0  0.0  ...  0.0   \n",
      "994                                    9.00  1.0  1.0  0.0  0.0  ...  1.0   \n",
      "995                                   10.00  1.0  1.0  0.0  0.0  ...  0.0   \n",
      "996                                    8.50  1.0  1.0  0.0  0.0  ...  0.0   \n",
      "\n",
      "          30   31   32   33   34   35   36   37   38  \n",
      "bldg_id                                               \n",
      "3        0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "5        0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "6        0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "9        0.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  \n",
      "12       0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "992      0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  \n",
      "993      0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  \n",
      "994      0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "995      0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "996      0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[458 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_targets_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "# Define custom dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, y_time_series, y_household_type, y_categorical):\n",
    "        self.data = data\n",
    "        self.y_time_series = y_time_series\n",
    "        self.y_household_type = y_household_type\n",
    "        self.y_categorical = y_categorical\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx], \n",
    "                self.y_time_series[idx], \n",
    "                self.y_household_type[idx], \n",
    "                self.y_categorical[idx])\n",
    "\n",
    "# Assuming y_time_series is same as data for simplicity\n",
    "dataset = TimeSeriesDataset(data, data, y_household_type, y_categorical)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample dataset (replace this with actual dataset loading)\n",
    "# X is the time series data with shape (num_samples, sequence_length, num_features)\n",
    "# y_time_series is the target time series with shape (num_samples, sequence_length, num_features)\n",
    "# y_household_type is the household category with shape (num_samples,)\n",
    "# y_categorical is the additional categorical attribute with shape (num_samples, num_attributes)\n",
    "\n",
    "X = torch.randn(100, 96, 1)  # Example shape for 100 samples, 96 timesteps, 1 feature\n",
    "y_time_series = torch.randn(100, 96, 1)\n",
    "y_household_type = torch.randint(0, 2, (100,))\n",
    "y_categorical = torch.randint(0, 3, (100, 2))\n",
    "\n",
    "dataset = TensorDataset(X, y_time_series, y_household_type, y_categorical)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes_household, num_classes_categorical):\n",
    "        super(MultiTaskLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Output layers for different tasks\n",
    "        self.time_series_head = nn.Linear(hidden_size, input_size)\n",
    "        self.household_type_head = nn.Linear(hidden_size, num_classes_household)\n",
    "        self.categorical_head = nn.Linear(hidden_size, num_classes_categorical)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the last output for classification tasks\n",
    "        \n",
    "        time_series_pred = self.time_series_head(lstm_out)\n",
    "        household_type_pred = self.household_type_head(lstm_out)\n",
    "        categorical_pred = self.categorical_head(lstm_out)\n",
    "        \n",
    "        return time_series_pred, household_type_pred, categorical_pred\n",
    "\n",
    "input_size = 1  # Number of features in time series\n",
    "hidden_size = 128\n",
    "num_classes_household = 2\n",
    "num_classes_categorical = 3\n",
    "\n",
    "model = MultiTaskLSTM(input_size, hidden_size, num_classes_household, num_classes_categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, time_series_pred, time_series_true, \n",
    "                household_type_pred, household_type_true, \n",
    "                categorical_pred, categorical_true):\n",
    "        loss_time_series = self.mse_loss(time_series_pred, time_series_true)\n",
    "        loss_household_type = self.cross_entropy_loss(household_type_pred, household_type_true)\n",
    "        \n",
    "        # If multiple categorical attributes, apply CE loss for each and sum them\n",
    "        loss_categorical = sum([self.cross_entropy_loss(categorical_pred[:, i], categorical_true[:, i]) \n",
    "                                for i in range(categorical_true.size(1))])\n",
    "        \n",
    "        total_loss = loss_time_series + loss_household_type + loss_categorical\n",
    "        return total_loss\n",
    "\n",
    "criterion = CustomLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_time_series_batch, y_household_type_batch, y_categorical_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        time_series_pred, household_type_pred, categorical_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(time_series_pred, y_time_series_batch, \n",
    "                         household_type_pred, y_household_type_batch, \n",
    "                         categorical_pred, y_categorical_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
