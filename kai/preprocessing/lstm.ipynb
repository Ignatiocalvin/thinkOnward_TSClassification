{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992323346090601\n",
      "[[765   2]\n",
      " [  9 664]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "from run import simple_classification, train_lstm, predict_lstm, create_submission, unique_values\n",
    "from preprocessing import TargetPreprocessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\"\n",
    "data_path = r\"\\data\\building-instinct-starter-notebook\\Starter notebook\"\n",
    "sys.path.append(base_path+data_path)\n",
    "sys.path.append(base_path+\"\\kai\")\n",
    "df_features = pd.read_parquet(base_path + '/preprocessed_data/standard_data.parquet', engine='pyarrow')\n",
    "df_features.sort_index(inplace=True)\n",
    "\n",
    "load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "\n",
    "# RandomForest classifier\n",
    "X, y = df_features, df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "df_test = pd.read_parquet(base_path + '/preprocessed_data/data_test.parquet', engine='pyarrow')\n",
    "df_test.sort_index(inplace=True)\n",
    "\n",
    "y_pred = simple_classification(X, y, df_test, print_performance=True)\n",
    "df_test[\"building_stock_type\"] = y_pred\n",
    "\n",
    "# The 2 LSTM models\n",
    "# 1. Preprocessing\n",
    "df_targets_res = df_targets[df_targets.building_stock_type == \"residential\"].filter(like='_res').copy()\n",
    "df_targets_com = df_targets[df_targets.building_stock_type == \"commercial\"].filter(like='_com').copy()\n",
    "target_preprocessor = TargetPreprocessor()\n",
    "df_targets_res, association_dict_res, encoder_res = target_preprocessor.preprocess_res(df_targets_res)\n",
    "df_targets_com, association_dict_com, encoder_com = target_preprocessor.preprocess_com(df_targets_com)\n",
    "unique_values_res = unique_values(df_targets_res)\n",
    "unique_values_com = unique_values(df_targets_com)\n",
    "\n",
    "common_indices = df_features.index.intersection(df_targets_com.index)\n",
    "X_com = df_features[df_features.index.isin(common_indices)]\n",
    "y_com = df_targets_com[df_targets_com.index.isin(common_indices)]\n",
    "# X_com = torch.tensor(X_com.values, dtype=torch.float32)\n",
    "# y_com = torch.tensor(y_com.values, dtype=torch.float32)\n",
    "X_train_com, X_val_com, y_train_com, y_val_com = train_test_split(X_com, y_com, test_size=0.2, random_state=42)\n",
    "X_train_com = torch.tensor(X_train_com.values, dtype=torch.float32)\n",
    "y_train_com = torch.tensor(y_train_com.values, dtype=torch.float32)\n",
    "X_val_com = torch.tensor(X_val_com.values, dtype=torch.float32)\n",
    "y_val_com = torch.tensor(y_val_com.values, dtype=torch.float32)\n",
    "\n",
    "X_res = df_features[df_features.index.isin(df_targets_res.index)]\n",
    "y_res = df_targets_res[df_targets_res.index.isin(df_features.index)]\n",
    "# X_res = torch.tensor(X_res.values, dtype=torch.float32)\n",
    "# y_res = torch.tensor(y_res.values, dtype=torch.float32)\n",
    "X_train_res, X_val_res, y_train_res, y_val_res = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "X_train_res = torch.tensor(X_train_res.values, dtype=torch.float32)\n",
    "y_train_res = torch.tensor(y_train_res.values, dtype=torch.float32)\n",
    "X_val_res = torch.tensor(X_val_res.values, dtype=torch.float32)\n",
    "y_val_res = torch.tensor(y_val_res.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Total Loss: 2058.0098, Numerical Loss: 2052.6116, Categorical Loss: 5.3981, Combined Loss: 7.4507, weights: 0.0010, 1.0000\n",
      "Epoch [2/10], Total Loss: 1963.4072, Numerical Loss: 1956.9991, Categorical Loss: 6.4080, Combined Loss: 8.3650, weights: 0.0010, 1.0000\n",
      "Epoch [3/10], Total Loss: 2829.2285, Numerical Loss: 2822.4009, Categorical Loss: 6.8275, Combined Loss: 9.6499, weights: 0.0010, 1.0000\n",
      "Epoch [4/10], Total Loss: 3277.5774, Numerical Loss: 3272.9563, Categorical Loss: 4.6212, Combined Loss: 7.8942, weights: 0.0010, 1.0000\n",
      "Epoch [5/10], Total Loss: 2792.4116, Numerical Loss: 2786.3699, Categorical Loss: 6.0418, Combined Loss: 8.8282, weights: 0.0010, 1.0000\n",
      "Epoch [6/10], Total Loss: 2913.7444, Numerical Loss: 2909.1914, Categorical Loss: 4.5529, Combined Loss: 7.4621, weights: 0.0010, 1.0000\n",
      "Epoch [7/10], Total Loss: 2467.3235, Numerical Loss: 2459.0903, Categorical Loss: 8.2333, Combined Loss: 10.6923, weights: 0.0010, 1.0000\n",
      "Epoch [8/10], Total Loss: 2952.6052, Numerical Loss: 2946.5930, Categorical Loss: 6.0121, Combined Loss: 8.9587, weights: 0.0010, 1.0000\n",
      "Epoch [9/10], Total Loss: 1593.1696, Numerical Loss: 1587.4396, Categorical Loss: 5.7300, Combined Loss: 7.3174, weights: 0.0010, 1.0000\n",
      "Epoch [10/10], Total Loss: 136.3162, Numerical Loss: 130.3729, Categorical Loss: 5.9433, Combined Loss: 6.0736, weights: 0.0010, 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 2. Training\n",
    "parameters = {\n",
    "    \"batch_size\" : 16,\n",
    "    \"hidden_size\" : 32,\n",
    "    \"num_epochs\" : 10,\n",
    "    \"weight_numerical\" : 10e-4,\n",
    "    \"weight_categorical\" : 1.0,\n",
    "    \"association_dict\": association_dict_com,\n",
    "    \"unique_values\": unique_values_com,\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"lr\": 0.01,\n",
    "    \"num_classes_categorical\":y_com.shape[1],}\n",
    "model_com, filename_com = train_lstm(X_com, y_com, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Total Loss: 57729.2969, Numerical Loss: 57722.6641, Categorical Loss: 6.6339, Combined Loss: 12.4061, weights: 0.0001, 1.0000\n",
      "Epoch [2/10], Total Loss: 98415.9531, Numerical Loss: 98408.3438, Categorical Loss: 7.6079, Combined Loss: 17.4487, weights: 0.0001, 1.0000\n",
      "Epoch [3/10], Total Loss: 76382.7188, Numerical Loss: 76375.7656, Categorical Loss: 6.9552, Combined Loss: 14.5928, weights: 0.0001, 1.0000\n",
      "Epoch [4/10], Total Loss: 104351.8984, Numerical Loss: 104345.6016, Categorical Loss: 6.3005, Combined Loss: 16.7350, weights: 0.0001, 1.0000\n",
      "Epoch [5/10], Total Loss: 84287.1484, Numerical Loss: 84280.3516, Categorical Loss: 6.7943, Combined Loss: 15.2223, weights: 0.0001, 1.0000\n",
      "Epoch [6/10], Total Loss: 96448.3359, Numerical Loss: 96441.8359, Categorical Loss: 6.5008, Combined Loss: 16.1450, weights: 0.0001, 1.0000\n",
      "Epoch [7/10], Total Loss: 105557.9766, Numerical Loss: 105551.2188, Categorical Loss: 6.7565, Combined Loss: 17.3116, weights: 0.0001, 1.0000\n",
      "Epoch [8/10], Total Loss: 84970.9219, Numerical Loss: 84963.4062, Categorical Loss: 7.5155, Combined Loss: 16.0118, weights: 0.0001, 1.0000\n",
      "Epoch [9/10], Total Loss: 84620.1875, Numerical Loss: 84612.9844, Categorical Loss: 7.2044, Combined Loss: 15.6657, weights: 0.0001, 1.0000\n",
      "Epoch [10/10], Total Loss: 112853.3047, Numerical Loss: 112846.7969, Categorical Loss: 6.5092, Combined Loss: 17.7939, weights: 0.0001, 1.0000\n"
     ]
    }
   ],
   "source": [
    "parameters_res = {\n",
    "    \"batch_size\" : 16,\n",
    "    \"hidden_size\" : 32,\n",
    "    \"num_epochs\" : 10,\n",
    "    \"weight_numerical\" : 10e-5,\n",
    "    \"weight_categorical\" : 1.0,\n",
    "    \"association_dict\": association_dict_res,\n",
    "    \"unique_values\": unique_values_res,\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 0.01,\n",
    "    \"num_classes_categorical\":y_res.shape[1],}\n",
    "model_res, filename_res = train_lstm(X_res, y_res, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\kai\\preprocessing\\run.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 GB\n",
      "Cached memory: 0.00 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 12.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:03<00:00, 10.72it/s]\n",
      "c:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\kai\\preprocessing\\run.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.01 GB\n",
      "Cached memory: 0.02 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.99 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_stock_type ['residential' 'commercial']\n",
      "in.comstock_building_type_group_com ['nan' 'Mercantile' 'Office']\n",
      "in.heating_fuel_com ['nan' 'NaturalGas']\n",
      "in.hvac_category_com ['nan' 'Small Packaged Unit' 'Multizone CAV/VAV']\n",
      "in.number_of_stories_com ['nan' '2']\n",
      "in.ownership_type_com ['nan' 'owner_occupied' 'leased']\n",
      "in.vintage_com ['nan' 'Before 1946']\n",
      "in.wall_construction_type_com ['nan' 'WoodFramed']\n",
      "in.tstat_clg_sp_f..f_com ['nan' '73' '72' '71' '74' '75' '70' '76' '69']\n",
      "in.tstat_htg_sp_f..f_com ['nan' '69' '68' '67' '66' '70' '65' '64' '61' '63']\n",
      "in.weekday_opening_time..hr_com ['nan' '10.25' '10.0' '10.5' '9.75' '9.5' '9.0' '9.25']\n",
      "in.weekday_operating_hours..hr_com ['nan' '10.5' '10.25' '11.0' '9.75' '9.25' '9.0' '10.0' '10.75' '9.5'\n",
      " '8.75' '8.5']\n",
      "in.bedrooms_res ['2' '3' 'nan' '4']\n",
      "in.cooling_setpoint_res ['62F' '72F' 'nan' '70F' '60F' '68F' '67F' '65F' '76F' '75F']\n",
      "in.heating_setpoint_res ['60F' '68F' 'nan' '67F' '55F' '70F' '65F' '72F' '62F' '75F']\n",
      "in.geometry_building_type_recs_res ['Single-Family Detached' 'nan']\n",
      "in.geometry_floor_area_res ['0-499' 'nan']\n",
      "in.geometry_foundation_type_res ['Slab' 'nan']\n",
      "in.geometry_wall_type_res ['Wood Frame' 'nan']\n",
      "in.heating_fuel_res ['Natural Gas' 'nan' 'Electricity']\n",
      "in.income_res ['<10000' 'nan']\n",
      "in.roof_material_res ['Composition Shingles' 'nan' 'Asphalt Shingles, Medium']\n",
      "in.tenure_res ['Owner' 'nan' 'Renter']\n",
      "in.vacancy_status_res ['Occupied' 'nan' 'Vacant']\n",
      "in.vintage_res ['<1940' 'nan']\n"
     ]
    }
   ],
   "source": [
    "# 3. Predictions\n",
    "parameters_com = {\n",
    "    \"batch_size\" : 16,\n",
    "    \"hidden_size\" : 32,\n",
    "    \"num_epochs\" : 10,\n",
    "    \"weight_numerical\" : 10e-4,\n",
    "    \"weight_categorical\" : 1.0,\n",
    "    \"association_dict\": association_dict_com,\n",
    "    \"unique_values\": unique_values_com,\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"lr\": 0.01,\n",
    "    \"num_classes_categorical\" : y_com.shape[1],}\n",
    "df_test_com = df_test[df_test.building_stock_type == 1]\n",
    "arr_com = predict_lstm(df_test_com, parameters_com, encoder_com, filename=r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\kai\\checkpoints\\com_model_checkpoint_08_15_00_06.pth.tar\")\n",
    "parameters_res = {\n",
    "    \"batch_size\" : 16,\n",
    "    \"hidden_size\" : 32,\n",
    "    \"num_epochs\" : 10,\n",
    "    \"weight_numerical\" : 10e-5,\n",
    "    \"weight_categorical\" : 1.0,\n",
    "    \"association_dict\": association_dict_res,\n",
    "    \"unique_values\": unique_values_res,\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 0.01,\n",
    "    \"num_classes_categorical\":y_res.shape[1],}\n",
    "\n",
    "df_test_res = df_test[df_test.building_stock_type == 0]\n",
    "arr_res = predict_lstm(df_test_res, parameters_res, encoder_res, filename=r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\kai\\checkpoints\\res_model_checkpoint_08_15_00_15.pth.tar\")\n",
    "# 4. Submission\n",
    "df = create_submission(arr_com, arr_res, df_test, save_filepath=\"submission.parquet\")\n",
    "for col in df.columns:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 GB\n",
      "Cached memory: 0.00 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 12.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import print_gpu_memory\n",
    "print_gpu_memory()\n",
    "import os\n",
    "import sys\n",
    "base_path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\"\n",
    "data_path = r\"\\data\\building-instinct-starter-notebook\\Starter notebook\"\n",
    "sys.path.append(base_path+data_path)\n",
    "sys.path.append(base_path+\"\\kai\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from preprocessing import Preprocessor\n",
    "df_features = pd.read_parquet(base_path + '/preprocessed_data/standard_data.parquet', engine='pyarrow')\n",
    "df_features.sort_index(inplace=True)\n",
    "\n",
    "load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "df_targets.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992323346090601\n",
      "[[765   2]\n",
      " [  9 664]]\n"
     ]
    }
   ],
   "source": [
    "from run import simple_classification\n",
    "\n",
    "X, y = df_features, df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "\n",
    "df_test = pd.read_parquet(base_path + '/preprocessed_data/data_test.parquet', engine='pyarrow')\n",
    "df_test.sort_index(inplace=True)\n",
    "\n",
    "y_pred = simple_classification(X, y, df_test, print_performance=True)\n",
    "df_test[\"building_stock_type\"] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for commercial and residential predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import TargetPreprocessor\n",
    "\n",
    "df_targets_res = df_targets[df_targets.building_stock_type == \"residential\"].filter(like='_res').copy()\n",
    "df_targets_com = df_targets[df_targets.building_stock_type == \"commercial\"].filter(like='_com').copy()\n",
    "target_preprocessor = TargetPreprocessor()\n",
    "df_targets_res, association_dict_res, encoder_res = target_preprocessor.preprocess_res(df_targets_res)\n",
    "df_targets_com, association_dict_com, encoder_com = target_preprocessor.preprocess_com(df_targets_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values(df):\n",
    "    unique_values = {}\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() > 2:\n",
    "            unique_values[col] = df[col].unique()\n",
    "    return unique_values\n",
    "unique_values_res = unique_values(df_targets_res)\n",
    "unique_values_com = unique_values(df_targets_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indices = df_features.index.intersection(df_targets_com.index)\n",
    "# Filter the data\n",
    "X_com = df_features[df_features.index.isin(common_indices)]\n",
    "X_com = torch.tensor(X_com.values, dtype=torch.float32)\n",
    "y_com = df_targets_com[df_targets_com.index.isin(common_indices)]\n",
    "y_com = torch.tensor(y_com.values, dtype=torch.float32)\n",
    "\n",
    "X_res = df_features[df_features.index.isin(df_targets_res.index)]\n",
    "X_res = torch.tensor(X_res.values, dtype=torch.float32)\n",
    "y_res = df_targets_res[df_targets_res.index.isin(df_features.index)]\n",
    "y_res = torch.tensor(y_res.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## com model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Total Loss: 2989.7385, Numerical Loss: 2981.7322, Categorical Loss: 8.0063, Combined Loss: 10.9880, weights: 0.0010, 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_lstm\n\u001b[0;32m      2\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m82\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes_categorial\u001b[39m\u001b[38;5;124m\"\u001b[39m:y_com\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],}\n\u001b[1;32m---> 14\u001b[0m model_com \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_com\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_com\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\\kai\\preprocessing\\run.py:110\u001b[0m, in \u001b[0;36mtrain_lstm\u001b[1;34m(X, y, parameters)\u001b[0m\n\u001b[0;32m    107\u001b[0m     combined_loss \u001b[38;5;241m=\u001b[39m weight_numerical \u001b[38;5;241m*\u001b[39m loss_numerical \u001b[38;5;241m+\u001b[39m weight_categorical \u001b[38;5;241m*\u001b[39m loss_categorical\n\u001b[0;32m    109\u001b[0m     combined_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 110\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    111\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    113\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    114\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumerical Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_numerical\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    115\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategorical Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_categorical\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    116\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    117\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_numerical\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_categorical\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from run import train_lstm\n",
    "parameters = {\n",
    "    \"batch_size\" : 16,\n",
    "    \"hidden_size\" : 82,\n",
    "    \"num_epochs\" : 25,\n",
    "    \"weight_numerical\" : 10e-4,\n",
    "    \"weight_categorical\" : 1.0,\n",
    "    \"association_dict\": association_dict_com,\n",
    "    \"unique_values\": unique_values_com,\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 0.1,\n",
    "    \"num_classes_categorial\":y_com.shape[1],}\n",
    "\n",
    "model_com = train_lstm(X_com, y_com, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import predict_lstm\n",
    "df_test_com = df_test[df_test.building_stock_type == 1]\n",
    "# print(df_test_com[df_test_com.columns.difference([\"building_stock_type\"])].shape)\n",
    "X_test_com, y_pred_com = torch.tensor(df_test_com[df_test_com.columns.difference([\"building_stock_type\"])].values, dtype=torch.float32), df_test_com[\"building_stock_type\"]\n",
    "arr_com = predict_lstm(X_test_com, parameters, encoder_com, filename='/kai/checkpoints/com_model_checkpoint_08_13_23_14.pth.tar')\n",
    "for col in arr_com.columns:\n",
    "    print(arr_com[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KAI\\AppData\\Local\\Temp\\ipykernel_8236\\1250800709.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5293e+00, 1.9764e+03, 7.2813e+01, 6.8851e+01, 7.6482e+00, 1.0340e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5634e+00, 1.9740e+03, 7.2779e+01, 6.8623e+01, 7.6589e+00, 9.7448e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5879e+00, 1.9668e+03, 7.2692e+01, 6.8938e+01, 7.4934e+00, 1.0317e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5267e+00, 1.9784e+03, 7.2868e+01, 6.8746e+01, 7.6929e+00, 1.0096e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5937e+00, 1.9705e+03, 7.2706e+01, 6.8551e+01, 7.6284e+00, 9.5066e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5734e+00, 1.9731e+03, 7.2765e+01, 6.8610e+01, 7.6498e+00, 9.6687e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5944e+00, 1.9646e+03, 7.2639e+01, 6.8901e+01, 7.4703e+00, 1.0279e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5388e+00, 1.9711e+03, 7.2682e+01, 6.8816e+01, 7.5872e+00, 1.0408e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5984e+00, 1.9688e+03, 7.2654e+01, 6.8500e+01, 7.6199e+00, 9.4605e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5828e+00, 1.9722e+03, 7.2749e+01, 6.8593e+01, 7.6409e+00, 9.5958e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5328e+00, 1.9736e+03, 7.2740e+01, 6.8825e+01, 7.6177e+00, 1.0378e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5460e+00, 1.9703e+03, 7.2675e+01, 6.8832e+01, 7.5715e+00, 1.0402e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5964e+00, 1.9697e+03, 7.2681e+01, 6.8527e+01, 7.6239e+00, 9.4808e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5311e+00, 1.9770e+03, 7.2832e+01, 6.8682e+01, 7.6888e+00, 1.0005e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5291e+00, 1.9778e+03, 7.2848e+01, 6.8723e+01, 7.6909e+00, 1.0067e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5620e+00, 1.9741e+03, 7.2780e+01, 6.8625e+01, 7.6601e+00, 9.7553e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([1.0000e+00, 1.9750e+03, 7.2000e+01, 6.8000e+01, 7.2500e+00, 6.7500e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from models import MultiTaskLSTM, CustomLoss, TimeSeriesDataset\n",
    "# from datetime import datetime\n",
    "\n",
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# batch_size, input_size, sequence_length, hidden_size, num_classes_categorical = 16, 1, X_com.shape[1], 64, y_com.shape[1]\n",
    "# # checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_08_06_14_49.pth.tar'\n",
    "# checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_08_13_23_14.pth.tar'\n",
    "# dataloader_com = DataLoader(TimeSeriesDataset(X_com, y_com), batch_size=batch_size, shuffle=True)\n",
    "# model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "# model = model.to(device)\n",
    "\n",
    "# checkpoint = torch.load(checkpoint_filename)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# for X_batch, y_categorical_batch in dataloader_com:\n",
    "#     for i in [0, 1]:\n",
    "#         _X_batch = X_batch[:, :].view(X_batch.shape[0], sequence_length, input_size)\n",
    "#         _X_batch = _X_batch.to(device)\n",
    "#         categorical_pred = model.predict(_X_batch, association_dict_com)\n",
    "#         print(categorical_pred)\n",
    "#         print(y_categorical_batch[i, :])\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the original com df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in.number_of_stories_com</th>\n",
       "      <th>in.vintage_com</th>\n",
       "      <th>in.tstat_clg_sp_f..f_com</th>\n",
       "      <th>in.tstat_htg_sp_f..f_com</th>\n",
       "      <th>in.weekday_opening_time..hr_com</th>\n",
       "      <th>in.weekday_operating_hours..hr_com</th>\n",
       "      <th>in.comstock_building_type_group_com</th>\n",
       "      <th>in.heating_fuel_com</th>\n",
       "      <th>in.hvac_category_com</th>\n",
       "      <th>in.ownership_type_com</th>\n",
       "      <th>in.wall_construction_type_com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1960 to 1969</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Multizone CAV/VAV</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1960 to 1969</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Multizone CAV/VAV</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1960 to 1969</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1960 to 1969</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    in.number_of_stories_com in.vintage_com  in.tstat_clg_sp_f..f_com  \\\n",
       "0                          2   1970 to 1979                        73   \n",
       "1                          2   1970 to 1979                        73   \n",
       "2                          2   1960 to 1969                        73   \n",
       "3                          2   1970 to 1979                        73   \n",
       "4                          2   1970 to 1979                        73   \n",
       "5                          2   1970 to 1979                        73   \n",
       "6                          2   1960 to 1969                        73   \n",
       "7                          2   1970 to 1979                        73   \n",
       "8                          2   1960 to 1969                        73   \n",
       "9                          2   1970 to 1979                        73   \n",
       "10                         2   1970 to 1979                        73   \n",
       "11                         2   1970 to 1979                        73   \n",
       "12                         2   1960 to 1969                        73   \n",
       "13                         2   1970 to 1979                        73   \n",
       "14                         2   1970 to 1979                        73   \n",
       "15                         2   1970 to 1979                        73   \n",
       "\n",
       "    in.tstat_htg_sp_f..f_com in.weekday_opening_time..hr_com  \\\n",
       "0                         69                            7.75   \n",
       "1                         69                            7.75   \n",
       "2                         69                             7.5   \n",
       "3                         69                            7.75   \n",
       "4                         69                            7.75   \n",
       "5                         69                            7.75   \n",
       "6                         69                             7.5   \n",
       "7                         69                             7.5   \n",
       "8                         69                             7.5   \n",
       "9                         69                            7.75   \n",
       "10                        69                             7.5   \n",
       "11                        69                             7.5   \n",
       "12                        69                             7.5   \n",
       "13                        69                            7.75   \n",
       "14                        69                            7.75   \n",
       "15                        69                            7.75   \n",
       "\n",
       "   in.weekday_operating_hours..hr_com in.comstock_building_type_group_com  \\\n",
       "0                               10.25                          Mercantile   \n",
       "1                                9.75                          Mercantile   \n",
       "2                               10.25                          Mercantile   \n",
       "3                                10.0                          Mercantile   \n",
       "4                                 9.5                              Office   \n",
       "5                                9.75                              Office   \n",
       "6                               10.25                          Mercantile   \n",
       "7                                10.5                          Mercantile   \n",
       "8                                 9.5                              Office   \n",
       "9                                 9.5                              Office   \n",
       "10                               10.5                          Mercantile   \n",
       "11                               10.5                          Mercantile   \n",
       "12                                9.5                              Office   \n",
       "13                               10.0                          Mercantile   \n",
       "14                               10.0                          Mercantile   \n",
       "15                               9.75                          Mercantile   \n",
       "\n",
       "   in.heating_fuel_com in.hvac_category_com in.ownership_type_com  \\\n",
       "0           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "1           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "2           NaturalGas    Multizone CAV/VAV        owner_occupied   \n",
       "3           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "4           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "5           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "6           NaturalGas    Multizone CAV/VAV        owner_occupied   \n",
       "7           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "8           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "9           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "10          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "11          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "12          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "13          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "14          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "15          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "\n",
       "   in.wall_construction_type_com  \n",
       "0                           Mass  \n",
       "1                           Mass  \n",
       "2                           Mass  \n",
       "3                           Mass  \n",
       "4                           Mass  \n",
       "5                           Mass  \n",
       "6                           Mass  \n",
       "7                           Mass  \n",
       "8                           Mass  \n",
       "9                           Mass  \n",
       "10                          Mass  \n",
       "11                          Mass  \n",
       "12                          Mass  \n",
       "13                          Mass  \n",
       "14                          Mass  \n",
       "15                          Mass  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from postprocessing import inverse_process\n",
    "# com_arr_df = inverse_process(categorical_pred, encoder_com) # todo: standardize the target numerical columns in preprocessing\n",
    "# display(com_arr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654, 35040)\n"
     ]
    }
   ],
   "source": [
    "# # file_path = base_path + data_path + r\"\\building-instinct-test-data\"\n",
    "# # df_features = Preprocessor.load_standard_df(file_path)\n",
    "# # df_features.sort_index(inplace=True)\n",
    "# # df_features.to_parquet(base_path + '/preprocessed_data/data_test.parquet', engine='pyarrow')\n",
    "\n",
    "# # df_test = pd.read_parquet(base_path + '/preprocessed_data/data_test.parquet', engine='pyarrow')\n",
    "# # df_test.sort_index(inplace=True)\n",
    "\n",
    "# # TODO: clf that predicts com or res\n",
    "# # then filter by com & res\n",
    "# df_test_com = df_test[df_test.building_stock_type == 1]\n",
    "# # df_test = df_test[df_test.index.isin(df_features.index)]\n",
    "# # X_test_com = torch.tensor(df_test.values, dtype=torch.float32)\n",
    "# # X_test_com = X_test_com.to(device)\n",
    "# print(df_test_com[df_test_com.columns.difference([\"building_stock_type\"])].shape)\n",
    "# X_test_com, y_pred_com = torch.tensor(df_test_com[df_test_com.columns.difference([\"building_stock_type\"])].values, dtype=torch.float32), df_test_com[\"building_stock_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KAI\\AppData\\Local\\Temp\\ipykernel_8236\\4240090578.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 0.03 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:11<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "# def free_gpu_memory():\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.synchronize()\n",
    "# free_gpu_memory()\n",
    "# model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "# model = model.to(device)\n",
    "# # criterion = CustomLoss(association_dict_com)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# # checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_08_06_14_49.pth.tar'\n",
    "# checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_08_13_23_14.pth.tar'\n",
    "# checkpoint = torch.load(checkpoint_filename)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# best_loss = checkpoint['loss']\n",
    "\n",
    "# predictions = []\n",
    "# dataloader = DataLoader(TimeSeriesDataset(X_test_com, torch.zeros(X_test_com.shape[0], y_com.shape[1])), batch_size=16, shuffle=False)\n",
    "\n",
    "# print_gpu_memory()\n",
    "# for X_batch, y_categorical_batch in tqdm(dataloader):\n",
    "#     model.eval()\n",
    "#     _X_batch = X_batch.view(X_batch.shape[0], sequence_length, input_size)\n",
    "#     _X_batch = _X_batch.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         categorical_pred = model.predict(_X_batch, association_dict_com)\n",
    "#     predictions.append(categorical_pred.cpu())\n",
    "#     del _X_batch, categorical_pred\n",
    "#     free_gpu_memory()\n",
    "# predictions = torch.cat(predictions, dim=0)\n",
    "# arr_df_com = inverse_process(predictions, encoder_com)\n",
    "# arr_df_com.index = df_test_com.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "['1970 to 1979' '1960 to 1969']\n",
      "[73]\n",
      "[69 68]\n",
      "[7.5 7.75]\n",
      "[10.25 10.5 10.0 9.75 9.5]\n",
      "['Mercantile' 'Office']\n",
      "['NaturalGas']\n",
      "['Small Packaged Unit' 'Multizone CAV/VAV']\n",
      "['owner_occupied']\n",
      "['Mass']\n"
     ]
    }
   ],
   "source": [
    "# for col in arr_df_com.columns:\n",
    "#     print(arr_df_com[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Define custom loss function\n",
    "class CustomLoss_res(nn.Module):\n",
    "    def __init__(self, column_groups, valid_labels):\n",
    "        super(CustomLoss_res, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.column_groups = column_groups  # Dictionary mapping attribute prefixes to column indices\n",
    "        self.valid_labels = {k: torch.tensor(v).float() for k, v in valid_labels.items()}  # Convert valid labels to tensor\n",
    "        self.end_numerical = int(min([min(v) for v in self.column_groups.values()]))\n",
    "\n",
    "    def custom_closest_loss(self, predicted, true, valid_labels):\n",
    "        # Expand dimensions to allow broadcasting (batch_size, num_valid_labels)\n",
    "        predicted_expanded = predicted.unsqueeze(1)  # (batch_size, 1)\n",
    "        valid_labels_expanded = valid_labels.unsqueeze(0)  # (1, num_valid_labels)\n",
    "        \n",
    "        # Calculate the absolute differences\n",
    "        distances = torch.abs(predicted_expanded - valid_labels_expanded)  # (batch_size, num_valid_labels)\n",
    "        \n",
    "        # Find the closest valid label (index of the smallest distance)\n",
    "        min_distances, min_indices = torch.min(distances, dim=1)  # min_distances: (batch_size,), min_indices: (batch_size,)\n",
    "        \n",
    "        # Get the corresponding closest labels\n",
    "        closest_labels = valid_labels[min_indices]  # (batch_size,)\n",
    "        \n",
    "        # Compute the loss only where the closest label is not equal to the true label\n",
    "        mask = closest_labels != true\n",
    "        loss = torch.abs(predicted[mask] - true[mask]).mean()  # Mean absolute error over all incorrect predictions\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, categorical_pred, categorical_true):\n",
    "\n",
    "        loss_numerical = 0.0\n",
    "        i = 0\n",
    "        # Loop over each attribute group\n",
    "        for attr, labels in self.valid_labels.items():# TODO: maybe speed it up by inserting all labels as a matrix and performing matrix operations\n",
    "            # Calculate the loss for this group using the valid labels\n",
    "            loss_numerical += self.custom_closest_loss(categorical_pred[:, i], categorical_true[:, i], labels)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Compute numerical loss (assuming the first few columns are numerical)\n",
    "        # loss_numerical = self.custom_closest_loss(categorical_pred[:, :self.end_numerical], categorical_true[:, :self.end_numerical])\n",
    "        # loss_numerical = self.mse_loss(categorical_pred[:, :self.end_numerical], categorical_true[:, :self.end_numerical])\n",
    "\n",
    "        # Initialize categorical loss\n",
    "        loss_categorical = 0.0\n",
    "        \n",
    "        # For each attribute group, compute the cross-entropy loss\n",
    "        for attr, indices in self.column_groups.items():\n",
    "            # Extract logits for the current attribute\n",
    "            logits = categorical_pred[:, indices]\n",
    "            \n",
    "            # Extract the true labels for the current attribute\n",
    "            # Convert one-hot encoding to class indices\n",
    "            true_labels = torch.argmax(categorical_true[:, indices], dim=1)\n",
    "            \n",
    "            # Compute cross-entropy loss for the current attribute\n",
    "            loss_categorical += F.cross_entropy(logits, true_labels)\n",
    "\n",
    "        total_loss = loss_numerical + loss_categorical\n",
    "        return total_loss, loss_numerical, loss_categorical   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Total Loss: 72215.5859, Numerical Loss: 72208.4219, Categorical Loss: 7.1626, Combined Loss: 79.3710, weights: 0.0010, 1.0000\n",
      "Epoch [2/20], Total Loss: 70440.7812, Numerical Loss: 70434.2188, Categorical Loss: 6.5609, Combined Loss: 76.9951, weights: 0.0010, 1.0000\n",
      "Epoch [3/20], Total Loss: 99618.4062, Numerical Loss: 99611.2344, Categorical Loss: 7.1681, Combined Loss: 106.7793, weights: 0.0010, 1.0000\n",
      "Epoch [4/20], Total Loss: 101878.4922, Numerical Loss: 101872.8750, Categorical Loss: 5.6162, Combined Loss: 107.4891, weights: 0.0010, 1.0000\n",
      "Epoch [5/20], Total Loss: 68431.3125, Numerical Loss: 68425.3828, Categorical Loss: 5.9283, Combined Loss: 74.3537, weights: 0.0010, 1.0000\n",
      "Epoch [6/20], Total Loss: 62581.7617, Numerical Loss: 62576.2070, Categorical Loss: 5.5551, Combined Loss: 68.1314, weights: 0.0010, 1.0000\n",
      "Epoch [7/20], Total Loss: 81158.4453, Numerical Loss: 81151.3750, Categorical Loss: 7.0724, Combined Loss: 88.2238, weights: 0.0010, 1.0000\n",
      "Epoch [8/20], Total Loss: 95010.0625, Numerical Loss: 95003.9609, Categorical Loss: 6.1023, Combined Loss: 101.1063, weights: 0.0010, 1.0000\n",
      "Epoch [9/20], Total Loss: 92636.0547, Numerical Loss: 92629.1641, Categorical Loss: 6.8875, Combined Loss: 99.5167, weights: 0.0010, 1.0000\n",
      "Epoch [10/20], Total Loss: 79122.4219, Numerical Loss: 79115.6016, Categorical Loss: 6.8201, Combined Loss: 85.9357, weights: 0.0010, 1.0000\n",
      "Epoch [11/20], Total Loss: 76308.3438, Numerical Loss: 76300.3125, Categorical Loss: 8.0311, Combined Loss: 84.3314, weights: 0.0010, 1.0000\n",
      "Epoch [12/20], Total Loss: 66294.1484, Numerical Loss: 66287.3438, Categorical Loss: 6.8069, Combined Loss: 73.0942, weights: 0.0010, 1.0000\n",
      "Epoch [13/20], Total Loss: 110709.2812, Numerical Loss: 110701.2344, Categorical Loss: 8.0502, Combined Loss: 118.7514, weights: 0.0010, 1.0000\n",
      "Epoch [14/20], Total Loss: 100721.2891, Numerical Loss: 100715.2188, Categorical Loss: 6.0722, Combined Loss: 106.7874, weights: 0.0010, 1.0000\n",
      "Epoch [15/20], Total Loss: 106180.7344, Numerical Loss: 106173.9141, Categorical Loss: 6.8205, Combined Loss: 112.9945, weights: 0.0010, 1.0000\n",
      "Epoch [16/20], Total Loss: 56932.9062, Numerical Loss: 56926.1992, Categorical Loss: 6.7060, Combined Loss: 63.6322, weights: 0.0010, 1.0000\n",
      "Epoch [17/20], Total Loss: 111957.0312, Numerical Loss: 111950.8906, Categorical Loss: 6.1386, Combined Loss: 118.0895, weights: 0.0010, 1.0000\n",
      "Epoch [18/20], Total Loss: 91562.3750, Numerical Loss: 91556.7109, Categorical Loss: 5.6651, Combined Loss: 97.2218, weights: 0.0010, 1.0000\n",
      "Epoch [19/20], Total Loss: 118411.7109, Numerical Loss: 118405.1094, Categorical Loss: 6.6012, Combined Loss: 125.0063, weights: 0.0010, 1.0000\n",
      "Epoch [20/20], Total Loss: 59289.5234, Numerical Loss: 59281.9258, Categorical Loss: 7.5981, Combined Loss: 66.8801, weights: 0.0010, 1.0000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from models import MultiTaskLSTM, CustomLoss, TimeSeriesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# model parameters\n",
    "batch_size = 16\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_size, sequence_length = 1, X_res.shape[1]\n",
    "num_classes_categorical = y_res.shape[1]\n",
    "hidden_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "weight_numerical = 10e-4\n",
    "weight_categorical = 1.0\n",
    "\n",
    "best_combined_loss = float('inf')\n",
    "checkpoint_filename = base_path+ f'/kai/checkpoints/res_model_checkpoint_{datetime.now().strftime(\"%m_%d_%H_%M\")}.pth.tar'\n",
    "\n",
    "# create dataloaders\n",
    "# dataloader_com = DataLoader(TimeSeriesDataset(X_com, y_com), batch_size=batch_size, shuffle=True)\n",
    "dataloader_res = DataLoader(TimeSeriesDataset(X_res, y_res), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "model = model.to(device)\n",
    "criterion = CustomLoss_1(association_dict_res, unique_values_res)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_categorical_batch in dataloader_res:\n",
    "        X_batch = X_batch.view(X_batch.shape[0], sequence_length, input_size)\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_categorical_batch = y_categorical_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        categorical_pred = model(X_batch)\n",
    "        loss, loss_numerical, loss_categorical = criterion(categorical_pred, y_categorical_batch)\n",
    "        \n",
    "        # Combine losses with adjusted weights\n",
    "        combined_loss = weight_numerical * loss_numerical + weight_categorical * loss_categorical\n",
    "\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Total Loss: {loss.item():.4f}, '\n",
    "                  f'Numerical Loss: {loss_numerical.item():.4f}, '\n",
    "                  f'Categorical Loss: {loss_categorical.item():.4f}, '\n",
    "                  f'Combined Loss: {combined_loss.item():.4f}, '\n",
    "                  f'weights: {weight_numerical:.4f}, {weight_categorical:.4f}')\n",
    "    \n",
    "    if combined_loss.item() < best_combined_loss:\n",
    "        torch.save({'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': combined_loss.item(),}, checkpoint_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### todo postprocessing + analysis of res and com labels in general! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_closest_values(predictions, possible_values):\n",
    "    \"\"\"\n",
    "    Map predicted values to the closest possible values.\n",
    "\n",
    "    Args:\n",
    "    - predictions (list of lists): The predicted values.\n",
    "    - possible_values (dict): A dictionary where keys are column names and values are lists of possible values.\n",
    "\n",
    "    Returns:\n",
    "    - mapped_predictions (list of lists): The predictions mapped to the closest possible values.\n",
    "    \"\"\"\n",
    "    def closest_value(predicted, possible):\n",
    "        return possible[np.argmin((np.array(possible) - predicted)**2)]\n",
    "    \n",
    "    mapped_predictions = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        mapped_row = []\n",
    "        for col_name, pred_value in zip(possible_values.keys(), prediction):\n",
    "            mapped_value = closest_value(pred_value, possible_values[col_name])\n",
    "            mapped_row.append(mapped_value)\n",
    "        mapped_predictions.append(mapped_row)\n",
    "    \n",
    "    return mapped_predictions\n",
    "\n",
    "def inverse_process_res(prediction, encoder):\n",
    "    prediction = prediction.cpu().detach().numpy()\n",
    "\n",
    "    # Possible values for each column\n",
    "    possible_values = {\n",
    "        'in.bedrooms_res': [3, 2, 1, 4, 5],\n",
    "        'in.cooling_setpoint_res': [68, 75, 72, 70, 78, 80, 60, 65, 76, 67, 62],\n",
    "        'in.heating_setpoint_res': [75, 72, 68, 70, 62, 55, 65, 78, 76, 67, 60, 80],\n",
    "        'in.geometry_floor_area_res': [1749,  874, 1249,  624, 2249, 4000, 2749, 3499,  249],\n",
    "        'in.income_res': [109999,  12499,  64999,  89999,  42499,  22499,  47499, 169999, 129999,\n",
    "                            200000,  37499,  54999,  32499,  10000, 189999,  74999,  17499,  27499,\n",
    "                            149999],\n",
    "        'in.vintage_res': [1940, 1975, 1985, 1945, 1995, 1955, 1965, 2005, 2015]\n",
    "    }\n",
    "    mapped_predictions = prediction.copy()\n",
    "    # Map the predictions\n",
    "    mapped_predictions_1 = map_to_closest_values(prediction[:, :6], possible_values)\n",
    "    mapped_predictions_2 = encoder.inverse_transform(prediction[:, 6:])\n",
    "\n",
    "    mapped_predictions = np.concatenate((mapped_predictions_1, mapped_predictions_2), axis=1)\n",
    "    cols = ['in.bedrooms_res', 'in.cooling_setpoint_res', 'in.heating_setpoint_res',\n",
    "            'in.geometry_floor_area_res', 'in.income_res', 'in.vintage_res',\n",
    "            'in.geometry_building_type_recs_res', 'in.geometry_foundation_type_res', \n",
    "            'in.geometry_wall_type_res', 'in.heating_fuel_res', 'in.roof_material_res',\n",
    "            'in.tenure_res', 'in.vacancy_status_res']\n",
    "    \n",
    "    mapped_df = pd.DataFrame(mapped_predictions, columns=cols)\n",
    "\n",
    "    vintage_mapping = {1940:'<1940', 1945:'1940s', 1955:'1950s', 1965:'1960s',\n",
    "                           1975:'1970s', 1985:'1980s', 1995:'1990s', 2005:'2000s', 2015:'2010s'}\n",
    "    mapped_df['in.vintage_res'] = mapped_df['in.vintage_res'].map(vintage_mapping)\n",
    "    mapped_df['in.cooling_setpoint_res'] = mapped_df['in.cooling_setpoint_res'].apply(lambda x: str(x)+\"F\").astype(str)\n",
    "    mapped_df['in.heating_setpoint_res'] = mapped_df['in.heating_setpoint_res'].apply(lambda x: str(x)+\"F\").astype(str)\n",
    "    mapped_df['in.bedrooms_res'] = mapped_df['in.bedrooms_res'].astype(str)\n",
    "\n",
    "    income_mapping = {109999: '100000-119999', 12499: '10000-14999', 64999: '60000-69999', 89999: '80000-99999', \n",
    "                      42499: '40000-44999', 22499: '20000-24999', 47499: '45000-49999', 169999: '160000-179999', \n",
    "                      129999: '120000-139999', 200000: '200000+', 37499: '35000-39999', 54999: '50000-59999', \n",
    "                      32499: '30000-34999', 10000: '<10000', 189999: '180000-199999', 74999: '70000-79999', \n",
    "                      17499: '15000-19999', 27499: '25000-29999', 149999: '140000-159999'}\n",
    "    mapped_df['in.income_res'] = mapped_df['in.income_res'].map(income_mapping)\n",
    "\n",
    "    geometry_mapping = {1749: '1500-1999', 874: '750-999', 1249: '1000-1499', 624: '500-749', 2249: '2000-2499',\n",
    "                        4000: '4000+', 2749: '2500-2999', 3499: '3000-3999', 249: '0-499'}\n",
    "    mapped_df['in.geometry_floor_area_res'] = mapped_df['in.geometry_floor_area_res'].map(geometry_mapping)\n",
    "\n",
    "    return mapped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KAI\\AppData\\Local\\Temp\\ipykernel_8236\\1758560243.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 0.03 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.64it/s]\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "def free_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "free_gpu_memory()\n",
    "\n",
    "batch_size = 16\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_size, sequence_length = 1, X_res.shape[1]\n",
    "num_classes_categorical = y_res.shape[1]\n",
    "hidden_size = 64\n",
    "num_epochs = 20\n",
    "model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "model = model.to(device)\n",
    "# criterion = CustomLoss_res(association_dict_res)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# checkpoint_filename = base_path+ f'/kai/checkpoints/res_model_checkpoint_08_09_22_35.pth.tar'\n",
    "checkpoint_filename = base_path+ f'/kai/checkpoints/res_model_checkpoint_08_13_23_37.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_filename)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "best_loss = checkpoint['loss']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "df_test_res = df_test[df_test.building_stock_type == 0]\n",
    "X_test_res, y_pred_res = torch.tensor(df_test_res[df_test_res.columns.difference([\"building_stock_type\"])].values, dtype=torch.float32), df_test_res[\"building_stock_type\"]\n",
    "dataloader = DataLoader(TimeSeriesDataset(X_test_res, torch.zeros(X_test_res.shape[0], y_res.shape[1])), batch_size=16, shuffle=False)\n",
    "\n",
    "print_gpu_memory()\n",
    "for X_batch, y_categorical_batch in tqdm(dataloader):\n",
    "    model.eval()\n",
    "    _X_batch = X_batch.view(X_batch.shape[0], sequence_length, input_size)\n",
    "    _X_batch = _X_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        categorical_pred = model.predict(_X_batch, association_dict_res)\n",
    "    predictions.append(categorical_pred.cpu())\n",
    "    del _X_batch, categorical_pred\n",
    "    free_gpu_memory()\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "arr_df_res = inverse_process_res(predictions, encoder_res)\n",
    "arr_df_res.index = df_test_res.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in.bedrooms_res\n",
      "['2' '3']\n",
      "in.cooling_setpoint_res\n",
      "['72F']\n",
      "in.heating_setpoint_res\n",
      "['70F' '72F' '65F' '68F' '55F' '60F' '62F' '67F']\n",
      "in.geometry_floor_area_res\n",
      "['1500-1999' '1000-1499']\n",
      "in.income_res\n",
      "['<10000']\n",
      "in.vintage_res\n",
      "['1990s' '1980s' '1950s' '1960s' '1970s']\n",
      "in.geometry_building_type_recs_res\n",
      "['Single-Family Detached' 'Multi-Family with 5+ Units']\n",
      "in.geometry_foundation_type_res\n",
      "['Slab']\n",
      "in.geometry_wall_type_res\n",
      "['Wood Frame']\n",
      "in.heating_fuel_res\n",
      "['Natural Gas' 'Electricity']\n",
      "in.roof_material_res\n",
      "['Composition Shingles' 'Asphalt Shingles, Medium']\n",
      "in.tenure_res\n",
      "['Renter' 'Owner']\n",
      "in.vacancy_status_res\n",
      "['Occupied' 'Vacant']\n"
     ]
    }
   ],
   "source": [
    "for col in arr_df_res.columns:\n",
    "    print(col)\n",
    "    print(arr_df_res[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        building_stock_type in.comstock_building_type_group_com  \\\n",
      "bldg_id                                                           \n",
      "1               residential                                 nan   \n",
      "2               residential                                 nan   \n",
      "3                commercial                          Mercantile   \n",
      "4                commercial                          Mercantile   \n",
      "5               residential                                 nan   \n",
      "...                     ...                                 ...   \n",
      "1436             commercial                          Mercantile   \n",
      "1437            residential                                 nan   \n",
      "1438            residential                                 nan   \n",
      "1439             commercial                          Mercantile   \n",
      "1440            residential                                 nan   \n",
      "\n",
      "        in.heating_fuel_com in.hvac_category_com in.number_of_stories_com  \\\n",
      "bldg_id                                                                     \n",
      "1                       nan                  nan                      nan   \n",
      "2                       nan                  nan                      nan   \n",
      "3                NaturalGas  Small Packaged Unit                        2   \n",
      "4                NaturalGas  Small Packaged Unit                        2   \n",
      "5                       nan                  nan                      nan   \n",
      "...                     ...                  ...                      ...   \n",
      "1436             NaturalGas  Small Packaged Unit                        2   \n",
      "1437                    nan                  nan                      nan   \n",
      "1438                    nan                  nan                      nan   \n",
      "1439             NaturalGas  Small Packaged Unit                        2   \n",
      "1440                    nan                  nan                      nan   \n",
      "\n",
      "        in.ownership_type_com in.vintage_com in.wall_construction_type_com  \\\n",
      "bldg_id                                                                      \n",
      "1                         nan            nan                           nan   \n",
      "2                         nan            nan                           nan   \n",
      "3              owner_occupied   1970 to 1979                          Mass   \n",
      "4              owner_occupied   1970 to 1979                          Mass   \n",
      "5                         nan            nan                           nan   \n",
      "...                       ...            ...                           ...   \n",
      "1436           owner_occupied   1970 to 1979                          Mass   \n",
      "1437                      nan            nan                           nan   \n",
      "1438                      nan            nan                           nan   \n",
      "1439           owner_occupied   1970 to 1979                          Mass   \n",
      "1440                      nan            nan                           nan   \n",
      "\n",
      "        in.tstat_clg_sp_f..f_com in.tstat_htg_sp_f..f_com  ...  \\\n",
      "bldg_id                                                    ...   \n",
      "1                            nan                      nan  ...   \n",
      "2                            nan                      nan  ...   \n",
      "3                             73                       69  ...   \n",
      "4                             73                       69  ...   \n",
      "5                            nan                      nan  ...   \n",
      "...                          ...                      ...  ...   \n",
      "1436                          73                       69  ...   \n",
      "1437                         nan                      nan  ...   \n",
      "1438                         nan                      nan  ...   \n",
      "1439                          73                       69  ...   \n",
      "1440                         nan                      nan  ...   \n",
      "\n",
      "        in.geometry_building_type_recs_res in.geometry_floor_area_res  \\\n",
      "bldg_id                                                                 \n",
      "1                   Single-Family Detached                  1500-1999   \n",
      "2                   Single-Family Detached                  1500-1999   \n",
      "3                                      nan                        nan   \n",
      "4                                      nan                        nan   \n",
      "5                   Single-Family Detached                  1500-1999   \n",
      "...                                    ...                        ...   \n",
      "1436                                   nan                        nan   \n",
      "1437                Single-Family Detached                  1000-1499   \n",
      "1438            Multi-Family with 5+ Units                  1500-1999   \n",
      "1439                                   nan                        nan   \n",
      "1440                Single-Family Detached                  1500-1999   \n",
      "\n",
      "        in.geometry_foundation_type_res in.geometry_wall_type_res  \\\n",
      "bldg_id                                                             \n",
      "1                                  Slab                Wood Frame   \n",
      "2                                  Slab                Wood Frame   \n",
      "3                                   nan                       nan   \n",
      "4                                   nan                       nan   \n",
      "5                                  Slab                Wood Frame   \n",
      "...                                 ...                       ...   \n",
      "1436                                nan                       nan   \n",
      "1437                               Slab                Wood Frame   \n",
      "1438                               Slab                Wood Frame   \n",
      "1439                                nan                       nan   \n",
      "1440                               Slab                Wood Frame   \n",
      "\n",
      "        in.heating_fuel_res in.income_res      in.roof_material_res  \\\n",
      "bldg_id                                                               \n",
      "1               Natural Gas        <10000      Composition Shingles   \n",
      "2               Natural Gas        <10000      Composition Shingles   \n",
      "3                       nan           nan                       nan   \n",
      "4                       nan           nan                       nan   \n",
      "5               Natural Gas        <10000      Composition Shingles   \n",
      "...                     ...           ...                       ...   \n",
      "1436                    nan           nan                       nan   \n",
      "1437            Electricity        <10000  Asphalt Shingles, Medium   \n",
      "1438            Natural Gas        <10000  Asphalt Shingles, Medium   \n",
      "1439                    nan           nan                       nan   \n",
      "1440            Natural Gas        <10000  Asphalt Shingles, Medium   \n",
      "\n",
      "        in.tenure_res in.vacancy_status_res in.vintage_res  \n",
      "bldg_id                                                     \n",
      "1              Renter              Occupied          1990s  \n",
      "2               Owner              Occupied          1980s  \n",
      "3                 nan                   nan            nan  \n",
      "4                 nan                   nan            nan  \n",
      "5              Renter              Occupied          1990s  \n",
      "...               ...                   ...            ...  \n",
      "1436              nan                   nan            nan  \n",
      "1437           Renter                Vacant          1950s  \n",
      "1438           Renter              Occupied          1990s  \n",
      "1439              nan                   nan            nan  \n",
      "1440           Renter              Occupied          1990s  \n",
      "\n",
      "[1440 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from run import create_submission\n",
    "\n",
    "df = create_submission(arr_df_com, arr_df_res, df_test, save_filepath=\"submission.parquet\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
