{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 GB\n",
      "Cached memory: 0.00 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 12.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import print_gpu_memory\n",
    "print_gpu_memory()\n",
    "import os\n",
    "import sys\n",
    "base_path = r\"C:\\Users\\KAI\\Coding\\ThinkOnward_challenge\\thinkOnward_TSClassification\"\n",
    "data_path = r\"\\data\\building-instinct-starter-notebook\\Starter notebook\"\n",
    "sys.path.append(base_path+data_path)\n",
    "sys.path.append(base_path+\"\\kai\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from utils import (calculate_average_hourly_energy_consumption, train_model, get_pred, calculate_hierarchical_f1_score,\n",
    "# sample_submission_generator)\n",
    "from preprocessing import Preprocessor\n",
    "# file_path = base_path + data_path + r\"\\building-instinct-train-data\"\n",
    "# df_features = Preprocessor.load_standard_df(file_path)\n",
    "# df_features.sort_index(inplace=True)\n",
    "# df_features.to_parquet(base_path + '/preprocessed_data/data_9000.parquet', engine='pyarrow')\n",
    "\n",
    "# df_loaded_1 = pd.read_parquet(base_path + '/preprocessed_data/data_3000.parquet', engine='pyarrow')\n",
    "# df_loaded_2 = pd.read_parquet(base_path + '/preprocessed_data/data_6000.parquet', engine='pyarrow')\n",
    "# df_loaded_3 = pd.read_parquet(base_path + '/preprocessed_data/data_9000.parquet', engine='pyarrow')\n",
    "\n",
    "# df_features = pd.concat([df_loaded_1, df_loaded_2, df_loaded_3], axis=0)\n",
    "# df_features.sort_index(inplace=True)\n",
    "# df_features.to_parquet(base_path + '/preprocessed_data/standard_data.parquet', engine='pyarrow')\n",
    "\n",
    "df_features = pd.read_parquet(base_path + '/preprocessed_data/standard_data.parquet', engine='pyarrow')\n",
    "df_features.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import TargetPreprocessor\n",
    "load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')#path to the train label file\n",
    "df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "df_targets.sort_index(inplace=True)\n",
    "\n",
    "df_targets_res = df_targets[df_targets.building_stock_type == \"residential\"].filter(like='_res').copy()\n",
    "df_targets_com = df_targets[df_targets.building_stock_type == \"commercial\"].filter(like='_com').copy()\n",
    "target_preprocessor = TargetPreprocessor()\n",
    "df_targets_res, association_dict_res, encoder_res = target_preprocessor.preprocess_res(df_targets_res)\n",
    "df_targets_com, association_dict_com, encoder_com = target_preprocessor.preprocess_com(df_targets_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values(df):\n",
    "    unique_values = {}\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() > 2:\n",
    "            unique_values[col] = df[col].unique()\n",
    "    return unique_values\n",
    "unique_values_res = unique_values(df_targets_res)\n",
    "unique_values_com = unique_values(df_targets_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indices = df_features.index.intersection(df_targets_com.index)\n",
    "# Filter the data\n",
    "X_com = df_features[df_features.index.isin(common_indices)]\n",
    "X_com = torch.tensor(X_com.values, dtype=torch.float32)\n",
    "y_com = df_targets_com[df_targets_com.index.isin(common_indices)]\n",
    "y_com = torch.tensor(y_com.values, dtype=torch.float32)\n",
    "\n",
    "X_res = df_features[df_features.index.isin(df_targets_res.index)]\n",
    "X_res = torch.tensor(X_res.values, dtype=torch.float32)\n",
    "y_res = df_targets_res[df_targets_res.index.isin(df_features.index)]\n",
    "y_res = torch.tensor(y_res.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992323346090601\n",
      "[[765   2]\n",
      " [  9 664]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = df_features, df_targets[\"building_stock_type\"].map({\"residential\": 0, \"commercial\": 1})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "            ('scaler', StandardScaler(), X.columns),\n",
    "            ('encoder', OneHotEncoder(), [])\n",
    "        ])),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f1_score(y_test, y_pred, average='macro'))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(base_path + '/preprocessed_data/data_test.parquet', engine='pyarrow')\n",
    "df_test.sort_index(inplace=True)\n",
    "\n",
    "clf = Pipeline([('preprocessor', ColumnTransformer([\n",
    "            ('scaler', StandardScaler(), X.columns),\n",
    "            ('encoder', OneHotEncoder(), [])\n",
    "        ])),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(df_test)\n",
    "df_test[\"building_stock_type\"] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## com model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CustomLoss_1(nn.Module):\n",
    "    def __init__(self, column_groups, valid_labels):\n",
    "        super(CustomLoss_1, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.column_groups = column_groups  # Dictionary mapping attribute prefixes to column indices\n",
    "        self.valid_labels = {k: torch.tensor(v).float() for k, v in valid_labels.items()}  # Convert valid labels to tensor\n",
    "        self.end_numerical = int(min([min(v) for v in self.column_groups.values()]))\n",
    "\n",
    "    def custom_closest_loss(self, predicted, true, valid_labels):\n",
    "        valid_labels = valid_labels.to(predicted.device)\n",
    "        # Expand dimensions to allow broadcasting (batch_size, num_valid_labels)\n",
    "        predicted_expanded = predicted.unsqueeze(1)  # (batch_size, 1)\n",
    "        valid_labels_expanded = valid_labels.unsqueeze(0)  # (1, num_valid_labels)\n",
    "        \n",
    "        # Calculate the absolute differences\n",
    "        distances = torch.abs(predicted_expanded - valid_labels_expanded)  # (batch_size, num_valid_labels)\n",
    "        \n",
    "        # Find the closest valid label (index of the smallest distance)\n",
    "        min_distances, min_indices = torch.min(distances, dim=1)  # min_distances: (batch_size,), min_indices: (batch_size,)\n",
    "        \n",
    "        # Get the corresponding closest labels\n",
    "        closest_labels = valid_labels[min_indices]  # (batch_size,)\n",
    "        \n",
    "        # Compute the loss only where the closest label is not equal to the true label\n",
    "        mask = closest_labels != true\n",
    "        loss = torch.abs(predicted[mask] - true[mask]).mean()  # Mean absolute error over all incorrect predictions\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, categorical_pred, categorical_true):\n",
    "\n",
    "        loss_numerical = 0.0\n",
    "        i = 0\n",
    "        # Loop over each attribute group\n",
    "        for attr, labels in self.valid_labels.items():# TODO: maybe speed it up by inserting all labels as a matrix and performing matrix operations\n",
    "            # Calculate the loss for this group using the valid labels\n",
    "            loss_numerical += self.custom_closest_loss(categorical_pred[:, i], categorical_true[:, i], labels)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Compute numerical loss (assuming the first few columns are numerical)\n",
    "        # loss_numerical = self.custom_closest_loss(categorical_pred[:, :self.end_numerical], categorical_true[:, :self.end_numerical])\n",
    "        # loss_numerical = self.mse_loss(categorical_pred[:, :self.end_numerical], categorical_true[:, :self.end_numerical])\n",
    "\n",
    "        # Initialize categorical loss\n",
    "        loss_categorical = 0.0\n",
    "        \n",
    "        # For each attribute group, compute the cross-entropy loss\n",
    "        for attr, indices in self.column_groups.items():\n",
    "            # Extract logits for the current attribute\n",
    "            logits = categorical_pred[:, indices]\n",
    "            \n",
    "            # Extract the true labels for the current attribute\n",
    "            # Convert one-hot encoding to class indices\n",
    "            true_labels = torch.argmax(categorical_true[:, indices], dim=1)\n",
    "            \n",
    "            # Compute cross-entropy loss for the current attribute\n",
    "            loss_categorical += F.cross_entropy(logits, true_labels)\n",
    "\n",
    "        total_loss = loss_numerical + loss_categorical\n",
    "        return total_loss, loss_numerical, loss_categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 GB\n",
      "Cached memory: 0.00 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 12.00 GB\n",
      "Epoch [1/20], Total Loss: 2549.7341, Numerical Loss: 2544.4231, Categorical Loss: 5.3109, Combined Loss: 30.7551, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [2/20], Total Loss: 2356.4426, Numerical Loss: 2350.7190, Categorical Loss: 5.7237, Combined Loss: 29.2308, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [3/20], Total Loss: 2204.7012, Numerical Loss: 2198.8752, Categorical Loss: 5.8258, Combined Loss: 27.8146, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [4/20], Total Loss: 2071.7246, Numerical Loss: 2065.8181, Categorical Loss: 5.9064, Combined Loss: 26.5646, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [5/20], Total Loss: 897.1531, Numerical Loss: 892.1793, Categorical Loss: 4.9738, Combined Loss: 13.8956, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [6/20], Total Loss: 590.7463, Numerical Loss: 583.9551, Categorical Loss: 6.7912, Combined Loss: 12.6307, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [7/20], Total Loss: 801.6683, Numerical Loss: 796.1685, Categorical Loss: 5.4998, Combined Loss: 13.4614, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [8/20], Total Loss: 31.6502, Numerical Loss: 27.0031, Categorical Loss: 4.6472, Combined Loss: 4.9172, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [9/20], Total Loss: 584.3300, Numerical Loss: 577.0483, Categorical Loss: 7.2816, Combined Loss: 13.0521, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [10/20], Total Loss: 1269.8893, Numerical Loss: 1263.3409, Categorical Loss: 6.5483, Combined Loss: 19.1817, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [11/20], Total Loss: 962.3442, Numerical Loss: 957.3538, Categorical Loss: 4.9903, Combined Loss: 14.5639, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [12/20], Total Loss: 45.5909, Numerical Loss: 40.3702, Categorical Loss: 5.2207, Combined Loss: 5.6244, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [13/20], Total Loss: 48.9712, Numerical Loss: 44.7087, Categorical Loss: 4.2625, Combined Loss: 4.7096, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [14/20], Total Loss: 51.7537, Numerical Loss: 45.8056, Categorical Loss: 5.9481, Combined Loss: 6.4061, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [15/20], Total Loss: 1111.3348, Numerical Loss: 1105.7170, Categorical Loss: 5.6178, Combined Loss: 16.6750, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [16/20], Total Loss: 1113.0431, Numerical Loss: 1108.2148, Categorical Loss: 4.8283, Combined Loss: 15.9104, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [17/20], Total Loss: 580.2933, Numerical Loss: 575.2693, Categorical Loss: 5.0240, Combined Loss: 10.7767, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [18/20], Total Loss: 656.5991, Numerical Loss: 650.0414, Categorical Loss: 6.5577, Combined Loss: 13.0581, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [19/20], Total Loss: 1125.8773, Numerical Loss: 1120.7841, Categorical Loss: 5.0933, Combined Loss: 16.3011, weights: 0.0100, 1.0000\n",
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 11.32 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n",
      "Epoch [20/20], Total Loss: 499.2005, Numerical Loss: 495.3571, Categorical Loss: 3.8434, Combined Loss: 8.7970, weights: 0.0100, 1.0000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from models import MultiTaskLSTM, CustomLoss, TimeSeriesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# model parameters\n",
    "batch_size = 16\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_size, sequence_length = 1, X_com.shape[1]\n",
    "num_classes_categorical = y_com.shape[1]\n",
    "hidden_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "weight_numerical = 10e-3\n",
    "weight_categorical = 1.0\n",
    "\n",
    "best_combined_loss = float('inf')\n",
    "checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_{datetime.now().strftime(\"%m_%d_%H_%M\")}.pth.tar'\n",
    "\n",
    "# create dataloaders\n",
    "dataloader_com = DataLoader(TimeSeriesDataset(X_com, y_com), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "model = model.to(device)\n",
    "criterion = CustomLoss_1(association_dict_com, unique_values_com)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # print_gpu_memory()\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_categorical_batch in dataloader_com:\n",
    "        X_batch = X_batch.view(X_batch.shape[0], sequence_length, input_size)\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_categorical_batch = y_categorical_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        categorical_pred = model(X_batch)\n",
    "        loss, loss_numerical, loss_categorical = criterion(categorical_pred, y_categorical_batch)\n",
    "        \n",
    "        # Combine losses with adjusted weights\n",
    "        combined_loss = weight_numerical * loss_numerical + weight_categorical * loss_categorical\n",
    "\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Total Loss: {loss.item():.4f}, '\n",
    "                  f'Numerical Loss: {loss_numerical.item():.4f}, '\n",
    "                  f'Categorical Loss: {loss_categorical.item():.4f}, '\n",
    "                  f'Combined Loss: {combined_loss.item():.4f}, '\n",
    "                  f'weights: {weight_numerical:.4f}, {weight_categorical:.4f}')\n",
    "    \n",
    "    if combined_loss.item() < best_combined_loss:\n",
    "        torch.save({'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': combined_loss.item(),}, checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KAI\\AppData\\Local\\Temp\\ipykernel_8236\\1250800709.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5293e+00, 1.9764e+03, 7.2813e+01, 6.8851e+01, 7.6482e+00, 1.0340e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5634e+00, 1.9740e+03, 7.2779e+01, 6.8623e+01, 7.6589e+00, 9.7448e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5879e+00, 1.9668e+03, 7.2692e+01, 6.8938e+01, 7.4934e+00, 1.0317e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5267e+00, 1.9784e+03, 7.2868e+01, 6.8746e+01, 7.6929e+00, 1.0096e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5937e+00, 1.9705e+03, 7.2706e+01, 6.8551e+01, 7.6284e+00, 9.5066e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5734e+00, 1.9731e+03, 7.2765e+01, 6.8610e+01, 7.6498e+00, 9.6687e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5944e+00, 1.9646e+03, 7.2639e+01, 6.8901e+01, 7.4703e+00, 1.0279e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5388e+00, 1.9711e+03, 7.2682e+01, 6.8816e+01, 7.5872e+00, 1.0408e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5984e+00, 1.9688e+03, 7.2654e+01, 6.8500e+01, 7.6199e+00, 9.4605e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5828e+00, 1.9722e+03, 7.2749e+01, 6.8593e+01, 7.6409e+00, 9.5958e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5328e+00, 1.9736e+03, 7.2740e+01, 6.8825e+01, 7.6177e+00, 1.0378e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5460e+00, 1.9703e+03, 7.2675e+01, 6.8832e+01, 7.5715e+00, 1.0402e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5964e+00, 1.9697e+03, 7.2681e+01, 6.8527e+01, 7.6239e+00, 9.4808e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5311e+00, 1.9770e+03, 7.2832e+01, 6.8682e+01, 7.6888e+00, 1.0005e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5291e+00, 1.9778e+03, 7.2848e+01, 6.8723e+01, 7.6909e+00, 1.0067e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5620e+00, 1.9741e+03, 7.2780e+01, 6.8625e+01, 7.6601e+00, 9.7553e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([1.0000e+00, 1.9750e+03, 7.2000e+01, 6.8000e+01, 7.2500e+00, 6.7500e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from models import MultiTaskLSTM, CustomLoss, TimeSeriesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size, input_size, sequence_length, hidden_size, num_classes_categorical = 16, 1, X_com.shape[1], 64, y_com.shape[1]\n",
    "# checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_08_06_14_49.pth.tar'\n",
    "checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_08_13_23_14.pth.tar'\n",
    "dataloader_com = DataLoader(TimeSeriesDataset(X_com, y_com), batch_size=batch_size, shuffle=True)\n",
    "model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_filename)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "for X_batch, y_categorical_batch in dataloader_com:\n",
    "    for i in [0, 1]:\n",
    "        _X_batch = X_batch[:, :].view(X_batch.shape[0], sequence_length, input_size)\n",
    "        _X_batch = _X_batch.to(device)\n",
    "        categorical_pred = model.predict(_X_batch, association_dict_com)\n",
    "        print(categorical_pred)\n",
    "        print(y_categorical_batch[i, :])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the original com df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in.number_of_stories_com</th>\n",
       "      <th>in.vintage_com</th>\n",
       "      <th>in.tstat_clg_sp_f..f_com</th>\n",
       "      <th>in.tstat_htg_sp_f..f_com</th>\n",
       "      <th>in.weekday_opening_time..hr_com</th>\n",
       "      <th>in.weekday_operating_hours..hr_com</th>\n",
       "      <th>in.comstock_building_type_group_com</th>\n",
       "      <th>in.heating_fuel_com</th>\n",
       "      <th>in.hvac_category_com</th>\n",
       "      <th>in.ownership_type_com</th>\n",
       "      <th>in.wall_construction_type_com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1960 to 1969</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Multizone CAV/VAV</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1960 to 1969</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Multizone CAV/VAV</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1960 to 1969</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1960 to 1969</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Office</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1970 to 1979</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Mercantile</td>\n",
       "      <td>NaturalGas</td>\n",
       "      <td>Small Packaged Unit</td>\n",
       "      <td>owner_occupied</td>\n",
       "      <td>Mass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    in.number_of_stories_com in.vintage_com  in.tstat_clg_sp_f..f_com  \\\n",
       "0                          2   1970 to 1979                        73   \n",
       "1                          2   1970 to 1979                        73   \n",
       "2                          2   1960 to 1969                        73   \n",
       "3                          2   1970 to 1979                        73   \n",
       "4                          2   1970 to 1979                        73   \n",
       "5                          2   1970 to 1979                        73   \n",
       "6                          2   1960 to 1969                        73   \n",
       "7                          2   1970 to 1979                        73   \n",
       "8                          2   1960 to 1969                        73   \n",
       "9                          2   1970 to 1979                        73   \n",
       "10                         2   1970 to 1979                        73   \n",
       "11                         2   1970 to 1979                        73   \n",
       "12                         2   1960 to 1969                        73   \n",
       "13                         2   1970 to 1979                        73   \n",
       "14                         2   1970 to 1979                        73   \n",
       "15                         2   1970 to 1979                        73   \n",
       "\n",
       "    in.tstat_htg_sp_f..f_com in.weekday_opening_time..hr_com  \\\n",
       "0                         69                            7.75   \n",
       "1                         69                            7.75   \n",
       "2                         69                             7.5   \n",
       "3                         69                            7.75   \n",
       "4                         69                            7.75   \n",
       "5                         69                            7.75   \n",
       "6                         69                             7.5   \n",
       "7                         69                             7.5   \n",
       "8                         69                             7.5   \n",
       "9                         69                            7.75   \n",
       "10                        69                             7.5   \n",
       "11                        69                             7.5   \n",
       "12                        69                             7.5   \n",
       "13                        69                            7.75   \n",
       "14                        69                            7.75   \n",
       "15                        69                            7.75   \n",
       "\n",
       "   in.weekday_operating_hours..hr_com in.comstock_building_type_group_com  \\\n",
       "0                               10.25                          Mercantile   \n",
       "1                                9.75                          Mercantile   \n",
       "2                               10.25                          Mercantile   \n",
       "3                                10.0                          Mercantile   \n",
       "4                                 9.5                              Office   \n",
       "5                                9.75                              Office   \n",
       "6                               10.25                          Mercantile   \n",
       "7                                10.5                          Mercantile   \n",
       "8                                 9.5                              Office   \n",
       "9                                 9.5                              Office   \n",
       "10                               10.5                          Mercantile   \n",
       "11                               10.5                          Mercantile   \n",
       "12                                9.5                              Office   \n",
       "13                               10.0                          Mercantile   \n",
       "14                               10.0                          Mercantile   \n",
       "15                               9.75                          Mercantile   \n",
       "\n",
       "   in.heating_fuel_com in.hvac_category_com in.ownership_type_com  \\\n",
       "0           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "1           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "2           NaturalGas    Multizone CAV/VAV        owner_occupied   \n",
       "3           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "4           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "5           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "6           NaturalGas    Multizone CAV/VAV        owner_occupied   \n",
       "7           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "8           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "9           NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "10          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "11          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "12          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "13          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "14          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "15          NaturalGas  Small Packaged Unit        owner_occupied   \n",
       "\n",
       "   in.wall_construction_type_com  \n",
       "0                           Mass  \n",
       "1                           Mass  \n",
       "2                           Mass  \n",
       "3                           Mass  \n",
       "4                           Mass  \n",
       "5                           Mass  \n",
       "6                           Mass  \n",
       "7                           Mass  \n",
       "8                           Mass  \n",
       "9                           Mass  \n",
       "10                          Mass  \n",
       "11                          Mass  \n",
       "12                          Mass  \n",
       "13                          Mass  \n",
       "14                          Mass  \n",
       "15                          Mass  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from postprocessing import inverse_process\n",
    "com_arr_df = inverse_process(categorical_pred, encoder_com) # todo: standardize the target numerical columns in preprocessing\n",
    "display(com_arr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654, 35040)\n"
     ]
    }
   ],
   "source": [
    "# file_path = base_path + data_path + r\"\\building-instinct-test-data\"\n",
    "# df_features = Preprocessor.load_standard_df(file_path)\n",
    "# df_features.sort_index(inplace=True)\n",
    "# df_features.to_parquet(base_path + '/preprocessed_data/data_test.parquet', engine='pyarrow')\n",
    "\n",
    "# df_test = pd.read_parquet(base_path + '/preprocessed_data/data_test.parquet', engine='pyarrow')\n",
    "# df_test.sort_index(inplace=True)\n",
    "\n",
    "# TODO: clf that predicts com or res\n",
    "# then filter by com & res\n",
    "df_test_com = df_test[df_test.building_stock_type == 1]\n",
    "# df_test = df_test[df_test.index.isin(df_features.index)]\n",
    "# X_test_com = torch.tensor(df_test.values, dtype=torch.float32)\n",
    "# X_test_com = X_test_com.to(device)\n",
    "print(df_test_com[df_test_com.columns.difference([\"building_stock_type\"])].shape)\n",
    "X_test_com, y_pred_com = torch.tensor(df_test_com[df_test_com.columns.difference([\"building_stock_type\"])].values, dtype=torch.float32), df_test_com[\"building_stock_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KAI\\AppData\\Local\\Temp\\ipykernel_8236\\4240090578.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 0.03 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:11<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "def free_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "free_gpu_memory()\n",
    "model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "model = model.to(device)\n",
    "# criterion = CustomLoss(association_dict_com)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_08_06_14_49.pth.tar'\n",
    "checkpoint_filename = base_path+ f'/kai/checkpoints/com_model_checkpoint_08_13_23_14.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_filename)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "best_loss = checkpoint['loss']\n",
    "\n",
    "predictions = []\n",
    "dataloader = DataLoader(TimeSeriesDataset(X_test_com, torch.zeros(X_test_com.shape[0], y_com.shape[1])), batch_size=16, shuffle=False)\n",
    "\n",
    "print_gpu_memory()\n",
    "for X_batch, y_categorical_batch in tqdm(dataloader):\n",
    "    model.eval()\n",
    "    _X_batch = X_batch.view(X_batch.shape[0], sequence_length, input_size)\n",
    "    _X_batch = _X_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        categorical_pred = model.predict(_X_batch, association_dict_com)\n",
    "    predictions.append(categorical_pred.cpu())\n",
    "    del _X_batch, categorical_pred\n",
    "    free_gpu_memory()\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "arr_df_com = inverse_process(predictions, encoder_com)\n",
    "arr_df_com.index = df_test_com.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "['1970 to 1979' '1960 to 1969']\n",
      "[73]\n",
      "[69 68]\n",
      "[7.5 7.75]\n",
      "[10.25 10.5 10.0 9.75 9.5]\n",
      "['Mercantile' 'Office']\n",
      "['NaturalGas']\n",
      "['Small Packaged Unit' 'Multizone CAV/VAV']\n",
      "['owner_occupied']\n",
      "['Mass']\n"
     ]
    }
   ],
   "source": [
    "for col in arr_df_com.columns:\n",
    "    print(arr_df_com[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Define custom loss function\n",
    "class CustomLoss_res(nn.Module):\n",
    "    def __init__(self, column_groups, valid_labels):\n",
    "        super(CustomLoss_res, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.column_groups = column_groups  # Dictionary mapping attribute prefixes to column indices\n",
    "        self.valid_labels = {k: torch.tensor(v).float() for k, v in valid_labels.items()}  # Convert valid labels to tensor\n",
    "        self.end_numerical = int(min([min(v) for v in self.column_groups.values()]))\n",
    "\n",
    "    def custom_closest_loss(self, predicted, true, valid_labels):\n",
    "        # Expand dimensions to allow broadcasting (batch_size, num_valid_labels)\n",
    "        predicted_expanded = predicted.unsqueeze(1)  # (batch_size, 1)\n",
    "        valid_labels_expanded = valid_labels.unsqueeze(0)  # (1, num_valid_labels)\n",
    "        \n",
    "        # Calculate the absolute differences\n",
    "        distances = torch.abs(predicted_expanded - valid_labels_expanded)  # (batch_size, num_valid_labels)\n",
    "        \n",
    "        # Find the closest valid label (index of the smallest distance)\n",
    "        min_distances, min_indices = torch.min(distances, dim=1)  # min_distances: (batch_size,), min_indices: (batch_size,)\n",
    "        \n",
    "        # Get the corresponding closest labels\n",
    "        closest_labels = valid_labels[min_indices]  # (batch_size,)\n",
    "        \n",
    "        # Compute the loss only where the closest label is not equal to the true label\n",
    "        mask = closest_labels != true\n",
    "        loss = torch.abs(predicted[mask] - true[mask]).mean()  # Mean absolute error over all incorrect predictions\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, categorical_pred, categorical_true):\n",
    "\n",
    "        loss_numerical = 0.0\n",
    "        i = 0\n",
    "        # Loop over each attribute group\n",
    "        for attr, labels in self.valid_labels.items():# TODO: maybe speed it up by inserting all labels as a matrix and performing matrix operations\n",
    "            # Calculate the loss for this group using the valid labels\n",
    "            loss_numerical += self.custom_closest_loss(categorical_pred[:, i], categorical_true[:, i], labels)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Compute numerical loss (assuming the first few columns are numerical)\n",
    "        # loss_numerical = self.custom_closest_loss(categorical_pred[:, :self.end_numerical], categorical_true[:, :self.end_numerical])\n",
    "        # loss_numerical = self.mse_loss(categorical_pred[:, :self.end_numerical], categorical_true[:, :self.end_numerical])\n",
    "\n",
    "        # Initialize categorical loss\n",
    "        loss_categorical = 0.0\n",
    "        \n",
    "        # For each attribute group, compute the cross-entropy loss\n",
    "        for attr, indices in self.column_groups.items():\n",
    "            # Extract logits for the current attribute\n",
    "            logits = categorical_pred[:, indices]\n",
    "            \n",
    "            # Extract the true labels for the current attribute\n",
    "            # Convert one-hot encoding to class indices\n",
    "            true_labels = torch.argmax(categorical_true[:, indices], dim=1)\n",
    "            \n",
    "            # Compute cross-entropy loss for the current attribute\n",
    "            loss_categorical += F.cross_entropy(logits, true_labels)\n",
    "\n",
    "        total_loss = loss_numerical + loss_categorical\n",
    "        return total_loss, loss_numerical, loss_categorical   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Total Loss: 72215.5859, Numerical Loss: 72208.4219, Categorical Loss: 7.1626, Combined Loss: 79.3710, weights: 0.0010, 1.0000\n",
      "Epoch [2/20], Total Loss: 70440.7812, Numerical Loss: 70434.2188, Categorical Loss: 6.5609, Combined Loss: 76.9951, weights: 0.0010, 1.0000\n",
      "Epoch [3/20], Total Loss: 99618.4062, Numerical Loss: 99611.2344, Categorical Loss: 7.1681, Combined Loss: 106.7793, weights: 0.0010, 1.0000\n",
      "Epoch [4/20], Total Loss: 101878.4922, Numerical Loss: 101872.8750, Categorical Loss: 5.6162, Combined Loss: 107.4891, weights: 0.0010, 1.0000\n",
      "Epoch [5/20], Total Loss: 68431.3125, Numerical Loss: 68425.3828, Categorical Loss: 5.9283, Combined Loss: 74.3537, weights: 0.0010, 1.0000\n",
      "Epoch [6/20], Total Loss: 62581.7617, Numerical Loss: 62576.2070, Categorical Loss: 5.5551, Combined Loss: 68.1314, weights: 0.0010, 1.0000\n",
      "Epoch [7/20], Total Loss: 81158.4453, Numerical Loss: 81151.3750, Categorical Loss: 7.0724, Combined Loss: 88.2238, weights: 0.0010, 1.0000\n",
      "Epoch [8/20], Total Loss: 95010.0625, Numerical Loss: 95003.9609, Categorical Loss: 6.1023, Combined Loss: 101.1063, weights: 0.0010, 1.0000\n",
      "Epoch [9/20], Total Loss: 92636.0547, Numerical Loss: 92629.1641, Categorical Loss: 6.8875, Combined Loss: 99.5167, weights: 0.0010, 1.0000\n",
      "Epoch [10/20], Total Loss: 79122.4219, Numerical Loss: 79115.6016, Categorical Loss: 6.8201, Combined Loss: 85.9357, weights: 0.0010, 1.0000\n",
      "Epoch [11/20], Total Loss: 76308.3438, Numerical Loss: 76300.3125, Categorical Loss: 8.0311, Combined Loss: 84.3314, weights: 0.0010, 1.0000\n",
      "Epoch [12/20], Total Loss: 66294.1484, Numerical Loss: 66287.3438, Categorical Loss: 6.8069, Combined Loss: 73.0942, weights: 0.0010, 1.0000\n",
      "Epoch [13/20], Total Loss: 110709.2812, Numerical Loss: 110701.2344, Categorical Loss: 8.0502, Combined Loss: 118.7514, weights: 0.0010, 1.0000\n",
      "Epoch [14/20], Total Loss: 100721.2891, Numerical Loss: 100715.2188, Categorical Loss: 6.0722, Combined Loss: 106.7874, weights: 0.0010, 1.0000\n",
      "Epoch [15/20], Total Loss: 106180.7344, Numerical Loss: 106173.9141, Categorical Loss: 6.8205, Combined Loss: 112.9945, weights: 0.0010, 1.0000\n",
      "Epoch [16/20], Total Loss: 56932.9062, Numerical Loss: 56926.1992, Categorical Loss: 6.7060, Combined Loss: 63.6322, weights: 0.0010, 1.0000\n",
      "Epoch [17/20], Total Loss: 111957.0312, Numerical Loss: 111950.8906, Categorical Loss: 6.1386, Combined Loss: 118.0895, weights: 0.0010, 1.0000\n",
      "Epoch [18/20], Total Loss: 91562.3750, Numerical Loss: 91556.7109, Categorical Loss: 5.6651, Combined Loss: 97.2218, weights: 0.0010, 1.0000\n",
      "Epoch [19/20], Total Loss: 118411.7109, Numerical Loss: 118405.1094, Categorical Loss: 6.6012, Combined Loss: 125.0063, weights: 0.0010, 1.0000\n",
      "Epoch [20/20], Total Loss: 59289.5234, Numerical Loss: 59281.9258, Categorical Loss: 7.5981, Combined Loss: 66.8801, weights: 0.0010, 1.0000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from models import MultiTaskLSTM, CustomLoss, TimeSeriesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# model parameters\n",
    "batch_size = 16\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_size, sequence_length = 1, X_res.shape[1]\n",
    "num_classes_categorical = y_res.shape[1]\n",
    "hidden_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "weight_numerical = 10e-4\n",
    "weight_categorical = 1.0\n",
    "\n",
    "best_combined_loss = float('inf')\n",
    "checkpoint_filename = base_path+ f'/kai/checkpoints/res_model_checkpoint_{datetime.now().strftime(\"%m_%d_%H_%M\")}.pth.tar'\n",
    "\n",
    "# create dataloaders\n",
    "# dataloader_com = DataLoader(TimeSeriesDataset(X_com, y_com), batch_size=batch_size, shuffle=True)\n",
    "dataloader_res = DataLoader(TimeSeriesDataset(X_res, y_res), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "model = model.to(device)\n",
    "criterion = CustomLoss_1(association_dict_res, unique_values_res)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_categorical_batch in dataloader_res:\n",
    "        X_batch = X_batch.view(X_batch.shape[0], sequence_length, input_size)\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_categorical_batch = y_categorical_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        categorical_pred = model(X_batch)\n",
    "        loss, loss_numerical, loss_categorical = criterion(categorical_pred, y_categorical_batch)\n",
    "        \n",
    "        # Combine losses with adjusted weights\n",
    "        combined_loss = weight_numerical * loss_numerical + weight_categorical * loss_categorical\n",
    "\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Total Loss: {loss.item():.4f}, '\n",
    "                  f'Numerical Loss: {loss_numerical.item():.4f}, '\n",
    "                  f'Categorical Loss: {loss_categorical.item():.4f}, '\n",
    "                  f'Combined Loss: {combined_loss.item():.4f}, '\n",
    "                  f'weights: {weight_numerical:.4f}, {weight_categorical:.4f}')\n",
    "    \n",
    "    if combined_loss.item() < best_combined_loss:\n",
    "        torch.save({'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': combined_loss.item(),}, checkpoint_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### todo postprocessing + analysis of res and com labels in general! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_closest_values(predictions, possible_values):\n",
    "    \"\"\"\n",
    "    Map predicted values to the closest possible values.\n",
    "\n",
    "    Args:\n",
    "    - predictions (list of lists): The predicted values.\n",
    "    - possible_values (dict): A dictionary where keys are column names and values are lists of possible values.\n",
    "\n",
    "    Returns:\n",
    "    - mapped_predictions (list of lists): The predictions mapped to the closest possible values.\n",
    "    \"\"\"\n",
    "    def closest_value(predicted, possible):\n",
    "        return possible[np.argmin((np.array(possible) - predicted)**2)]\n",
    "    \n",
    "    mapped_predictions = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        mapped_row = []\n",
    "        for col_name, pred_value in zip(possible_values.keys(), prediction):\n",
    "            mapped_value = closest_value(pred_value, possible_values[col_name])\n",
    "            mapped_row.append(mapped_value)\n",
    "        mapped_predictions.append(mapped_row)\n",
    "    \n",
    "    return mapped_predictions\n",
    "\n",
    "def inverse_process_res(prediction, encoder):\n",
    "    prediction = prediction.cpu().detach().numpy()\n",
    "\n",
    "    # Possible values for each column\n",
    "    possible_values = {\n",
    "        'in.bedrooms_res': [3, 2, 1, 4, 5],\n",
    "        'in.cooling_setpoint_res': [68, 75, 72, 70, 78, 80, 60, 65, 76, 67, 62],\n",
    "        'in.heating_setpoint_res': [75, 72, 68, 70, 62, 55, 65, 78, 76, 67, 60, 80],\n",
    "        'in.geometry_floor_area_res': [1749,  874, 1249,  624, 2249, 4000, 2749, 3499,  249],\n",
    "        'in.income_res': [109999,  12499,  64999,  89999,  42499,  22499,  47499, 169999, 129999,\n",
    "                            200000,  37499,  54999,  32499,  10000, 189999,  74999,  17499,  27499,\n",
    "                            149999],\n",
    "        'in.vintage_res': [1940, 1975, 1985, 1945, 1995, 1955, 1965, 2005, 2015]\n",
    "    }\n",
    "    mapped_predictions = prediction.copy()\n",
    "    # Map the predictions\n",
    "    mapped_predictions_1 = map_to_closest_values(prediction[:, :6], possible_values)\n",
    "    mapped_predictions_2 = encoder.inverse_transform(prediction[:, 6:])\n",
    "\n",
    "    mapped_predictions = np.concatenate((mapped_predictions_1, mapped_predictions_2), axis=1)\n",
    "    cols = ['in.bedrooms_res', 'in.cooling_setpoint_res', 'in.heating_setpoint_res',\n",
    "            'in.geometry_floor_area_res', 'in.income_res', 'in.vintage_res',\n",
    "            'in.geometry_building_type_recs_res', 'in.geometry_foundation_type_res', \n",
    "            'in.geometry_wall_type_res', 'in.heating_fuel_res', 'in.roof_material_res',\n",
    "            'in.tenure_res', 'in.vacancy_status_res']\n",
    "    \n",
    "    mapped_df = pd.DataFrame(mapped_predictions, columns=cols)\n",
    "\n",
    "    vintage_mapping = {1940:'<1940', 1945:'1940s', 1955:'1950s', 1965:'1960s',\n",
    "                           1975:'1970s', 1985:'1980s', 1995:'1990s', 2005:'2000s', 2015:'2010s'}\n",
    "    mapped_df['in.vintage_res'] = mapped_df['in.vintage_res'].map(vintage_mapping)\n",
    "    mapped_df['in.cooling_setpoint_res'] = mapped_df['in.cooling_setpoint_res'].apply(lambda x: str(x)+\"F\").astype(str)\n",
    "    mapped_df['in.heating_setpoint_res'] = mapped_df['in.heating_setpoint_res'].apply(lambda x: str(x)+\"F\").astype(str)\n",
    "    mapped_df['in.bedrooms_res'] = mapped_df['in.bedrooms_res'].astype(str)\n",
    "\n",
    "    income_mapping = {109999: '100000-119999', 12499: '10000-14999', 64999: '60000-69999', 89999: '80000-99999', \n",
    "                      42499: '40000-44999', 22499: '20000-24999', 47499: '45000-49999', 169999: '160000-179999', \n",
    "                      129999: '120000-139999', 200000: '200000+', 37499: '35000-39999', 54999: '50000-59999', \n",
    "                      32499: '30000-34999', 10000: '<10000', 189999: '180000-199999', 74999: '70000-79999', \n",
    "                      17499: '15000-19999', 27499: '25000-29999', 149999: '140000-159999'}\n",
    "    mapped_df['in.income_res'] = mapped_df['in.income_res'].map(income_mapping)\n",
    "\n",
    "    geometry_mapping = {1749: '1500-1999', 874: '750-999', 1249: '1000-1499', 624: '500-749', 2249: '2000-2499',\n",
    "                        4000: '4000+', 2749: '2500-2999', 3499: '3000-3999', 249: '0-499'}\n",
    "    mapped_df['in.geometry_floor_area_res'] = mapped_df['in.geometry_floor_area_res'].map(geometry_mapping)\n",
    "\n",
    "    return mapped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KAI\\AppData\\Local\\Temp\\ipykernel_8236\\1758560243.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.02 GB\n",
      "Cached memory: 0.03 GB\n",
      "Total memory: 12.00 GB\n",
      "Unused memory: 11.98 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.64it/s]\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "def free_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "free_gpu_memory()\n",
    "\n",
    "batch_size = 16\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_size, sequence_length = 1, X_res.shape[1]\n",
    "num_classes_categorical = y_res.shape[1]\n",
    "hidden_size = 64\n",
    "num_epochs = 20\n",
    "model = MultiTaskLSTM(input_size, hidden_size, num_classes_categorical)\n",
    "model = model.to(device)\n",
    "# criterion = CustomLoss_res(association_dict_res)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# checkpoint_filename = base_path+ f'/kai/checkpoints/res_model_checkpoint_08_09_22_35.pth.tar'\n",
    "checkpoint_filename = base_path+ f'/kai/checkpoints/res_model_checkpoint_08_13_23_37.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_filename)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "best_loss = checkpoint['loss']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "df_test_res = df_test[df_test.building_stock_type == 0]\n",
    "X_test_res, y_pred_res = torch.tensor(df_test_res[df_test_res.columns.difference([\"building_stock_type\"])].values, dtype=torch.float32), df_test_res[\"building_stock_type\"]\n",
    "dataloader = DataLoader(TimeSeriesDataset(X_test_res, torch.zeros(X_test_res.shape[0], y_res.shape[1])), batch_size=16, shuffle=False)\n",
    "\n",
    "print_gpu_memory()\n",
    "for X_batch, y_categorical_batch in tqdm(dataloader):\n",
    "    model.eval()\n",
    "    _X_batch = X_batch.view(X_batch.shape[0], sequence_length, input_size)\n",
    "    _X_batch = _X_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        categorical_pred = model.predict(_X_batch, association_dict_res)\n",
    "    predictions.append(categorical_pred.cpu())\n",
    "    del _X_batch, categorical_pred\n",
    "    free_gpu_memory()\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "arr_df_res = inverse_process_res(predictions, encoder_res)\n",
    "arr_df_res.index = df_test_res.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in.bedrooms_res\n",
      "['2' '3']\n",
      "in.cooling_setpoint_res\n",
      "['72F']\n",
      "in.heating_setpoint_res\n",
      "['70F' '72F' '65F' '68F' '55F' '60F' '62F' '67F']\n",
      "in.geometry_floor_area_res\n",
      "['1500-1999' '1000-1499']\n",
      "in.income_res\n",
      "['<10000']\n",
      "in.vintage_res\n",
      "['1990s' '1980s' '1950s' '1960s' '1970s']\n",
      "in.geometry_building_type_recs_res\n",
      "['Single-Family Detached' 'Multi-Family with 5+ Units']\n",
      "in.geometry_foundation_type_res\n",
      "['Slab']\n",
      "in.geometry_wall_type_res\n",
      "['Wood Frame']\n",
      "in.heating_fuel_res\n",
      "['Natural Gas' 'Electricity']\n",
      "in.roof_material_res\n",
      "['Composition Shingles' 'Asphalt Shingles, Medium']\n",
      "in.tenure_res\n",
      "['Renter' 'Owner']\n",
      "in.vacancy_status_res\n",
      "['Occupied' 'Vacant']\n"
     ]
    }
   ],
   "source": [
    "for col in arr_df_res.columns:\n",
    "    print(col)\n",
    "    print(arr_df_res[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        building_stock_type in.comstock_building_type_group_com  \\\n",
      "bldg_id                                                           \n",
      "1               residential                                 nan   \n",
      "2               residential                                 nan   \n",
      "3                commercial                          Mercantile   \n",
      "4                commercial                          Mercantile   \n",
      "5               residential                                 nan   \n",
      "...                     ...                                 ...   \n",
      "1436             commercial                          Mercantile   \n",
      "1437            residential                                 nan   \n",
      "1438            residential                                 nan   \n",
      "1439             commercial                          Mercantile   \n",
      "1440            residential                                 nan   \n",
      "\n",
      "        in.heating_fuel_com in.hvac_category_com in.number_of_stories_com  \\\n",
      "bldg_id                                                                     \n",
      "1                       nan                  nan                      nan   \n",
      "2                       nan                  nan                      nan   \n",
      "3                NaturalGas  Small Packaged Unit                        2   \n",
      "4                NaturalGas  Small Packaged Unit                        2   \n",
      "5                       nan                  nan                      nan   \n",
      "...                     ...                  ...                      ...   \n",
      "1436             NaturalGas  Small Packaged Unit                        2   \n",
      "1437                    nan                  nan                      nan   \n",
      "1438                    nan                  nan                      nan   \n",
      "1439             NaturalGas  Small Packaged Unit                        2   \n",
      "1440                    nan                  nan                      nan   \n",
      "\n",
      "        in.ownership_type_com in.vintage_com in.wall_construction_type_com  \\\n",
      "bldg_id                                                                      \n",
      "1                         nan            nan                           nan   \n",
      "2                         nan            nan                           nan   \n",
      "3              owner_occupied   1970 to 1979                          Mass   \n",
      "4              owner_occupied   1970 to 1979                          Mass   \n",
      "5                         nan            nan                           nan   \n",
      "...                       ...            ...                           ...   \n",
      "1436           owner_occupied   1970 to 1979                          Mass   \n",
      "1437                      nan            nan                           nan   \n",
      "1438                      nan            nan                           nan   \n",
      "1439           owner_occupied   1970 to 1979                          Mass   \n",
      "1440                      nan            nan                           nan   \n",
      "\n",
      "        in.tstat_clg_sp_f..f_com in.tstat_htg_sp_f..f_com  ...  \\\n",
      "bldg_id                                                    ...   \n",
      "1                            nan                      nan  ...   \n",
      "2                            nan                      nan  ...   \n",
      "3                             73                       69  ...   \n",
      "4                             73                       69  ...   \n",
      "5                            nan                      nan  ...   \n",
      "...                          ...                      ...  ...   \n",
      "1436                          73                       69  ...   \n",
      "1437                         nan                      nan  ...   \n",
      "1438                         nan                      nan  ...   \n",
      "1439                          73                       69  ...   \n",
      "1440                         nan                      nan  ...   \n",
      "\n",
      "        in.geometry_building_type_recs_res in.geometry_floor_area_res  \\\n",
      "bldg_id                                                                 \n",
      "1                   Single-Family Detached                  1500-1999   \n",
      "2                   Single-Family Detached                  1500-1999   \n",
      "3                                      nan                        nan   \n",
      "4                                      nan                        nan   \n",
      "5                   Single-Family Detached                  1500-1999   \n",
      "...                                    ...                        ...   \n",
      "1436                                   nan                        nan   \n",
      "1437                Single-Family Detached                  1000-1499   \n",
      "1438            Multi-Family with 5+ Units                  1500-1999   \n",
      "1439                                   nan                        nan   \n",
      "1440                Single-Family Detached                  1500-1999   \n",
      "\n",
      "        in.geometry_foundation_type_res in.geometry_wall_type_res  \\\n",
      "bldg_id                                                             \n",
      "1                                  Slab                Wood Frame   \n",
      "2                                  Slab                Wood Frame   \n",
      "3                                   nan                       nan   \n",
      "4                                   nan                       nan   \n",
      "5                                  Slab                Wood Frame   \n",
      "...                                 ...                       ...   \n",
      "1436                                nan                       nan   \n",
      "1437                               Slab                Wood Frame   \n",
      "1438                               Slab                Wood Frame   \n",
      "1439                                nan                       nan   \n",
      "1440                               Slab                Wood Frame   \n",
      "\n",
      "        in.heating_fuel_res in.income_res      in.roof_material_res  \\\n",
      "bldg_id                                                               \n",
      "1               Natural Gas        <10000      Composition Shingles   \n",
      "2               Natural Gas        <10000      Composition Shingles   \n",
      "3                       nan           nan                       nan   \n",
      "4                       nan           nan                       nan   \n",
      "5               Natural Gas        <10000      Composition Shingles   \n",
      "...                     ...           ...                       ...   \n",
      "1436                    nan           nan                       nan   \n",
      "1437            Electricity        <10000  Asphalt Shingles, Medium   \n",
      "1438            Natural Gas        <10000  Asphalt Shingles, Medium   \n",
      "1439                    nan           nan                       nan   \n",
      "1440            Natural Gas        <10000  Asphalt Shingles, Medium   \n",
      "\n",
      "        in.tenure_res in.vacancy_status_res in.vintage_res  \n",
      "bldg_id                                                     \n",
      "1              Renter              Occupied          1990s  \n",
      "2               Owner              Occupied          1980s  \n",
      "3                 nan                   nan            nan  \n",
      "4                 nan                   nan            nan  \n",
      "5              Renter              Occupied          1990s  \n",
      "...               ...                   ...            ...  \n",
      "1436              nan                   nan            nan  \n",
      "1437           Renter                Vacant          1950s  \n",
      "1438           Renter              Occupied          1990s  \n",
      "1439              nan                   nan            nan  \n",
      "1440           Renter              Occupied          1990s  \n",
      "\n",
      "[1440 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_submission(df_com, df_res, df_test, save_filepath=None):\n",
    "\n",
    "    load_filepath_labels = os.path.join(base_path + data_path,'building-instinct-train-label', 'train_label.parquet')\n",
    "\n",
    "    df_targets = pd.read_parquet(load_filepath_labels, engine='pyarrow')\n",
    "    df_targets.sort_index(inplace=True)\n",
    "\n",
    "    bldg_id_list = [i for i in range(1,1441)]\n",
    "\n",
    "    df = pd.DataFrame(index=bldg_id_list, columns=df_targets.columns)\n",
    "    df.index.name = df_targets.index.name\n",
    "\n",
    "    # Populate the first column 'building_stock_type'\n",
    "    df['building_stock_type'] = df_test[\"building_stock_type\"].map({0: 'residential', 1: 'commercial'})\n",
    "\n",
    "    # Separate columns into residential and commercial\n",
    "    res_columns = [col for col in df_targets.columns if col.endswith('_res')]\n",
    "    com_columns = [col for col in df_targets.columns if col.endswith('_com')]\n",
    "\n",
    "    # Populate the rest of the columns based on the value of 'building_stock_type'\n",
    "    for bldg_id in df.index:\n",
    "        if df.at[bldg_id, 'building_stock_type'] == 'residential':\n",
    "            df.loc[bldg_id, com_columns] = np.nan\n",
    "            for col in res_columns:\n",
    "                df.at[bldg_id, col] = df_res.at[bldg_id, col]\n",
    "        else:  # commercial\n",
    "            df.loc[bldg_id, res_columns] = np.nan\n",
    "            for col in com_columns:\n",
    "                df.at[bldg_id, col] = df_com.at[bldg_id, col]\n",
    "    df = df.astype(str)\n",
    "    if save_filepath:\n",
    "        df.to_parquet(save_filepath)\n",
    "    return df\n",
    "\n",
    "df = create_submission(arr_df_com, arr_df_res, df_test, save_filepath=\"submission.parquet\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
